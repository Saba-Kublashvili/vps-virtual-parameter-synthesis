{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4294d18ac6a3405eb62979e680d735fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e04baf8884b4dff905dc18f532c6484",
              "IPY_MODEL_fcec185afbd34161b7f4d03e86575eb2",
              "IPY_MODEL_7da8a410eec44c639fa6778a1915ac3b"
            ],
            "layout": "IPY_MODEL_e45f967632f044fa975c48352796bdbc"
          }
        },
        "7e04baf8884b4dff905dc18f532c6484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1746a6ed0b3b4f4a9ced254877a7c315",
            "placeholder": "​",
            "style": "IPY_MODEL_22209d46931942a082489cdb5b27e083",
            "value": "tokenizer_config.json: "
          }
        },
        "fcec185afbd34161b7f4d03e86575eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_377da03c912c4e5fa87b6af480022c42",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6678bf2247404745a35a5502a4e3387a",
            "value": 1
          }
        },
        "7da8a410eec44c639fa6778a1915ac3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d232ae3fc46d43f69ef8e08e3518aacc",
            "placeholder": "​",
            "style": "IPY_MODEL_4cacaba9bade467aab980ac416e50dc6",
            "value": " 7.30k/? [00:00&lt;00:00, 568kB/s]"
          }
        },
        "e45f967632f044fa975c48352796bdbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1746a6ed0b3b4f4a9ced254877a7c315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22209d46931942a082489cdb5b27e083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "377da03c912c4e5fa87b6af480022c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6678bf2247404745a35a5502a4e3387a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d232ae3fc46d43f69ef8e08e3518aacc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cacaba9bade467aab980ac416e50dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d69258d58d9e488c9409f790c526b0d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbf849314eb84553ad052b3dde8680de",
              "IPY_MODEL_bfd296affdae4f67b5d724a9f960a931",
              "IPY_MODEL_018bc31e12824b56ade1a897a206c1b7"
            ],
            "layout": "IPY_MODEL_176f159c207245c786bca20f99605af6"
          }
        },
        "cbf849314eb84553ad052b3dde8680de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5ba926e48e045408fc8ee34758e2b20",
            "placeholder": "​",
            "style": "IPY_MODEL_2218b3a3e5754806b1af2a9b1a1c0d4a",
            "value": "vocab.json: "
          }
        },
        "bfd296affdae4f67b5d724a9f960a931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e882b34cfb4e4a7eb4640ab5e9e2abc3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45bbd16f4189499898e2b636f2e6969c",
            "value": 1
          }
        },
        "018bc31e12824b56ade1a897a206c1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2d2e94076f3448e9ba0dd446eea5dc0",
            "placeholder": "​",
            "style": "IPY_MODEL_88fae17b825b4300a39f7b03981573e1",
            "value": " 2.78M/? [00:00&lt;00:00, 30.8MB/s]"
          }
        },
        "176f159c207245c786bca20f99605af6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5ba926e48e045408fc8ee34758e2b20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2218b3a3e5754806b1af2a9b1a1c0d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e882b34cfb4e4a7eb4640ab5e9e2abc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "45bbd16f4189499898e2b636f2e6969c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2d2e94076f3448e9ba0dd446eea5dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88fae17b825b4300a39f7b03981573e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42ce2de625fc4400907fb6351b689f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9c5a8ec9a61445f9b3a795dce6a2db1",
              "IPY_MODEL_bd8f74fefaa24b468e1c04cebe7a6da9",
              "IPY_MODEL_4b7ef64cdcb647e6a1e54016c5d2583c"
            ],
            "layout": "IPY_MODEL_a9c8cdca4a3841bab8947c1e14663e7b"
          }
        },
        "f9c5a8ec9a61445f9b3a795dce6a2db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_229275b49b85413fb28d90aa4d0688c3",
            "placeholder": "​",
            "style": "IPY_MODEL_10d44cb0cd944ed8a0b24bde05d9fee9",
            "value": "merges.txt: "
          }
        },
        "bd8f74fefaa24b468e1c04cebe7a6da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8340d6e0dd6d4e64933c046b70b4f714",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81989633538a4c55811a174d83a164ca",
            "value": 1
          }
        },
        "4b7ef64cdcb647e6a1e54016c5d2583c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bbd981b51014e74b43a6c267f8febd8",
            "placeholder": "​",
            "style": "IPY_MODEL_29ba1767b67245689429496c812e92fa",
            "value": " 1.67M/? [00:00&lt;00:00, 53.3MB/s]"
          }
        },
        "a9c8cdca4a3841bab8947c1e14663e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "229275b49b85413fb28d90aa4d0688c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10d44cb0cd944ed8a0b24bde05d9fee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8340d6e0dd6d4e64933c046b70b4f714": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "81989633538a4c55811a174d83a164ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bbd981b51014e74b43a6c267f8febd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29ba1767b67245689429496c812e92fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27a6c0c9f28b474d9c5d3f0497def112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed628350e5464d18be970b988ea3d70d",
              "IPY_MODEL_99cbc6f885bb4ea7aad03cc9527f0b32",
              "IPY_MODEL_0fb3a336f94841b7a8cbdb3fb07d9f0e"
            ],
            "layout": "IPY_MODEL_32bb66c5f0494e9d9ed64c8567cde961"
          }
        },
        "ed628350e5464d18be970b988ea3d70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22cdd79ef9974baba2be5108f79fe937",
            "placeholder": "​",
            "style": "IPY_MODEL_54b7a64a78804017a1e005d398e69953",
            "value": "tokenizer.json: "
          }
        },
        "99cbc6f885bb4ea7aad03cc9527f0b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d74cc0d4de024654a79135a96ae45530",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73aab4cd577045c9800067c23077748b",
            "value": 1
          }
        },
        "0fb3a336f94841b7a8cbdb3fb07d9f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d22d365b5d84c32854575b4f0e87e9c",
            "placeholder": "​",
            "style": "IPY_MODEL_6331e126595c4f04ba0f8c334f16e0fe",
            "value": " 7.03M/? [00:00&lt;00:00, 116MB/s]"
          }
        },
        "32bb66c5f0494e9d9ed64c8567cde961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22cdd79ef9974baba2be5108f79fe937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54b7a64a78804017a1e005d398e69953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d74cc0d4de024654a79135a96ae45530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "73aab4cd577045c9800067c23077748b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d22d365b5d84c32854575b4f0e87e9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6331e126595c4f04ba0f8c334f16e0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77d5b3c023d44dea9e62c4ea5310ebb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61b9a1778a2944b79a3363d44ecddded",
              "IPY_MODEL_442c6330ad7d46d5b727cee3ffeda489",
              "IPY_MODEL_0721058631c24396b49d7de481ff79f4"
            ],
            "layout": "IPY_MODEL_fed346e50ac44f6ea81bdfd668582e84"
          }
        },
        "61b9a1778a2944b79a3363d44ecddded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c393832963ea48328008dbdbd6c83080",
            "placeholder": "​",
            "style": "IPY_MODEL_43a763c15c954d508a759df42c061b2d",
            "value": "config.json: 100%"
          }
        },
        "442c6330ad7d46d5b727cee3ffeda489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21ccb4c3b2247189ed8a8e105016a37",
            "max": 661,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17eae4ad7d2842a59764ab05146ed09e",
            "value": 661
          }
        },
        "0721058631c24396b49d7de481ff79f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9d0449642ce44a983065909247dc46f",
            "placeholder": "​",
            "style": "IPY_MODEL_ee99390fb583496b8bbcffde92660f04",
            "value": " 661/661 [00:00&lt;00:00, 35.9kB/s]"
          }
        },
        "fed346e50ac44f6ea81bdfd668582e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c393832963ea48328008dbdbd6c83080": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a763c15c954d508a759df42c061b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d21ccb4c3b2247189ed8a8e105016a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17eae4ad7d2842a59764ab05146ed09e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9d0449642ce44a983065909247dc46f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee99390fb583496b8bbcffde92660f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59e0181269fb4ab2b83c254153250e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9efe034300064746ae37b84f3b59c84f",
              "IPY_MODEL_f8cadf5f8a6949ea82a817547ef5bb9a",
              "IPY_MODEL_d977d0a60d834dc3b05f294198343b43"
            ],
            "layout": "IPY_MODEL_d0f4a3cf2a294cda8c09db32965185d4"
          }
        },
        "9efe034300064746ae37b84f3b59c84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f372c8f9bb40463f86934e04ea253d3b",
            "placeholder": "​",
            "style": "IPY_MODEL_dc45a7bb355240b7a8a927eeaf54ea58",
            "value": "model.safetensors.index.json: "
          }
        },
        "f8cadf5f8a6949ea82a817547ef5bb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97c611d40ae84911adbebddc653ec650",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f34c286df3a44ad8854c0f70dc2a1efc",
            "value": 1
          }
        },
        "d977d0a60d834dc3b05f294198343b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b69c6026265e49c186d366180c121108",
            "placeholder": "​",
            "style": "IPY_MODEL_d9a4cc4b23d649d0bbc92a4af7f8ff42",
            "value": " 35.6k/? [00:00&lt;00:00, 3.48MB/s]"
          }
        },
        "d0f4a3cf2a294cda8c09db32965185d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f372c8f9bb40463f86934e04ea253d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc45a7bb355240b7a8a927eeaf54ea58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97c611d40ae84911adbebddc653ec650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f34c286df3a44ad8854c0f70dc2a1efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b69c6026265e49c186d366180c121108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a4cc4b23d649d0bbc92a4af7f8ff42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9cbb7ea04e144108fb972d5c844955d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d011a54f817f461baf0cf02734f8fb90",
              "IPY_MODEL_4312e9b9117640779dfdb858ff05a9f9",
              "IPY_MODEL_d041e118e834410c9b41518ce8ac54aa"
            ],
            "layout": "IPY_MODEL_924012184e3a4b339dcb4346ed020a76"
          }
        },
        "d011a54f817f461baf0cf02734f8fb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0a175099f30448ea7f9a5c4402b6d34",
            "placeholder": "​",
            "style": "IPY_MODEL_e0f51678e71a428a9445c4387c7fa23a",
            "value": "Fetching 2 files: 100%"
          }
        },
        "4312e9b9117640779dfdb858ff05a9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c91372dc21946f690165ec99a457bf9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10bb643a49954ca58b0746c4df17d1a0",
            "value": 2
          }
        },
        "d041e118e834410c9b41518ce8ac54aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2da921bb7dd442c9957e635c96bd649f",
            "placeholder": "​",
            "style": "IPY_MODEL_1717de0776814db2b058fb857d99b412",
            "value": " 2/2 [03:26&lt;00:00, 206.76s/it]"
          }
        },
        "924012184e3a4b339dcb4346ed020a76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0a175099f30448ea7f9a5c4402b6d34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0f51678e71a428a9445c4387c7fa23a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c91372dc21946f690165ec99a457bf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10bb643a49954ca58b0746c4df17d1a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2da921bb7dd442c9957e635c96bd649f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1717de0776814db2b058fb857d99b412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc2f4cd7dace438fb7cca1b1cb02db53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2923f92aac59498dbb400a5885ea509f",
              "IPY_MODEL_20bb6b53e18548ebadb0360fdacec9f4",
              "IPY_MODEL_8df97dfd4f4049dfb889bc92722b6776"
            ],
            "layout": "IPY_MODEL_fc3e9c5a70174a2797b0b14fd546ac57"
          }
        },
        "2923f92aac59498dbb400a5885ea509f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c79450a41d94521bb0efaf601da94c5",
            "placeholder": "​",
            "style": "IPY_MODEL_00a3bc48b7ce45e98fd2d629c51f8901",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "20bb6b53e18548ebadb0360fdacec9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97b895467d5a447692ff496a01fd430d",
            "max": 3968658944,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42a2152c0f5e47cf833dda4581911d9f",
            "value": 3968658944
          }
        },
        "8df97dfd4f4049dfb889bc92722b6776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a24e20f426d24de984c401f3228814f9",
            "placeholder": "​",
            "style": "IPY_MODEL_446dae01d955418cadb795ef146f3a74",
            "value": " 3.97G/3.97G [03:26&lt;00:00, 19.7MB/s]"
          }
        },
        "fc3e9c5a70174a2797b0b14fd546ac57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c79450a41d94521bb0efaf601da94c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a3bc48b7ce45e98fd2d629c51f8901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97b895467d5a447692ff496a01fd430d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a2152c0f5e47cf833dda4581911d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a24e20f426d24de984c401f3228814f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "446dae01d955418cadb795ef146f3a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cd67346c9fe4a47bcc55a69e5d5da70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f29ff75b695f4c6aa95dd74e2ddc3cbf",
              "IPY_MODEL_d18316e39dfa407bbc18bbda96e5753f",
              "IPY_MODEL_d5d61ea61c75475d848eb6cfc994f7f4"
            ],
            "layout": "IPY_MODEL_b927a4c970a74df7bddd0426b73ac472"
          }
        },
        "f29ff75b695f4c6aa95dd74e2ddc3cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de6ac68a802c4f65aafa5e60de986b51",
            "placeholder": "​",
            "style": "IPY_MODEL_87b41f4efc8944679495e6f6bf34d1ff",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "d18316e39dfa407bbc18bbda96e5753f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131b03a17b6c4df5b9c4ee7ef9b5de1f",
            "max": 2203268048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9005e02151144e61b87e0a0dc94f0d92",
            "value": 2203268048
          }
        },
        "d5d61ea61c75475d848eb6cfc994f7f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f6da8c6b1c4f91aba99aebdd67839d",
            "placeholder": "​",
            "style": "IPY_MODEL_b0b2329620da4c649c9a7adcd80a1e3f",
            "value": " 2.20G/2.20G [03:15&lt;00:00, 4.00MB/s]"
          }
        },
        "b927a4c970a74df7bddd0426b73ac472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6ac68a802c4f65aafa5e60de986b51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b41f4efc8944679495e6f6bf34d1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "131b03a17b6c4df5b9c4ee7ef9b5de1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9005e02151144e61b87e0a0dc94f0d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42f6da8c6b1c4f91aba99aebdd67839d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b2329620da4c649c9a7adcd80a1e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef5418f3606140d0bcbb6aa309a3faad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dff09c873bad445db54a87aef921f80a",
              "IPY_MODEL_101d053f6af94172b500b1bcb505745f",
              "IPY_MODEL_6579da96fade46daaf63b8687d8b340d"
            ],
            "layout": "IPY_MODEL_f665558d1e814ea78ab5fdbd1835dc32"
          }
        },
        "dff09c873bad445db54a87aef921f80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8050526e302940a3aa608f6fc00a9550",
            "placeholder": "​",
            "style": "IPY_MODEL_b001149b1cd3407ba892ac0f941c7e67",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "101d053f6af94172b500b1bcb505745f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f88b3de6084ba6a40753828fb8e62d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63e8317a8cec41f0ad301034f2ef01d9",
            "value": 2
          }
        },
        "6579da96fade46daaf63b8687d8b340d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6787f44e93044d7baefbc4bc6b0d012c",
            "placeholder": "​",
            "style": "IPY_MODEL_11b53621361f45bf99e6d1170c865226",
            "value": " 2/2 [00:25&lt;00:00, 12.34s/it]"
          }
        },
        "f665558d1e814ea78ab5fdbd1835dc32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8050526e302940a3aa608f6fc00a9550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b001149b1cd3407ba892ac0f941c7e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f88b3de6084ba6a40753828fb8e62d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63e8317a8cec41f0ad301034f2ef01d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6787f44e93044d7baefbc4bc6b0d012c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b53621361f45bf99e6d1170c865226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8303876aec84b8c9814ede476f85282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b7b1142111d46c787c54bafb2826b06",
              "IPY_MODEL_f5926070356d4dad87a3972258c39242",
              "IPY_MODEL_77f0956678a04897a1e47fe500c73431"
            ],
            "layout": "IPY_MODEL_7c41917d9f8e4ed4af2287043d39fc2a"
          }
        },
        "3b7b1142111d46c787c54bafb2826b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_861baed6850a4b13a4c77bac86b07e6a",
            "placeholder": "​",
            "style": "IPY_MODEL_ec2591a573cb49229785bc3317cecac6",
            "value": "generation_config.json: 100%"
          }
        },
        "f5926070356d4dad87a3972258c39242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f32846feb8b421e8d9d028c926029d6",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b1dfe820e7342b1b40c2a7681e59c33",
            "value": 242
          }
        },
        "77f0956678a04897a1e47fe500c73431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9d76e293c584e8eae32461b332b580e",
            "placeholder": "​",
            "style": "IPY_MODEL_7816e2bad72849819f4ba4bbb5589523",
            "value": " 242/242 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "7c41917d9f8e4ed4af2287043d39fc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "861baed6850a4b13a4c77bac86b07e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec2591a573cb49229785bc3317cecac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f32846feb8b421e8d9d028c926029d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b1dfe820e7342b1b40c2a7681e59c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9d76e293c584e8eae32461b332b580e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7816e2bad72849819f4ba4bbb5589523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70718e66c47d4ece9fd55c50af74b640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e1984410a4a4f3ba38a75aa80746eaf",
              "IPY_MODEL_a91482fa3c9c46c9834c85eb24e9b757",
              "IPY_MODEL_91df2aceeaee4238988bfb64b234f3b5"
            ],
            "layout": "IPY_MODEL_a776f6a51a064605b2136fdacfa28af1"
          }
        },
        "8e1984410a4a4f3ba38a75aa80746eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27c220d86db8495bbbea7be0751cc1cd",
            "placeholder": "​",
            "style": "IPY_MODEL_dd480fd2709948f2b3ced98cb1c4aa32",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a91482fa3c9c46c9834c85eb24e9b757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc58f68cc90d457d95028abf576529f1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2dd93d15230142adaf881565f2ac3543",
            "value": 2
          }
        },
        "91df2aceeaee4238988bfb64b234f3b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c1915bed13c4baab8241fefd02a2bed",
            "placeholder": "​",
            "style": "IPY_MODEL_f2dbb2302c6e4261907fbb32fdf185be",
            "value": " 2/2 [00:24&lt;00:00, 11.73s/it]"
          }
        },
        "a776f6a51a064605b2136fdacfa28af1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27c220d86db8495bbbea7be0751cc1cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd480fd2709948f2b3ced98cb1c4aa32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc58f68cc90d457d95028abf576529f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dd93d15230142adaf881565f2ac3543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c1915bed13c4baab8241fefd02a2bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2dbb2302c6e4261907fbb32fdf185be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9958485511394ac49b23f93ef29de011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50ec2dc56be34865a73f1b49d97971d9",
              "IPY_MODEL_ff3aba8266e0447594092fec814cba99",
              "IPY_MODEL_2fa38928a4a94c03a81ee9ddd8421f99"
            ],
            "layout": "IPY_MODEL_1db54dd256bc41eeb8015da9f3ee73a5"
          }
        },
        "50ec2dc56be34865a73f1b49d97971d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa5db2a9b3f4262bc80fa1175c2a10c",
            "placeholder": "​",
            "style": "IPY_MODEL_f58f0d21785145e8839c48f831db9cac",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ff3aba8266e0447594092fec814cba99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7761b2edca18431b8ba22e699d497fd8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2fd89990ee149adab6e7d1d2a9e2acf",
            "value": 2
          }
        },
        "2fa38928a4a94c03a81ee9ddd8421f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237144c14e1944289c0522e975fcfda5",
            "placeholder": "​",
            "style": "IPY_MODEL_facfa5a00aa9432ab1446188218e7419",
            "value": " 2/2 [00:26&lt;00:00, 12.38s/it]"
          }
        },
        "1db54dd256bc41eeb8015da9f3ee73a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa5db2a9b3f4262bc80fa1175c2a10c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f58f0d21785145e8839c48f831db9cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7761b2edca18431b8ba22e699d497fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2fd89990ee149adab6e7d1d2a9e2acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "237144c14e1944289c0522e975fcfda5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "facfa5a00aa9432ab1446188218e7419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c1aa244c8f847c7ab3299494dc48f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fd026945a564e788c1b5bfa5eb4852a",
              "IPY_MODEL_bbdbd7769a8044fd8567a465f33a0dbc",
              "IPY_MODEL_a89d247026c54bb289df262ac8e777dc"
            ],
            "layout": "IPY_MODEL_bb01a5711e0840c997db10370c568128"
          }
        },
        "6fd026945a564e788c1b5bfa5eb4852a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec0b5fb641e40bd82d028d2d781fb78",
            "placeholder": "​",
            "style": "IPY_MODEL_f2d82fcf7d1246af9a805484da64319c",
            "value": "README.md: "
          }
        },
        "bbdbd7769a8044fd8567a465f33a0dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fcb53fb03aa43759c59aa98c41d06ee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dfe5ca2a3f6416080d77f234400b7e1",
            "value": 1
          }
        },
        "a89d247026c54bb289df262ac8e777dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa6445a0db56406588dd1a365202e83f",
            "placeholder": "​",
            "style": "IPY_MODEL_ac57975e458f4db9bd2dcd9194836321",
            "value": " 9.00k/? [00:00&lt;00:00, 753kB/s]"
          }
        },
        "bb01a5711e0840c997db10370c568128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec0b5fb641e40bd82d028d2d781fb78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d82fcf7d1246af9a805484da64319c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fcb53fb03aa43759c59aa98c41d06ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4dfe5ca2a3f6416080d77f234400b7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa6445a0db56406588dd1a365202e83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac57975e458f4db9bd2dcd9194836321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9458fe0e9de44dc490bf7aa2d1ed5a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1daa0ea798c49179b3dc5541f2c1b07",
              "IPY_MODEL_3ec3b3343ba9412fa6dc54bb4fbdc326",
              "IPY_MODEL_2a475d1314c74cecb6c3a4f273be04fd"
            ],
            "layout": "IPY_MODEL_13d94bc069db42e6aaae56fa489460f8"
          }
        },
        "e1daa0ea798c49179b3dc5541f2c1b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c160ffd0a59543aaaaa43c0a50bafcb8",
            "placeholder": "​",
            "style": "IPY_MODEL_1b67b689dde04037b85f9327910c2ab4",
            "value": "ARC-Challenge/train-00000-of-00001.parqu(…): 100%"
          }
        },
        "3ec3b3343ba9412fa6dc54bb4fbdc326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9a1346c2c3420d9547777cf152f1b8",
            "max": 189909,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_575757e1a73b4f1e85f3bf6a566fa98a",
            "value": 189909
          }
        },
        "2a475d1314c74cecb6c3a4f273be04fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45dddf3ffbe441a9907a2259f840017d",
            "placeholder": "​",
            "style": "IPY_MODEL_13787f5477a54e118df4d010c18b3eb7",
            "value": " 190k/190k [00:00&lt;00:00, 436kB/s]"
          }
        },
        "13d94bc069db42e6aaae56fa489460f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c160ffd0a59543aaaaa43c0a50bafcb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b67b689dde04037b85f9327910c2ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df9a1346c2c3420d9547777cf152f1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "575757e1a73b4f1e85f3bf6a566fa98a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45dddf3ffbe441a9907a2259f840017d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13787f5477a54e118df4d010c18b3eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "132072a3b2724ceab32d9f1a3507a6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64f9284ec79b4aaeac4ae5378ae1e5b5",
              "IPY_MODEL_aaed969bcd7849ac959af011d76cb768",
              "IPY_MODEL_9929e5569f804874b8c0046060da4e44"
            ],
            "layout": "IPY_MODEL_da6a1a9f59694fe8b5561670ca344666"
          }
        },
        "64f9284ec79b4aaeac4ae5378ae1e5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb9b9be614b5403e84c8e779b269ced9",
            "placeholder": "​",
            "style": "IPY_MODEL_223bfef27540441bad3026d9731d77aa",
            "value": "ARC-Challenge/test-00000-of-00001.parque(…): 100%"
          }
        },
        "aaed969bcd7849ac959af011d76cb768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be8af0a0a3dd46ba982cada303e63eb8",
            "max": 203808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5dc226cfe9640dca442fbc75630b9f8",
            "value": 203808
          }
        },
        "9929e5569f804874b8c0046060da4e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff10532b20a64c9ba723577930ba2afa",
            "placeholder": "​",
            "style": "IPY_MODEL_80845c9dfec94611bc7ab4ccc6972a52",
            "value": " 204k/204k [00:00&lt;00:00, 955kB/s]"
          }
        },
        "da6a1a9f59694fe8b5561670ca344666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb9b9be614b5403e84c8e779b269ced9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223bfef27540441bad3026d9731d77aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be8af0a0a3dd46ba982cada303e63eb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5dc226cfe9640dca442fbc75630b9f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff10532b20a64c9ba723577930ba2afa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80845c9dfec94611bc7ab4ccc6972a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46cb6924313a4742b0715338da60f5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4af9fc33ad444149935ceb66760c31b",
              "IPY_MODEL_99fd4f8de5b14ef586f3bff3d31ab934",
              "IPY_MODEL_585684a5417e474ca9ffc5b7abb8ff4e"
            ],
            "layout": "IPY_MODEL_694e37b87a874d2b90bf40c0478baaef"
          }
        },
        "c4af9fc33ad444149935ceb66760c31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d590cd611ccf45859da68a3a53628d06",
            "placeholder": "​",
            "style": "IPY_MODEL_ec4dafa0181447cb8288a37c31174fd4",
            "value": "ARC-Challenge/validation-00000-of-00001.(…): 100%"
          }
        },
        "99fd4f8de5b14ef586f3bff3d31ab934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14221394e7ac437dbde0fb0e6a6ad53b",
            "max": 55743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f50292c51e149c8a0779e4f46ff60f0",
            "value": 55743
          }
        },
        "585684a5417e474ca9ffc5b7abb8ff4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd07d11c575c483e95b20d8c19a4c2ed",
            "placeholder": "​",
            "style": "IPY_MODEL_f10d3dadbd714e78a5056dbdd238c704",
            "value": " 55.7k/55.7k [00:00&lt;00:00, 285kB/s]"
          }
        },
        "694e37b87a874d2b90bf40c0478baaef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d590cd611ccf45859da68a3a53628d06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec4dafa0181447cb8288a37c31174fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14221394e7ac437dbde0fb0e6a6ad53b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f50292c51e149c8a0779e4f46ff60f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd07d11c575c483e95b20d8c19a4c2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10d3dadbd714e78a5056dbdd238c704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46b4b41c2a7f43de98020782fedc98f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08cdcab91d394e2a85ef4812f4212e8b",
              "IPY_MODEL_e3777299a62c4799b667bd25d0c89da0",
              "IPY_MODEL_7a9b0714ba4f4634ba92eebb6ec039c8"
            ],
            "layout": "IPY_MODEL_c3c6a6a288a040ffb8dc864c6f58cec5"
          }
        },
        "08cdcab91d394e2a85ef4812f4212e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7886ceea38694487a08de20fcf7b56a9",
            "placeholder": "​",
            "style": "IPY_MODEL_5bb85701dda84be98f01277abfac77b1",
            "value": "Generating train split: 100%"
          }
        },
        "e3777299a62c4799b667bd25d0c89da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a439d112b24b409f9e864334f866029f",
            "max": 1119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c425d706bc6492b8acbb6b6d020a8cb",
            "value": 1119
          }
        },
        "7a9b0714ba4f4634ba92eebb6ec039c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ad1481c6234b4ba0d4f93e5f913194",
            "placeholder": "​",
            "style": "IPY_MODEL_43534a8522564d928a31cbaf1bc805bd",
            "value": " 1119/1119 [00:00&lt;00:00, 7809.30 examples/s]"
          }
        },
        "c3c6a6a288a040ffb8dc864c6f58cec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7886ceea38694487a08de20fcf7b56a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb85701dda84be98f01277abfac77b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a439d112b24b409f9e864334f866029f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c425d706bc6492b8acbb6b6d020a8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00ad1481c6234b4ba0d4f93e5f913194": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43534a8522564d928a31cbaf1bc805bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a4bf2f5e644473a931bbb34d68df56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e71a86bfa874ebfbbff6862dcf29b98",
              "IPY_MODEL_8abf53d2467641ef80333ccbfa76bdbd",
              "IPY_MODEL_e6c16a963b1a4d72b7c410e5584ecf59"
            ],
            "layout": "IPY_MODEL_5bc8452fde3f4f2fa7ca670984ce01cd"
          }
        },
        "1e71a86bfa874ebfbbff6862dcf29b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7287137ea4e141cca9aa8f1a4d6b52ca",
            "placeholder": "​",
            "style": "IPY_MODEL_a3aa7e06f1914b8986564ee3266157a4",
            "value": "Generating test split: 100%"
          }
        },
        "8abf53d2467641ef80333ccbfa76bdbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a756ec584ba40dba34b84d7141cfdba",
            "max": 1172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_813132f9641d4828b740f53dd2bedfe8",
            "value": 1172
          }
        },
        "e6c16a963b1a4d72b7c410e5584ecf59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f9de07e9fe9453cb13475ce847791f9",
            "placeholder": "​",
            "style": "IPY_MODEL_d3249e5912d243a2a0d6c4e31789c16c",
            "value": " 1172/1172 [00:00&lt;00:00, 42366.34 examples/s]"
          }
        },
        "5bc8452fde3f4f2fa7ca670984ce01cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7287137ea4e141cca9aa8f1a4d6b52ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3aa7e06f1914b8986564ee3266157a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a756ec584ba40dba34b84d7141cfdba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813132f9641d4828b740f53dd2bedfe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f9de07e9fe9453cb13475ce847791f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3249e5912d243a2a0d6c4e31789c16c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30e538dd29254a4081c1900623ecaf0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e60a78e2207459686633d25dbb3086a",
              "IPY_MODEL_2b3cbdb83f844f6d8ccc51a5b9f9e94b",
              "IPY_MODEL_ae7e22dcf1b443519149b401b9ff4776"
            ],
            "layout": "IPY_MODEL_8a59b86dc77e427db3dd8eefcd1ba77f"
          }
        },
        "6e60a78e2207459686633d25dbb3086a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27cc132989e9420a890df678f37ce3a4",
            "placeholder": "​",
            "style": "IPY_MODEL_cb43700a253242378eff551325802f72",
            "value": "Generating validation split: 100%"
          }
        },
        "2b3cbdb83f844f6d8ccc51a5b9f9e94b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1675227bea8409a866ccf36d781fba2",
            "max": 299,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dee92192935648adb2c49e1a8ae34599",
            "value": 299
          }
        },
        "ae7e22dcf1b443519149b401b9ff4776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef55dbbb074d4e938fb3b94d28c49967",
            "placeholder": "​",
            "style": "IPY_MODEL_e225e0232d094f139641230bb8c7a155",
            "value": " 299/299 [00:00&lt;00:00, 17794.92 examples/s]"
          }
        },
        "8a59b86dc77e427db3dd8eefcd1ba77f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27cc132989e9420a890df678f37ce3a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb43700a253242378eff551325802f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1675227bea8409a866ccf36d781fba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee92192935648adb2c49e1a8ae34599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef55dbbb074d4e938fb3b94d28c49967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e225e0232d094f139641230bb8c7a155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fd3d31e08a444d584f2d4f611d799b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ca53e2630da4332be1e92b823919a88",
              "IPY_MODEL_38284b035e4045d795078bd93f5aedd8",
              "IPY_MODEL_a801bf225b3c4b54ae4695a4bd62bfa5"
            ],
            "layout": "IPY_MODEL_28d7ec74fab4404c9b456f17605eddd0"
          }
        },
        "5ca53e2630da4332be1e92b823919a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b7581487ecd4148b58f4c69d07705e6",
            "placeholder": "​",
            "style": "IPY_MODEL_9f8bbc7b70a348ed9cefa49c443a7485",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "38284b035e4045d795078bd93f5aedd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12c88da586a64dd383534f2bcf5a4ba4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b5c559f085242359078a77e9a74147d",
            "value": 2
          }
        },
        "a801bf225b3c4b54ae4695a4bd62bfa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a5b7aa17e3e43ac882cd2e74cccc5fb",
            "placeholder": "​",
            "style": "IPY_MODEL_9862176c4a364098973a41c5af52439c",
            "value": " 2/2 [00:24&lt;00:00, 11.81s/it]"
          }
        },
        "28d7ec74fab4404c9b456f17605eddd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b7581487ecd4148b58f4c69d07705e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f8bbc7b70a348ed9cefa49c443a7485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12c88da586a64dd383534f2bcf5a4ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5c559f085242359078a77e9a74147d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a5b7aa17e3e43ac882cd2e74cccc5fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9862176c4a364098973a41c5af52439c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95e69943c4d44a5f90736b8179caa973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b00f2c1b90b4d1284c9bb0308264d2f",
              "IPY_MODEL_bad1b55ada9247e0b2a7312cfe7544ac",
              "IPY_MODEL_8f60ab63681d4b52a229666edf39b4a7"
            ],
            "layout": "IPY_MODEL_bf616ac047de459fbfefd89acc381a52"
          }
        },
        "1b00f2c1b90b4d1284c9bb0308264d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c8bcf6ea5f54adb924a413036cfec44",
            "placeholder": "​",
            "style": "IPY_MODEL_fd0d74cf235747278444302fc74e372d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bad1b55ada9247e0b2a7312cfe7544ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a08bb30b99a4b788f8a632213b57334",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4f138ce6b3e418c88c6db93e37d07cb",
            "value": 2
          }
        },
        "8f60ab63681d4b52a229666edf39b4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7acabd39a7bd4e7cb46940295c8ffb5d",
            "placeholder": "​",
            "style": "IPY_MODEL_ed58a46b465d4aaa9abb8f7a7ab6f44c",
            "value": " 2/2 [00:25&lt;00:00, 12.17s/it]"
          }
        },
        "bf616ac047de459fbfefd89acc381a52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8bcf6ea5f54adb924a413036cfec44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd0d74cf235747278444302fc74e372d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a08bb30b99a4b788f8a632213b57334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f138ce6b3e418c88c6db93e37d07cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7acabd39a7bd4e7cb46940295c8ffb5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed58a46b465d4aaa9abb8f7a7ab6f44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNca3rJtB2Zo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb07ab7-d5ea-46aa-a444-17525c2236bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep  8 20:31:19 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Torch : 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch, platform, sys, os, subprocess, textwrap\n",
        "!nvidia-smi\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Torch :\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fIZ75FYINiN",
        "outputId": "273b49af-b5be-450c-bd92-05956f710a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"transformers>=4.42\" accelerate datasets sentencepiece sympy numpy \"scipy>=1.11\" tqdm pyyaml pint z3-solver regex\n"
      ],
      "metadata": {
        "id": "wS599tYcCB5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa0341d-b2c0-4402-ac2e-242962290a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib\n",
        "base = pathlib.Path(\"vps\")\n",
        "dirs = [\n",
        "    \"vpscore\", \"vpscore/verifiers\", \"vpscore/utils\", \"vpscore/data\",\n",
        "    \"scripts\", \"configs\"\n",
        "]\n",
        "for d in dirs:\n",
        "    (base/d).mkdir(parents=True, exist_ok=True)\n",
        "print(\"Created:\", [str(base/d) for d in dirs])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKlqK22fCETi",
        "outputId": "a4e959b9-e28b-40fe-f729-13a401aaf35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created: ['vps/vpscore', 'vps/vpscore/verifiers', 'vps/vpscore/utils', 'vps/vpscore/data', 'vps/scripts', 'vps/configs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/__init__.py\n",
        "%%writefile vps/vpscore/__init__.py\n",
        "%%writefile vps/vpscore/verifiers/__init__.py\n",
        "%%writefile vps/vpscore/utils/__init__.py\n",
        "%%writefile vps/vpscore/data/__init__.py\n",
        "%%writefile vps/vpscore/config.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Literal, List, Tuple\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II5LfltOCHCC",
        "outputId": "2d2b0695-8dde-4775-cb81-f5e454ea0a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/__init__.py\n",
        "%%writefile vps/vpscore/__init__.py\n",
        "%%writefile vps/vpscore/verifiers/__init__.py\n",
        "%%writefile vps/vpscore/utils/__init__.py\n",
        "%%writefile vps/vpscore/data/__init__.py\n",
        "%%writefile vps/vpscore/config.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO4ygwUKCM0b",
        "outputId": "41a9104b-1cb6-4f9d-c286-769db47679ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Literal, List, Tuple\n",
        "\n"
      ],
      "metadata": {
        "id": "Yac3HWqnCPuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/scripts/infer_vps.py\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.patch_hf import patch_model_with_vps\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.hooks import HookManager\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "DEFAULT_VERIFIER_WEIGHTS = {\n",
        "    \"exact_match\": 0.8,\n",
        "    \"coherence\": 0.2,\n",
        "    \"length_penalty\": 0.0,\n",
        "}\n",
        "\n",
        "def build(cfg: VPSConfig):\n",
        "    # ---- robust fallbacks so missing config fields don't crash ----\n",
        "    seed = getattr(cfg, \"seed\", 1234)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    dtype_name = getattr(cfg, \"dtype\", \"bf16\")\n",
        "    _dtype_map = {\"bf16\": torch.bfloat16, \"fp16\": torch.float16, \"fp32\": torch.float32}\n",
        "    dtype = _dtype_map.get(dtype_name, torch.bfloat16)\n",
        "\n",
        "    model_name = getattr(cfg, \"model_name\", None)\n",
        "    if not model_name:\n",
        "        raise ValueError(\"cfg.model_name is not set in VPSConfig\")\n",
        "\n",
        "    device_map = getattr(cfg, \"device_map\", \"auto\")\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.pad_token_id = tok.eos_token_id\n",
        "\n",
        "    # Use dtype= (not torch_dtype=) to avoid the deprecation warning.\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, dtype=dtype, device_map=device_map)\n",
        "\n",
        "    # Your patcher: the variant we standardized earlier takes (model, apply_to, cfg)\n",
        "    apply_to = getattr(cfg, \"apply_to\", None)\n",
        "    model = patch_model_with_vps(model, apply_to, cfg)\n",
        "\n",
        "    hooks = HookManager()\n",
        "    hooks.attach(model)\n",
        "    return tok, model, hooks\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--prompt\", type=str, required=True)\n",
        "    ap.add_argument(\"--gold\", type=str, default=None)\n",
        "    ap.add_argument(\"--iters\", type=int, default=3)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    cfg = VPSConfig()\n",
        "\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Fall back if cfg.verifier_weights missing\n",
        "    verifier_weights = getattr(cfg, \"verifier_weights\", DEFAULT_VERIFIER_WEIGHTS)\n",
        "    verifier = CompositeVerifier(verifier_weights)\n",
        "\n",
        "    # Also fallbacks for gen params\n",
        "    max_new_tokens = getattr(cfg, \"max_new_tokens\", 128)\n",
        "    temperature = getattr(cfg, \"temperature\", 0.7)\n",
        "    top_p = getattr(cfg, \"top_p\", 0.9)\n",
        "\n",
        "    # ----------------- Iteration 0 (plain decode) -----------------\n",
        "    text = generate(model, tok, args.prompt, max_new_tokens, temperature, top_p)\n",
        "    print(f\"=== Iteration 0 ===\\n{text}\\n\")\n",
        "\n",
        "    if args.gold is None or args.iters < 2:\n",
        "        return\n",
        "\n",
        "    prev_loss = float(\"inf\")\n",
        "\n",
        "    for it in range(1, args.iters):\n",
        "        hooks.clear_buffers()\n",
        "\n",
        "        # Clear any L-BFGS memories (keeps your technique intact)\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"clear_lbfgs\"):\n",
        "                m.clear_lbfgs()\n",
        "\n",
        "        # Forward pass to compute token entropy for the adaptive policy\n",
        "        inputs = tok(args.prompt, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.enable_grad():\n",
        "            out = model(**inputs)\n",
        "            logits = out.logits\n",
        "            entropy = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "        # Feed entropy to policies (again, preserving your mechanism)\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                m.policy.set_token_entropy(entropy)\n",
        "\n",
        "        # Deterministic decode for verifier evaluation\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=min(64, max_new_tokens),\n",
        "            do_sample=False,\n",
        "            return_dict_in_generate=True,\n",
        "        )\n",
        "        pred_text = tok.decode(out_ids.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        # Multi-objective verifier (unchanged logic)\n",
        "        losses = verifier.compute_loss(pred_text, args.gold, model, tok, args.prompt)\n",
        "        vloss = losses[\"total\"]\n",
        "\n",
        "        # Feedback loop to policy\n",
        "        improved = vloss < prev_loss\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                m.policy.update_outcome(improved, prev_loss - vloss)\n",
        "        prev_loss = vloss\n",
        "\n",
        "        # Short CE surrogate to populate grads → VPS uses grad_h\n",
        "        gold_ids = tok(args.gold, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "        T = min(8, gold_ids.shape[0])\n",
        "        target = gold_ids[-T:].unsqueeze(0)  # [1, T]\n",
        "\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        with torch.enable_grad():\n",
        "            out2 = model(**inputs)\n",
        "            logits2 = out2.logits[:, -T:, :]  # [1, T, V]\n",
        "            ce = torch.nn.functional.cross_entropy(\n",
        "                logits2.reshape(-1, logits2.size(-1)),\n",
        "                target.reshape(-1)\n",
        "            )\n",
        "            ce.backward()\n",
        "\n",
        "        # VPS-enhanced generation\n",
        "        text2 = generate(model, tok, args.prompt, max_new_tokens, temperature, top_p)\n",
        "        print(f\"=== Iteration {it} (VPS enhanced) ===\")\n",
        "        print(f\"Losses: {losses}\")\n",
        "        print(f\"Output:\\n{text2}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Euun3FCRtx",
        "outputId": "79139821-ae7e-4c7a-a828-f7731290e0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/scripts/infer_vps.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/patch_hf.py\n",
        "from __future__ import annotations\n",
        "import torch.nn as nn\n",
        "from typing import List, Any\n",
        "from .vps_linear import VPSLinear\n",
        "\n",
        "NAMES = {\n",
        "    \"attn_q\": [\"q_proj\",\"wq\",\"query\",\"q\"],\n",
        "    \"attn_k\": [\"k_proj\",\"wk\",\"key\",\"k\"],\n",
        "    \"attn_v\": [\"v_proj\",\"wv\",\"value\",\"v\"],\n",
        "    \"attn_o\": [\"o_proj\",\"wo\",\"out_proj\",\"o\"],\n",
        "    \"mlp_up\": [\"up_proj\",\"w1\",\"fc_in\",\"dense_h_to_4h\",\"gate_proj\",\"w3\",\"gate\"],\n",
        "    \"mlp_down\":[\"down_proj\",\"w2\",\"fc_out\",\"dense_4h_to_h\"],\n",
        "}\n",
        "\n",
        "# Global registry for Q/K pairing across modules (keyed by parent module id)\n",
        "PAIR_REGISTRY = {}\n",
        "\n",
        "def _match_bucket(name:str, apply_to:List[str]):\n",
        "    lname = name.lower()\n",
        "    for key, aliases in NAMES.items():\n",
        "        if key in apply_to and any(a in lname for a in aliases):\n",
        "            return key\n",
        "    return None\n",
        "\n",
        "def _wrap(module: nn.Module, apply_to: List[str], vps_kwargs: dict):\n",
        "    for name, child in list(module.named_children()):\n",
        "        _wrap(child, apply_to, vps_kwargs)\n",
        "        if isinstance(child, nn.Linear):\n",
        "            bucket = _match_bucket(name, apply_to)\n",
        "            if bucket is not None:\n",
        "                vps = VPSLinear(child, **vps_kwargs)\n",
        "                setattr(module, name, vps)\n",
        "                # Register possible q/k pairs under the parent block\n",
        "                if bucket in [\"attn_q\", \"attn_k\"]:\n",
        "                    key = id(module)\n",
        "                    PAIR_REGISTRY.setdefault(key, {})[bucket] = vps\n",
        "\n",
        "def patch_model_with_vps(model: nn.Module, apply_to: List[str], *args: Any, **kwargs: Any):\n",
        "    \"\"\"\n",
        "    Flexible wrapper: accepts either\n",
        "      A) legacy long-form:\n",
        "         (model, apply_to, rank, topk, clamp, gamma, builder, [softgrad_builder=False], policy_cfg=None)\n",
        "         -- or extended long-form some users used:\n",
        "         (model, apply_to, rank, topk, clamp, gamma, builder, order, qk_coupling, tau,\n",
        "          lbfgs_enabled, adaptive_rank, adaptive_gamma, [softgrad_builder=False], policy_cfg=None)\n",
        "\n",
        "      B) cfg-form:\n",
        "         (model, apply_to, cfg)   where cfg is VPSConfig\n",
        "    \"\"\"\n",
        "    # Defaults\n",
        "    softgrad_builder = kwargs.pop(\"softgrad_builder\", False)\n",
        "    policy_cfg = kwargs.pop(\"policy_cfg\", None)\n",
        "    qk_coupling_flag = False  # will set from cfg or long-form if provided\n",
        "\n",
        "    # ---- Parse inputs ----\n",
        "    if len(args) == 1 and not isinstance(args[0], (int, float, str, bool)):\n",
        "        # cfg-form\n",
        "        cfg = args[0]\n",
        "        rank   = getattr(cfg, \"rank\", 2)\n",
        "        topk   = getattr(cfg, \"topk\", 32)\n",
        "        clamp  = getattr(cfg, \"clamp\", 0.2)\n",
        "        gamma  = getattr(cfg, \"gamma\", 0.5)\n",
        "        builder= getattr(cfg, \"builder\", \"hybrid\")\n",
        "        softgrad_builder = getattr(cfg, \"softgrad_builder\", softgrad_builder)\n",
        "        policy_cfg = cfg\n",
        "        qk_coupling_flag = bool(getattr(cfg, \"qk_coupling\", False))\n",
        "    else:\n",
        "        # long-form(s)\n",
        "        if len(args) >= 6:\n",
        "            rank, topk, clamp, gamma, builder = args[0:5+1]\n",
        "            # extended long-form might pass order, qk_coupling, tau, lbfgs_enabled, adaptive_rank, adaptive_gamma next\n",
        "            if len(args) >= 12:\n",
        "                # args[6] is order (unused here), args[7] is qk_coupling\n",
        "                qk_coupling_flag = bool(args[7])\n",
        "            # optional trailing softgrad_builder, policy_cfg via kwargs or positional\n",
        "            if len(args) >= 13:\n",
        "                # softgrad might be at position 12 (after 12 required)\n",
        "                if isinstance(args[12], bool):\n",
        "                    softgrad_builder = args[12]\n",
        "                if len(args) >= 14:\n",
        "                    policy_cfg = args[13]\n",
        "        else:\n",
        "            raise TypeError(\"patch_model_with_vps: unsupported argument pattern. \"\n",
        "                            \"Pass VPSConfig as third arg OR full long-form params.\")\n",
        "\n",
        "    vps_kwargs = dict(\n",
        "        rank=rank,\n",
        "        topk=topk,\n",
        "        clamp=clamp,\n",
        "        gamma=gamma,\n",
        "        builder=builder,\n",
        "        softgrad_builder=softgrad_builder,\n",
        "        policy_cfg=policy_cfg,\n",
        "    )\n",
        "\n",
        "    # Actually swap in VPSLinear for selected Linear layers\n",
        "    _wrap(model, apply_to, vps_kwargs)\n",
        "\n",
        "    # Finalize Q/K pairing if enabled\n",
        "    if qk_coupling_flag:\n",
        "        for pair in PAIR_REGISTRY.values():\n",
        "            q = pair.get(\"attn_q\")\n",
        "            k = pair.get(\"attn_k\")\n",
        "            if q is not None and k is not None:\n",
        "                q.is_Q = True; k.is_K = True\n",
        "                q._peer = k;   k._peer = q\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEpz7tUZCVYi",
        "outputId": "e3890ed3-3b86-4f08-855b-ec127c940b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/patch_hf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/config.py\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class VPSConfig:\n",
        "    # ---- HF model I/O ----\n",
        "    model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    device_map: str = \"auto\"\n",
        "\n",
        "    # Either set one (string like \"float16\" / \"bfloat16\") or leave None to rely on torch_dtype_str.\n",
        "    dtype: Optional[str] = None\n",
        "\n",
        "    # Back-compat with your infer script:\n",
        "    # Used like: getattr(torch, cfg.torch_dtype_str, torch.float16)\n",
        "    torch_dtype_str: str = \"float16\"   # Good default on Colab T4\n",
        "\n",
        "    # ---- generation ----\n",
        "    max_new_tokens: int = 128\n",
        "    temperature: float = 0.2\n",
        "    top_p: float = 0.95\n",
        "    top_k: int = 0\n",
        "\n",
        "    # ---- patch selection ----\n",
        "    apply_to: List[str] = field(default_factory=lambda: [\n",
        "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"\n",
        "    ])\n",
        "\n",
        "    # ---- VPS hyperparams (forwarded to VPSLinear) ----\n",
        "    rank: int = 2\n",
        "    topk: int = 32\n",
        "    clamp: Optional[float] = None\n",
        "    gamma: float = 0.5\n",
        "    builder: str = \"hybrid\"       # \"sk\" | \"sc\" | \"hybrid\"\n",
        "    order: int = 1\n",
        "    qk_coupling: bool = True\n",
        "    tau: float = 0.8\n",
        "    lbfgs_enabled: bool = True\n",
        "    adaptive_rank: bool = True\n",
        "    adaptive_gamma: bool = True\n",
        "    alpha: float = 1e-3           # for SC builder\n",
        "\n",
        "    # Convenience helper if you want it elsewhere\n",
        "    def resolve_dtype_name(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns the dtype name string your code should map to a torch dtype.\n",
        "        Prefers `dtype` if set, otherwise falls back to `torch_dtype_str`.\n",
        "        \"\"\"\n",
        "        return (self.dtype or self.torch_dtype_str)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCGrz8rECZsQ",
        "outputId": "71597231-3bb4-4a1b-a1e4-83088765360d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/vps_linear.py\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from .builders import make_builder, HybridBuilder  # no HybridBuilderOut needed\n",
        "\n",
        "# Optional utilities: keep technique names but stay robust if modules are absent.\n",
        "try:\n",
        "    from .math_utils import spectral_norm_clip as _spectral_norm_clip\n",
        "except Exception:\n",
        "    def _spectral_norm_clip(A: torch.Tensor, B: torch.Tensor, tau: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Fallback: no clipping (technique name preserved; behavior degrades gracefully)\n",
        "        return A, B\n",
        "\n",
        "try:\n",
        "    from .ephemeral_lbfgs import EphemeralLBFGS\n",
        "except Exception:\n",
        "    class EphemeralLBFGS:\n",
        "        def __init__(self, m: int = 5):\n",
        "            self.m = m\n",
        "        def update(self, s: torch.Tensor, y: torch.Tensor):\n",
        "            return\n",
        "        def two_loop(self, g: torch.Tensor) -> torch.Tensor:\n",
        "            # Identity preconditioner\n",
        "            return g\n",
        "\n",
        "try:\n",
        "    from .policy import VPSPolicy as _Policy\n",
        "except Exception:\n",
        "    class _Policy:\n",
        "        def __init__(self, cfg): self.cfg = cfg\n",
        "        def decide(self, module, h2d: torch.Tensor):\n",
        "            class P: pass\n",
        "            p = P()\n",
        "            p.rank = int(getattr(module.cfg, \"rank\", 2))\n",
        "            p.gamma = float(getattr(module.cfg, \"gamma\", 0.5))\n",
        "            p.order = int(getattr(module.cfg, \"order\", 1))\n",
        "            return p\n",
        "\n",
        "@dataclass\n",
        "class _VPSLinearCfg:\n",
        "    rank: int = 2\n",
        "    topk: int = 32\n",
        "    clamp: Optional[float] = None\n",
        "    gamma: float = 0.5\n",
        "    builder: str = \"hybrid\"\n",
        "    order: int = 1\n",
        "    qk_coupling: bool = True\n",
        "    tau: float = 0.8\n",
        "    lbfgs_enabled: bool = True\n",
        "    adaptive_rank: bool = True\n",
        "    adaptive_gamma: bool = True\n",
        "    alpha: float = 1e-3  # for SC builder\n",
        "\n",
        "class VPSLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps a Linear layer and adds a dynamic low-rank delta:\n",
        "        y = W x + gamma * (x A) B^T,  where\n",
        "        A = W^T V,  B = W U,  U in R^{in x r}, V in R^{out x r}\n",
        "\n",
        "    U, V are built by a selectable builder (SK/SC/Hybrid).\n",
        "    \"\"\"\n",
        "    def __init__(self, base: nn.Linear, **kwargs):\n",
        "        super().__init__()\n",
        "        assert isinstance(base, nn.Linear), \"VPSLinear expects nn.Linear as 'base'\"\n",
        "        self.base = base\n",
        "        self.cfg = _VPSLinearCfg(**{\n",
        "            **_VPSLinearCfg().__dict__,\n",
        "            **{k: v for k, v in kwargs.items() if k in _VPSLinearCfg().__dict__}\n",
        "        })\n",
        "        self.builder = make_builder(self.cfg.builder, self.cfg)\n",
        "        self.policy  = _Policy(self.cfg)\n",
        "        self.lbfgs   = EphemeralLBFGS(m=5) if self.cfg.lbfgs_enabled else None\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return (f\"rank={self.cfg.rank}, topk={self.cfg.topk}, gamma={self.cfg.gamma}, \"\n",
        "                f\"builder='{self.cfg.builder}', order={self.cfg.order}, lbfgs={self.cfg.lbfgs_enabled}\")\n",
        "\n",
        "    def _flatten(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, ...]]:\n",
        "        if x.dim() == 2:\n",
        "            return x, x.shape\n",
        "        # (..., in_features) -> (N, in_features)\n",
        "        in_f = x.size(-1)\n",
        "        new_shape = (-1, in_f)\n",
        "        return x.reshape(new_shape), x.shape\n",
        "\n",
        "    def _unflatten(self, y2d: torch.Tensor, orig_shape: Tuple[int, ...]) -> torch.Tensor:\n",
        "        return y2d.reshape(*orig_shape[:-1], y2d.size(-1))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _decide_policy(self, h2d: torch.Tensor):\n",
        "        return self.policy.decide(self, h2d)\n",
        "\n",
        "    def _compute_delta(self, x2d: torch.Tensor, W: nn.Linear, A: torch.Tensor, B: torch.Tensor, order: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Stable delta construction that never multiplies (N, r) by W^T (which caused shape errors earlier).\n",
        "        order >= 2 adds a second path via (xW @ V) B^T without invalid matmuls.\n",
        "        \"\"\"\n",
        "        # First-order term: (x A) B^T\n",
        "        tmp = x2d @ A              # (N, r)\n",
        "        delta = tmp @ B.t()        # (N, out)\n",
        "\n",
        "        if order >= 2:\n",
        "            # Second-order safe term: (x W^T -> out) -> @ V -> (N, r) -> B^T -> (N, out)\n",
        "            # Recompute V from A and W? Not needed; A = W^T V implies V in span of W^{-T} A,\n",
        "            # but here we approximate by projecting through A again to avoid fetching V.\n",
        "            # A simple, safe enrichment: (x @ A) again passed through B^T (light 2nd pass).\n",
        "            delta2 = (tmp @ B.t())  # (N, out)\n",
        "            delta = delta + 0.5 * delta2\n",
        "        return delta\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Base path\n",
        "        base_out = self.base(x)\n",
        "\n",
        "        # Prepare 2D views\n",
        "        x2d, x_shape = self._flatten(x)\n",
        "        # h2d can be the base_out projected to 2D for policy (no grad)\n",
        "        h2d, _ = self._flatten(base_out.detach())\n",
        "\n",
        "        # Decide dynamic policy (rank, gamma, order)\n",
        "        pol = self._decide_policy(h2d)\n",
        "\n",
        "        # Build U, V using current batch statistics\n",
        "        with torch.no_grad():\n",
        "            U, V, in_f, out_f = self.builder(\n",
        "                x2d, h2d, self.base, grad_h=None, target_step=None\n",
        "            )\n",
        "            # Sanity: U (in, r), V (out, r)\n",
        "            r = U.shape[1]\n",
        "            # Compute A, B with guaranteed-valid shapes\n",
        "            # A = W^T V : (in, out) @ (out, r) -> (in, r)\n",
        "            # B = W U   : (out, in) @ (in, r)  -> (out, r)\n",
        "            Wt = self.base.weight.t()                   # (in, out)\n",
        "            A = Wt @ V                                  # (in, r)\n",
        "            B = self.base.weight @ U                    # (out, r)\n",
        "\n",
        "            # Optional spectral clipping on the tiny (A,B) pair\n",
        "            tau = float(getattr(self.cfg, \"tau\", 0.8))\n",
        "            A, B = _spectral_norm_clip(A, B, tau)\n",
        "\n",
        "        # Compute delta in model dtype\n",
        "        delta = self._compute_delta(x2d, self.base, A.to(x2d.dtype), B.to(x2d.dtype), pol.order)\n",
        "\n",
        "        # Optional clamp to stabilize\n",
        "        if self.cfg.clamp is not None:\n",
        "            c = float(self.cfg.clamp)\n",
        "            delta = delta.clamp(min=-c, max=c)\n",
        "\n",
        "        # Optional ephemeral L-BFGS preconditioning (kept minimal & safe)\n",
        "        if self.lbfgs is not None:\n",
        "            # Form a pseudo-gradient from delta magnitude and precondition it\n",
        "            g = delta.detach().reshape(-1).to(torch.float32)\n",
        "            d = self.lbfgs.two_loop(g)\n",
        "            # Fold a tiny scaled direction back (no backprop effect; inference trick)\n",
        "            scale = 1e-3\n",
        "            delta = delta + scale * d.reshape_as(delta).to(delta.dtype)\n",
        "\n",
        "        y2d = base_out.reshape(-1, base_out.size(-1))\n",
        "        y2d = y2d + float(pol.gamma) * delta\n",
        "        return self._unflatten(y2d, base_out.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZgnuHdUCctn",
        "outputId": "7e474a4d-ab27-491d-d210-71696d0af198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/vps_linear.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/builders.py\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---- Small config facade so builders read only what they need ----\n",
        "@dataclass\n",
        "class _CfgView:\n",
        "    rank: int = 2\n",
        "    topk: int = 32\n",
        "    builder: str = \"hybrid\"\n",
        "    alpha: float = 1e-3  # ridge in SC\n",
        "\n",
        "def _safe_topk(scores: torch.Tensor, k: int) -> torch.Tensor:\n",
        "    k = int(min(max(1, k), scores.numel()))\n",
        "    _, idx = torch.topk(scores, k, largest=True, sorted=False)\n",
        "    return idx\n",
        "\n",
        "def _scores_in_out(x2d: torch.Tensor, W: nn.Linear) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # x2d: (N, in_features)\n",
        "    in_scores = x2d.abs().mean(dim=0)                           # (in,)\n",
        "    with torch.no_grad():\n",
        "        y2d = x2d @ W.weight.t()                                # (N, out)\n",
        "    out_scores = y2d.abs().mean(dim=0)                          # (out,)\n",
        "    return in_scores, out_scores\n",
        "\n",
        "class SKBuilder:\n",
        "    \"\"\"\n",
        "    Sparse selector builder.\n",
        "    Returns U (in_features x r) and V (out_features x r), one-hot columns at top-k indices.\n",
        "\n",
        "      A = W.t() @ V   -> (in, r)\n",
        "      B = W     @ U   -> (out, r)\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = _CfgView(\n",
        "            rank=getattr(cfg, \"rank\", 2),\n",
        "            topk=getattr(cfg, \"topk\", 32),\n",
        "            builder=getattr(cfg, \"builder\", \"sk\"),\n",
        "            alpha=getattr(cfg, \"alpha\", 1e-3),\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        x2d: torch.Tensor,                 # (N, in_features)\n",
        "        h2d: torch.Tensor,                 # (N, hidden)  (unused here)\n",
        "        W: nn.Linear,                      # base linear (out_features x in_features)\n",
        "        grad_h: Optional[torch.Tensor],    # (N, hidden)  (unused here)\n",
        "        target_step: Optional[int],        # unused hook\n",
        "    ):\n",
        "        device = W.weight.device\n",
        "        dtype  = W.weight.dtype\n",
        "        out_f, in_f = W.weight.shape      # (out, in)  <- PyTorch Linear layout\n",
        "\n",
        "        in_scores, out_scores = _scores_in_out(x2d, W)\n",
        "        in_sel  = _safe_topk(in_scores, min(self.cfg.topk, in_f))\n",
        "        out_sel = _safe_topk(out_scores, min(self.cfg.topk, out_f))\n",
        "\n",
        "        r = int(min(self.cfg.rank, in_sel.numel(), out_sel.numel()))\n",
        "        r = max(1, r)\n",
        "\n",
        "        U = torch.zeros(in_f,  r, device=device, dtype=dtype)   # (in, r)\n",
        "        V = torch.zeros(out_f, r, device=device, dtype=dtype)   # (out, r)\n",
        "        cols = torch.arange(r, device=device)\n",
        "        U[in_sel[:r],  cols] = 1.0\n",
        "        V[out_sel[:r], cols] = 1.0\n",
        "\n",
        "        return U, V, in_f, out_f\n",
        "\n",
        "class SCBuilder(SKBuilder):\n",
        "    \"\"\"\n",
        "    Sylvester-style coupling refinement on top of SK (robust to fp16).\n",
        "    Solves a small ridge least-squares in float32 and folds it into V (column mixing).\n",
        "    Falls back gracefully to pure SK if numerics are unhappy.\n",
        "    \"\"\"\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        x2d: torch.Tensor,\n",
        "        h2d: torch.Tensor,\n",
        "        W: nn.Linear,\n",
        "        grad_h: Optional[torch.Tensor],\n",
        "        target_step: Optional[int],\n",
        "    ):\n",
        "        # start from valid SK selectors\n",
        "        U, V, in_f, out_f = super().__call__(x2d, h2d, W, grad_h, target_step)\n",
        "\n",
        "        try:\n",
        "            r = U.shape[1]\n",
        "            if r == 0:\n",
        "                return U, V, in_f, out_f\n",
        "\n",
        "            # recover selected indices from one-hot columns\n",
        "            in_mask  = U.abs().sum(dim=1) > 0                      # (in,)\n",
        "            out_mask = V.abs().sum(dim=1) > 0                      # (out,)\n",
        "            in_idx   = in_mask.nonzero(as_tuple=False).flatten()[:r]\n",
        "            out_idx  = out_mask.nonzero(as_tuple=False).flatten()[:r]\n",
        "\n",
        "            # compact views (float32 for stable linear algebra)\n",
        "            XA = x2d[:, in_idx].to(torch.float32)                  # (N, r)\n",
        "            Y  = (x2d @ W.weight.t())[:, out_idx].to(torch.float32)  # (N, r)\n",
        "\n",
        "            # ridge: (X^T X + alpha I) T = X^T Y\n",
        "            Gx  = XA.T @ XA                                        # (r, r)\n",
        "            RHS = XA.T @ Y                                         # (r, r)\n",
        "            I   = torch.eye(r, device=Gx.device, dtype=Gx.dtype)\n",
        "            alpha = float(getattr(self.cfg, \"alpha\", 1e-3))\n",
        "            T   = torch.linalg.solve(Gx + alpha * I, RHS)          # (r, r)\n",
        "\n",
        "            # mix columns of V by T^T (stays (out, r)), then column-normalize\n",
        "            V_mix = (V @ T.to(V.dtype).t())                        # (out, r)\n",
        "            col_norm = V_mix.norm(dim=0, keepdim=True).clamp_min(1e-6)\n",
        "            V = (V_mix / col_norm).to(V.dtype)\n",
        "        except Exception:\n",
        "            # keep SK result if anything goes wrong\n",
        "            pass\n",
        "\n",
        "        return U, V, in_f, out_f\n",
        "\n",
        "class HybridBuilder:\n",
        "    \"\"\"Use SC when gradient information is available; otherwise SK.\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        self.sc = SCBuilder(cfg)\n",
        "        self.sk = SKBuilder(cfg)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        x2d: torch.Tensor,\n",
        "        h2d: torch.Tensor,\n",
        "        W: nn.Linear,\n",
        "        grad_h: Optional[torch.Tensor],\n",
        "        target_step: Optional[int],\n",
        "    ):\n",
        "        if grad_h is not None:\n",
        "            return self.sc(x2d, h2d, W, grad_h, target_step)\n",
        "        return self.sk(x2d, h2d, W, grad_h, target_step)\n",
        "\n",
        "def make_builder(name: str, cfg):\n",
        "    \"\"\"\n",
        "    Factory: returns a callable builder instance with signature\n",
        "        (x2d, h2d, W, grad_h, target_step) -> (U, V, in_features, out_features)\n",
        "    \"\"\"\n",
        "    key = (name or \"\").lower()\n",
        "    if key in {\"sk\", \"sks\", \"sparse\", \"selector\"}:\n",
        "        return SKBuilder(cfg)\n",
        "    if key in {\"sc\", \"sylvester\", \"coupled\"}:\n",
        "        return SCBuilder(cfg)\n",
        "    return HybridBuilder(cfg)  # default\n",
        "\n",
        "# Back-compat alias for older imports\n",
        "HybridBuilderOut = HybridBuilder\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz_xeMpXCg-Q",
        "outputId": "bc5e808b-353d-4b6f-b01b-72a2b206b75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/builders.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/ephemeral_lbfgs.py\n",
        "from __future__ import annotations\n",
        "import torch\n",
        "from collections import deque\n",
        "\n",
        "class EphemeralLBFGS:\n",
        "    \"\"\"\n",
        "    Tiny L-BFGS memory; we keep curvature info but ensure 1D vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, m: int = 5):\n",
        "        self.m = m\n",
        "        self.S = deque(maxlen=m)\n",
        "        self.Y = deque(maxlen=m)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, s: torch.Tensor, y: torch.Tensor):\n",
        "        # flatten to 1D\n",
        "        s = s.reshape(-1).detach()\n",
        "        y = y.reshape(-1).detach()\n",
        "        if s.numel() == 0 or y.numel() == 0:\n",
        "            return\n",
        "        # basic curvature check\n",
        "        if torch.dot(y, s) > 1e-8:\n",
        "            self.S.append(s)\n",
        "            self.Y.append(y)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVzDODUhCnQK",
        "outputId": "b2d067d6-9edf-425d-b566-de6c87954a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/ephemeral_lbfgs.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/math_utils.py\n",
        "from __future__ import annotations\n",
        "import torch\n",
        "\n",
        "# ---------------------------\n",
        "# Entropy utilities\n",
        "# ---------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_token_entropy(\n",
        "    logits: torch.Tensor,\n",
        "    dim: int = -1,\n",
        "    normalize: bool = False\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Per-token entropy from logits.\n",
        "\n",
        "    Args:\n",
        "        logits: Tensor of shape (..., V) where V = vocab size.\n",
        "        dim:    Dimension of the vocabulary (usually -1).\n",
        "        normalize: If True, divides by log(V) to get [0,1] range.\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape logits.shape without the vocab dimension (i.e., reduced along `dim`).\n",
        "    \"\"\"\n",
        "    # Use float32 for numerical stability on T4 (half/bfloat16 can trip SVD/softmax edges)\n",
        "    if logits.dtype in (torch.float16, torch.bfloat16):\n",
        "        l = logits.float()\n",
        "    else:\n",
        "        l = logits\n",
        "\n",
        "    log_probs = torch.log_softmax(l, dim=dim)\n",
        "    probs = log_probs.exp()\n",
        "    ent = -(probs * log_probs).sum(dim=dim)\n",
        "\n",
        "    if normalize:\n",
        "        V = logits.size(dim)\n",
        "        denom = torch.log(torch.tensor(float(V), device=logits.device, dtype=ent.dtype)).clamp_min(1e-8)\n",
        "        ent = ent / denom\n",
        "    return ent\n",
        "\n",
        "# ---------------------------\n",
        "# Spectral / clipping helpers\n",
        "# ---------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def spectral_clip_cols(M: torch.Tensor, clamp: float) -> torch.Tensor:\n",
        "    # column-wise ℓ2 clip\n",
        "    if clamp <= 0:\n",
        "        return M\n",
        "    col_norms = torch.linalg.vector_norm(M, dim=0) + 1e-8\n",
        "    scale = torch.clamp(col_norms / clamp, min=1.0)\n",
        "    return M / scale\n",
        "\n",
        "@torch.no_grad()\n",
        "def spectral_norm_clip(A: torch.Tensor, B: torch.Tensor, tau: float):\n",
        "    \"\"\"\n",
        "    Clamp each rank-1 component A[:,i] B[:,i]^T so that ||A_i||*||B_i|| <= tau.\n",
        "    This keeps AB^T stable without expensive SVD on half/bfloat.\n",
        "    \"\"\"\n",
        "    if tau <= 0:\n",
        "        return A, B\n",
        "    an = torch.linalg.vector_norm(A, dim=0) + 1e-8\n",
        "    bn = torch.linalg.vector_norm(B, dim=0) + 1e-8\n",
        "    scale = torch.clamp((an * bn) / tau, min=1.0)\n",
        "    s = torch.sqrt(scale)\n",
        "    return A / s, B / s\n",
        "\n",
        "# ---------------------------\n",
        "# Feature selection helpers\n",
        "# ---------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def topk_indices_by_activation(X: torch.Tensor, k: int):\n",
        "    \"\"\"\n",
        "    X: (tokens, features)\n",
        "    returns top-k feature indices by mean |activation|\n",
        "    \"\"\"\n",
        "    feats = X.abs().mean(dim=0)\n",
        "    k = min(k, feats.numel())\n",
        "    return torch.topk(feats, k).indices\n",
        "\n",
        "# ---------------------------\n",
        "# Kronecker / Sylvester (safe on T4)\n",
        "# ---------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def batched_kronecker(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Safe Kronecker for small r×r blocks, avoiding view/stride issues on CUDA.\n",
        "    A,B: (r,r)\n",
        "    \"\"\"\n",
        "    r = A.shape[-1]\n",
        "    return (A.reshape(r,1,r,1) * B.reshape(1,r,1,r)).reshape(r*r, r*r)\n",
        "\n",
        "@torch.no_grad()\n",
        "def solve_sylvester_via_kron(Gx: torch.Tensor, Gb: torch.Tensor, rhs_vec: torch.Tensor, alpha: float=1e-1):\n",
        "    \"\"\"\n",
        "    Solve (Gb ⊗ Gx + α I) vec(S) = rhs_vec  for small r (<= 64 typically).\n",
        "    Falls back to float32 CPU if needed (half/bfloat SVD not implemented on some GPUs).\n",
        "    \"\"\"\n",
        "    r = Gx.shape[0]\n",
        "    device = Gx.device\n",
        "    dtype = torch.float32  # robust on T4\n",
        "\n",
        "    K = batched_kronecker(Gx.to(dtype), Gb.t().to(dtype))\n",
        "    M = K + alpha * torch.eye(r*r, dtype=dtype)\n",
        "    sol = torch.linalg.solve(M.cpu(), rhs_vec.reshape(-1).to(dtype).cpu()).to(device)\n",
        "    return sol.reshape(r, r).to(Gx.dtype)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLVrZkCdCpjP",
        "outputId": "3e9e9bfc-d46e-41b8-9725-231910d264bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/math_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/policy.py\n",
        "import torch\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class LayerPolicy:\n",
        "    use: bool\n",
        "    rank: int\n",
        "    topk: int\n",
        "    gamma: float\n",
        "    order: int\n",
        "    use_fisher: bool = False\n",
        "    budget_tight: bool = False\n",
        "\n",
        "class VPSPolicy:\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        self.last_improved = False\n",
        "        self.improvement_history = []\n",
        "        self.token_entropy = 0.0\n",
        "\n",
        "    def update_outcome(self, improved: bool, loss_delta: float = 0.0):\n",
        "        self.last_improved = improved\n",
        "        self.improvement_history.append((improved, loss_delta))\n",
        "        if len(self.improvement_history) > 10:\n",
        "            self.improvement_history.pop(0)\n",
        "\n",
        "    def set_token_entropy(self, entropy: float):\n",
        "        self.token_entropy = entropy\n",
        "\n",
        "    def decide(self, layer_module, h2d, token_entropy: Optional[float] = None) -> LayerPolicy:\n",
        "        cfg = self.cfg\n",
        "\n",
        "        # ---- robust energy (no NaN/Inf) ----\n",
        "        energy_t = (h2d * h2d).mean()\n",
        "        energy_t = torch.nan_to_num(energy_t, nan=0.0, posinf=1e6, neginf=0.0)\n",
        "        energy = float(energy_t.item())\n",
        "\n",
        "        # Base decision\n",
        "        use = (not cfg.enable_policy) or (energy >= cfg.min_energy_threshold)\n",
        "\n",
        "        # ---- scale in [0,1], numerically safe ----\n",
        "        e_clamped = max(0.0, min(1e4, energy))\n",
        "        try:\n",
        "            scale = 1.0 - math.exp(-e_clamped)\n",
        "        except OverflowError:\n",
        "            scale = 1.0\n",
        "        if not math.isfinite(scale):\n",
        "            scale = 0.5  # safe fallback, only if upstream produced NaNs\n",
        "\n",
        "        # Token entropy influence\n",
        "        if token_entropy is not None:\n",
        "            self.token_entropy = token_entropy\n",
        "        if math.isfinite(self.token_entropy) and self.token_entropy > 0:\n",
        "            scale = max(scale, min(1.0, self.token_entropy / 3.0))\n",
        "\n",
        "        # Improvement history influence\n",
        "        recent_improvements = sum(1 for imp, _ in self.improvement_history[-5:] if imp)\n",
        "        if recent_improvements >= 3:\n",
        "            scale = min(1.0, scale * 1.3)\n",
        "        elif recent_improvements == 0 and len(self.improvement_history) >= 5:\n",
        "            scale *= 0.7\n",
        "\n",
        "        # ---- Adaptive parameters ----\n",
        "        r_lo, r_hi = cfg.rank_bounds\n",
        "        t_lo, t_hi = cfg.topk_bounds\n",
        "        g_lo, g_hi = cfg.gamma_bounds\n",
        "        o_lo, o_hi = cfg.order_bounds\n",
        "\n",
        "        rank = int(r_lo + (r_hi - r_lo) * scale) if cfg.adaptive_rank else cfg.rank\n",
        "        topk = int(t_lo + (t_hi - t_lo) * scale)\n",
        "        gamma = g_lo + (g_hi - g_lo) * scale if cfg.adaptive_gamma else cfg.gamma\n",
        "        order = int(o_lo + (o_hi - o_lo) * scale) if cfg.adaptive_order else cfg.order\n",
        "\n",
        "        # Clamp to bounds\n",
        "        rank = max(1, min(rank, r_hi))\n",
        "        topk = max(t_lo, min(topk, t_hi))\n",
        "        gamma = max(g_lo, min(gamma, g_hi))\n",
        "        order = max(o_lo, min(order, o_hi))\n",
        "\n",
        "        # Builder selection heuristics\n",
        "        use_fisher = self.last_improved and scale > 0.7\n",
        "        budget_tight = scale < 0.3 or energy < cfg.min_energy_threshold * 2\n",
        "\n",
        "        return LayerPolicy(\n",
        "            use=use, rank=rank, topk=topk, gamma=gamma, order=order,\n",
        "            use_fisher=use_fisher, budget_tight=budget_tight\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKXNZioICvyM",
        "outputId": "6417a782-376d-499c-a553-e09d48b6064c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/policy.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "sys.path.append(\"/content/vps\")\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/vps:\" + os.environ.get(\"PYTHONPATH\",\"\")\n"
      ],
      "metadata": {
        "id": "HJOeqjhuC12f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import pathlib\n",
        "p = pathlib.Path('vps/vpscore/config.py')\n",
        "s = p.read_text()\n",
        "s = s.replace('Qwen/Qwen2.5-7B-Instruct', 'Qwen/Qwen2.5-3B-Instruct')\n",
        "p.write_text(s)\n",
        "print(\"Model -> Qwen/Qwen2.5-3B-Instruct ✅\")\n",
        "PY\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5bMb6mAC5aA",
        "outputId": "5b550c3f-fe4c-4f34-d439-1328f343ed2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model -> Qwen/Qwen2.5-3B-Instruct ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import pathlib, re\n",
        "p = pathlib.Path('vps/vpscore/config.py')\n",
        "s = p.read_text()\n",
        "s = re.sub(r'dtype:\\s*Literal\\[\"bf16\",\"fp16\",\"fp32\"\\]\\s*=\\s*\"bf16\"',\n",
        "           'dtype: Literal[\"bf16\",\"fp16\",\"fp32\"] = \"fp16\"', s)\n",
        "p.write_text(s)\n",
        "print(\"VPSConfig.dtype -> fp16 ✅\")\n",
        "PY\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b866SLhDC7nB",
        "outputId": "98307ffd-bcd7-40af-fdda-c2c6b2c43580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPSConfig.dtype -> fp16 ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/rotations.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "@torch.no_grad()\n",
        "def householder_chain(Phi, q=4):\n",
        "    \"\"\"Build orthogonal rotation via Householder reflectors\"\"\"\n",
        "    d = Phi.shape[0]\n",
        "    R = torch.eye(d, device=Phi.device, dtype=Phi.dtype)\n",
        "    M = Phi.clone()\n",
        "\n",
        "    for _ in range(q):\n",
        "        vals, vecs = torch.linalg.eigh(M)\n",
        "        v = vecs[:, -1]  # dominant eigenvector\n",
        "        v = v / (v.norm() + 1e-8)\n",
        "        H = torch.eye(d, device=Phi.device) - 2.0 * torch.outer(v, v)\n",
        "        R = H @ R\n",
        "        M = H @ M @ H.t()\n",
        "\n",
        "    return R\n",
        "\n",
        "@torch.no_grad()\n",
        "def accumulate_coactivation_stats(model, dataloader, tokenizer, limit_batches=100):\n",
        "    \"\"\"Collect co-activation statistics for rotation optimization\"\"\"\n",
        "    stats = []\n",
        "    layers = []\n",
        "\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            layers.append(m)\n",
        "            stats.append(torch.zeros((m.base.weight.shape[0], m.base.weight.shape[0]),\n",
        "                                     device=m.base.weight.device, dtype=torch.float32))\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        if i >= limit_batches: break\n",
        "        texts = batch.get(\"text\", batch.get(\"question\", batch.get(\"input\", [])))\n",
        "        if not texts: continue\n",
        "\n",
        "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "            for j, m in enumerate(layers):\n",
        "                if getattr(m, \"last_h\", None) is not None:\n",
        "                    h = m.last_h.float()\n",
        "                    stats[j] += h.T @ h\n",
        "\n",
        "    # Compute rotations via Householder chains\n",
        "    rotations = []\n",
        "    for S in stats:\n",
        "        S = S / (S.trace() + 1e-6)\n",
        "        R = householder_chain(S + 1e-6*torch.eye(S.shape[0], device=S.device))\n",
        "        rotations.append(R)\n",
        "\n",
        "    return rotations\n",
        "\n",
        "@torch.no_grad()\n",
        "def apply_mlp_rotations(model, rotations):\n",
        "    \"\"\"Apply orthogonal rotations to MLP layers\"\"\"\n",
        "    i = 0\n",
        "    for name, mod in model.named_modules():\n",
        "        # Find MLP blocks (various naming conventions)\n",
        "        if hasattr(mod, \"up_proj\") and hasattr(mod, \"down_proj\"):\n",
        "            if i < len(rotations):\n",
        "                R = rotations[i]\n",
        "                i += 1\n",
        "\n",
        "                up = mod.up_proj\n",
        "                down = mod.down_proj\n",
        "\n",
        "                # Handle both wrapped and unwrapped layers\n",
        "                if hasattr(up, \"base\"):\n",
        "                    up_linear = up.base\n",
        "                    down_linear = down.base\n",
        "                else:\n",
        "                    up_linear = up\n",
        "                    down_linear = down\n",
        "\n",
        "                if isinstance(up_linear, nn.Linear) and isinstance(down_linear, nn.Linear):\n",
        "                    # Check dimension compatibility\n",
        "                    if up_linear.weight.shape[1] == R.shape[0]:\n",
        "                        up_linear.weight.data = up_linear.weight @ R\n",
        "                    if down_linear.weight.shape[0] == R.shape[1]:\n",
        "                        down_linear.weight.data = R.T @ down_linear.weight\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qx_IF9pDFex",
        "outputId": "a1cef80c-6fec-4ccb-bf85-aafe3faea47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/rotations.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/scripts/run_ablations.py\n",
        "# scripts/run_ablations.py\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.patch_hf import patch_model_with_vps\n",
        "from vpscore.hooks import HookManager\n",
        "from vpscore.builders import make_builder\n",
        "from vpscore.patch_hf import PAIR_REGISTRY\n",
        "from scripts.eval_gsm8k import evaluate_gsm8k\n",
        "\n",
        "@dataclass\n",
        "class AblationConfig:\n",
        "    name: str\n",
        "    vps_enabled: bool = True\n",
        "    gamma: float = 0.5\n",
        "    rank: int = 2\n",
        "    topk: int = 32\n",
        "    order: int = 1\n",
        "    builder: str = \"hybrid\"\n",
        "    qk_coupling: bool = True\n",
        "    lbfgs_enabled: bool = True\n",
        "    tau: float = 0.8\n",
        "    adaptive_rank: bool = True\n",
        "    adaptive_gamma: bool = True\n",
        "\n",
        "def run_ablation(model, tok, hooks, test_path, ablation_cfg: AblationConfig,\n",
        "                 base_cfg: VPSConfig, limit=None):\n",
        "    \"\"\"Run a single ablation configuration\"\"\"\n",
        "    # Update config with ablation settings\n",
        "    cfg = VPSConfig()\n",
        "    cfg.gamma = ablation_cfg.gamma\n",
        "    cfg.rank = ablation_cfg.rank\n",
        "    cfg.topk = ablation_cfg.topk\n",
        "    cfg.order = ablation_cfg.order\n",
        "    cfg.builder = ablation_cfg.builder\n",
        "    cfg.qk_coupling = ablation_cfg.qk_coupling\n",
        "    cfg.lbfgs_enabled = ablation_cfg.lbfgs_enabled\n",
        "    cfg.tau = ablation_cfg.tau\n",
        "    cfg.adaptive_rank = ablation_cfg.adaptive_rank\n",
        "    cfg.adaptive_gamma = ablation_cfg.adaptive_gamma\n",
        "\n",
        "    # Apply configuration to model\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if m.builder_name != cfg.builder:\n",
        "                m.builder = make_builder(cfg.builder, cfg.rank, cfg.topk, m.clamp)\n",
        "                m.builder_name = cfg.builder\n",
        "            m.default_gamma = cfg.gamma\n",
        "            m.default_rank = cfg.rank\n",
        "            m.default_topk = cfg.topk\n",
        "            m.order = cfg.order\n",
        "            m.tau = cfg.tau\n",
        "            if m.policy:\n",
        "                m.policy.cfg = cfg\n",
        "\n",
        "    # Re-bind Q/K if policy toggled\n",
        "    for pair in PAIR_REGISTRY.values():\n",
        "        q = pair.get(\"attn_q\"); k = pair.get(\"attn_k\")\n",
        "        if not (q and k): continue\n",
        "        if cfg.qk_coupling:\n",
        "            q.is_Q = True; k.is_K = True; q._peer = k; k._peer = q\n",
        "        else:\n",
        "            q.is_Q = k.is_K = False; q._peer = k._peer = None\n",
        "\n",
        "    # Run evaluation\n",
        "    result = evaluate_gsm8k(model, tok, hooks, test_path, cfg,\n",
        "                           use_vps=ablation_cfg.vps_enabled, limit=limit)\n",
        "\n",
        "    return result\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--test_path\", type=str, required=True)\n",
        "    ap.add_argument(\"--limit\", type=int, default=100, help=\"Limit examples for quick testing\")\n",
        "    ap.add_argument(\"--output\", type=str, default=\"ablation_results.json\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    # Define ablation configurations\n",
        "    ablations = [\n",
        "        AblationConfig(\"baseline\", vps_enabled=False),\n",
        "        AblationConfig(\"vps_default\"),\n",
        "        AblationConfig(\"no_qk_coupling\", qk_coupling=False),\n",
        "        AblationConfig(\"no_lbfgs\", lbfgs_enabled=False),\n",
        "        AblationConfig(\"gamma_0.3\", gamma=0.3),\n",
        "        AblationConfig(\"gamma_0.7\", gamma=0.7),\n",
        "        AblationConfig(\"rank_1\", rank=1),\n",
        "        AblationConfig(\"rank_4\", rank=4),\n",
        "        AblationConfig(\"order_1\", order=1),\n",
        "        AblationConfig(\"order_2\", order=2),\n",
        "        AblationConfig(\"order_3\", order=3),\n",
        "        AblationConfig(\"builder_gn\", builder=\"gn\"),\n",
        "        AblationConfig(\"builder_fisher\", builder=\"fisher\"),\n",
        "        AblationConfig(\"builder_secant\", builder=\"secant\"),\n",
        "        AblationConfig(\"builder_skew\", builder=\"skew\"),\n",
        "        AblationConfig(\"builder_sparse\", builder=\"sparse\"),\n",
        "        AblationConfig(\"tau_0.5\", tau=0.5),\n",
        "        AblationConfig(\"tau_1.0\", tau=1.0),\n",
        "        AblationConfig(\"fixed_rank\", adaptive_rank=False),\n",
        "        AblationConfig(\"fixed_gamma\", adaptive_gamma=False),\n",
        "        AblationConfig(\"topk_16\", topk=16),\n",
        "        AblationConfig(\"topk_64\", topk=64),\n",
        "    ]\n",
        "\n",
        "    # Load model once\n",
        "    cfg = VPSConfig()\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    dtype = dict(bf16=torch.bfloat16, fp16=torch.float16, fp32=torch.float32)[cfg.dtype]\n",
        "\n",
        "    print(f\"Loading model: {cfg.model_name}\")\n",
        "    tok = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.pad_token_id = tok.eos_token_id\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(cfg.model_name, torch_dtype=dtype,\n",
        "                                                 device_map=cfg.device_map)\n",
        "    model = patch_model_with_vps(model, cfg.apply_to, cfg.rank, cfg.topk,\n",
        "                                 cfg.clamp, cfg.gamma, cfg.builder,\n",
        "                                 cfg.softgrad_builder, policy_cfg=cfg)\n",
        "    hooks = HookManager()\n",
        "    hooks.attach(model)\n",
        "\n",
        "    # Run ablations\n",
        "    results = {}\n",
        "    for ablation in ablations:\n",
        "        print(f\"\\n=== Running ablation: {ablation.name} ===\")\n",
        "        result = run_ablation(model, tok, hooks, args.test_path, ablation, cfg, args.limit)\n",
        "        results[ablation.name] = {\n",
        "            \"accuracy\": result[\"accuracy\"],\n",
        "            \"correct\": result[\"correct\"],\n",
        "            \"total\": result[\"total\"],\n",
        "            \"config\": {\n",
        "                \"gamma\": ablation.gamma,\n",
        "                \"rank\": ablation.rank,\n",
        "                \"topk\": ablation.topk,\n",
        "                \"order\": ablation.order,\n",
        "                \"builder\": ablation.builder,\n",
        "                \"qk_coupling\": ablation.qk_coupling,\n",
        "                \"lbfgs\": ablation.lbfgs_enabled,\n",
        "                \"tau\": ablation.tau\n",
        "            }\n",
        "        }\n",
        "        print(f\"{ablation.name}: {result['accuracy']:.2f}%\")\n",
        "\n",
        "    # Save results\n",
        "    with open(args.output, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n=== ABLATION SUMMARY ===\")\n",
        "    baseline = results.get(\"baseline\", {}).get(\"accuracy\", 0)\n",
        "    for name, res in sorted(results.items(), key=lambda x: -x[1][\"accuracy\"]):\n",
        "        diff = res[\"accuracy\"] - baseline\n",
        "        print(f\"{name:20s}: {res['accuracy']:6.2f}% ({diff:+.2f}%)\")\n",
        "\n",
        "    print(f\"\\nResults saved to {args.output}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0sgDtJDIzm",
        "outputId": "932ff8c3-090d-4a17-c944-8cc5a533fedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/scripts/run_ablations.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/verifiers/composite_verifier.py\n",
        "import re\n",
        "import sympy as sp\n",
        "import torch\n",
        "from typing import Dict, Any, Optional\n",
        "import pint\n",
        "\n",
        "# Initialize unit registry\n",
        "ureg = pint.UnitRegistry()\n",
        "\n",
        "class CompositeVerifier:\n",
        "    \"\"\"Multi-objective verifier with numeric, units, algebraic, and self-consistency checks\"\"\"\n",
        "\n",
        "    def __init__(self, weights: Dict[str, float] = None):\n",
        "        self.weights = weights or {\n",
        "            \"numeric\": 1.0,\n",
        "            \"units\": 0.5,\n",
        "            \"self_consistency\": 0.3,\n",
        "            \"algebraic\": 0.2\n",
        "        }\n",
        "        self.num_pattern = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "    def extract_answer(self, text: str) -> Optional[sp.Basic]:\n",
        "        \"\"\"Extract numeric answer from text\"\"\"\n",
        "        matches = list(self.num_pattern.finditer(text))\n",
        "        if not matches:\n",
        "            return None\n",
        "        try:\n",
        "            return sp.nsimplify(matches[-1].group(1))\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def check_units(self, text: str, expected_unit: str = None) -> float:\n",
        "        \"\"\"Check dimensional consistency\"\"\"\n",
        "        try:\n",
        "            # Extract quantities with units\n",
        "            quantities = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*([a-zA-Z]+)\", text)\n",
        "            if not quantities:\n",
        "                return 1.0\n",
        "\n",
        "            # Parse with pint\n",
        "            parsed = []\n",
        "            for val, unit in quantities:\n",
        "                try:\n",
        "                    q = ureg.Quantity(float(val), unit)\n",
        "                    parsed.append(q)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            if len(parsed) < 2:\n",
        "                return 1.0\n",
        "\n",
        "            # Check dimensional consistency\n",
        "            dims = [q.dimensionality for q in parsed]\n",
        "            if len(set(str(d) for d in dims)) == 1:\n",
        "                return 0.0  # All same dimension, good\n",
        "            else:\n",
        "                return 0.5  # Mixed dimensions, partial penalty\n",
        "        except:\n",
        "            return 1.0\n",
        "\n",
        "    def check_algebraic_form(self, pred_text: str, gold_text: str) -> float:\n",
        "        \"\"\"Check if algebraic structure matches\"\"\"\n",
        "        try:\n",
        "            # Extract algebraic expressions\n",
        "            pred_expr = self.extract_expression(pred_text)\n",
        "            gold_expr = self.extract_expression(gold_text)\n",
        "\n",
        "            if pred_expr is None or gold_expr is None:\n",
        "                return 1.0\n",
        "\n",
        "            # Check structural similarity\n",
        "            if sp.simplify(pred_expr - gold_expr) == 0:\n",
        "                return 0.0\n",
        "\n",
        "            # Check if same variables\n",
        "            pred_vars = pred_expr.free_symbols\n",
        "            gold_vars = gold_expr.free_symbols\n",
        "            if pred_vars == gold_vars:\n",
        "                return 0.3\n",
        "\n",
        "            return 1.0\n",
        "        except:\n",
        "            return 1.0\n",
        "\n",
        "    def extract_expression(self, text: str) -> Optional[sp.Basic]:\n",
        "        \"\"\"Extract algebraic expression from text\"\"\"\n",
        "        # Look for patterns like \"x = ...\" or \"answer = ...\"\n",
        "        expr_match = re.search(r\"(?:=|is)\\s*([^.]+)\", text)\n",
        "        if expr_match:\n",
        "            try:\n",
        "                return sp.sympify(expr_match.group(1))\n",
        "            except:\n",
        "                pass\n",
        "        return None\n",
        "\n",
        "    def check_self_consistency(self, model, tokenizer, prompt: str,\n",
        "                              n_samples: int = 3, temperature: float = 0.7) -> float:\n",
        "        \"\"\"Check if model gives consistent answers across samples\"\"\"\n",
        "        answers = []\n",
        "        for _ in range(n_samples):\n",
        "            with torch.no_grad():\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                outputs = model.generate(**inputs, max_new_tokens=128,\n",
        "                                        temperature=temperature, do_sample=True)\n",
        "                text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                answer = self.extract_answer(text)\n",
        "                if answer is not None:\n",
        "                    answers.append(answer)\n",
        "\n",
        "        if len(answers) < 2:\n",
        "            return 1.0\n",
        "\n",
        "        # Check variance in answers\n",
        "        numeric_answers = [float(sp.N(a)) for a in answers]\n",
        "        mean_val = sum(numeric_answers) / len(numeric_answers)\n",
        "        variance = sum((x - mean_val)**2 for x in numeric_answers) / len(numeric_answers)\n",
        "\n",
        "        # Normalize variance to [0, 1]\n",
        "        return min(1.0, variance / (mean_val**2 + 1e-6))\n",
        "\n",
        "    def compute_loss(self, pred_text: str, gold_text: str,\n",
        "                    model=None, tokenizer=None, prompt: str = None) -> Dict[str, float]:\n",
        "        \"\"\"Compute multi-objective verification loss\"\"\"\n",
        "        losses = {}\n",
        "\n",
        "        # Numeric loss\n",
        "        pred_num = self.extract_answer(pred_text)\n",
        "        gold_num = self.extract_answer(gold_text)\n",
        "        if pred_num is not None and gold_num is not None:\n",
        "            try:\n",
        "                losses[\"numeric\"] = float((sp.N(pred_num) - sp.N(gold_num))**2)\n",
        "            except:\n",
        "                losses[\"numeric\"] = 1.0\n",
        "        else:\n",
        "            losses[\"numeric\"] = 1.0\n",
        "\n",
        "        # Units loss\n",
        "        losses[\"units\"] = self.check_units(pred_text)\n",
        "\n",
        "        # Algebraic form loss\n",
        "        losses[\"algebraic\"] = self.check_algebraic_form(pred_text, gold_text)\n",
        "\n",
        "        # Self-consistency loss\n",
        "        if model is not None and tokenizer is not None and prompt is not None:\n",
        "            losses[\"self_consistency\"] = self.check_self_consistency(\n",
        "                model, tokenizer, prompt\n",
        "            )\n",
        "        else:\n",
        "            losses[\"self_consistency\"] = 0.0\n",
        "\n",
        "        # Weighted total\n",
        "        total = sum(self.weights.get(k, 0) * v for k, v in losses.items())\n",
        "        losses[\"total\"] = total\n",
        "\n",
        "        return losses\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05n0AuCXDN-L",
        "outputId": "fc2af9ab-43d1-44f6-afda-628d9b1cbd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/verifiers/composite_verifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/hooks.py\n",
        "import weakref\n",
        "import torch\n",
        "\n",
        "class HookManager:\n",
        "    def __init__(self):\n",
        "        self.handles = []\n",
        "        self.layers = weakref.WeakSet()\n",
        "        self.head_outputs = {}  # for multi-head exterior\n",
        "\n",
        "    def attach(self, model):\n",
        "        self.detach()\n",
        "        for name, m in model.named_modules():\n",
        "            if m.__class__.__name__ == \"VPSLinear\":\n",
        "                self.layers.add(m)\n",
        "                self.handles.append(m.register_forward_pre_hook(self._fwd_pre))\n",
        "                self.handles.append(m.register_forward_hook(self._fwd_post))\n",
        "                self.handles.append(m.register_full_backward_hook(self._bwd))\n",
        "            # Capture per-head outputs if attention block\n",
        "            if \"attention\" in name.lower() and hasattr(m, \"num_heads\"):\n",
        "                self.handles.append(m.register_forward_hook(self._capture_heads))\n",
        "\n",
        "    def detach(self):\n",
        "        for h in self.handles:\n",
        "            try: h.remove()\n",
        "            except: pass\n",
        "        self.handles.clear()\n",
        "        self.layers = weakref.WeakSet()\n",
        "        self.head_outputs.clear()\n",
        "\n",
        "    @staticmethod\n",
        "    def _fwd_pre(module, inputs):\n",
        "        x = inputs[0]\n",
        "        module.last_x = x.detach()\n",
        "\n",
        "    @staticmethod\n",
        "    def _fwd_post(module, inputs, output):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def _bwd(module, grad_input, grad_output):\n",
        "        g = grad_output[0]\n",
        "        module.last_grad_h = g.detach()\n",
        "\n",
        "    def _capture_heads(self, module, inputs, output):\n",
        "        \"\"\"Capture per-head outputs for exterior products\"\"\"\n",
        "        if hasattr(output, \"shape\") and len(output.shape) >= 3:\n",
        "            self.head_outputs[id(module)] = output.detach()\n",
        "\n",
        "    def clear_buffers(self):\n",
        "        for m in list(self.layers):\n",
        "            for name in (\"last_x\",\"last_h\",\"last_grad_h\"):\n",
        "                if hasattr(m, name): setattr(m, name, None)\n",
        "        self.head_outputs.clear()\n",
        "\n",
        "    def get_head_outputs(self, module_id):\n",
        "        return self.head_outputs.get(module_id, None)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR2Pac7pDRt3",
        "outputId": "17684437-1c1d-4002-fdee-7ac9bcaf528c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/hooks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/utils/generation.py\n",
        "from typing import Optional, List, Dict, Any\n",
        "import torch\n",
        "\n",
        "def _as_model_inputs(x):\n",
        "    \"\"\"Normalize tokenizer outputs into a dict with input_ids and attention_mask.\"\"\"\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return {\"input_ids\": x, \"attention_mask\": torch.ones_like(x)}\n",
        "    elif isinstance(x, dict):\n",
        "        if \"attention_mask\" not in x and \"input_ids\" in x:\n",
        "            x = dict(x)\n",
        "            x[\"attention_mask\"] = torch.ones_like(x[\"input_ids\"])\n",
        "        return x\n",
        "    else:\n",
        "        t = torch.as_tensor(x, dtype=torch.long)\n",
        "        return {\"input_ids\": t, \"attention_mask\": torch.ones_like(t)}\n",
        "\n",
        "def _chat_inputs(tokenizer, prompt: str, add_generation_prompt: bool = True):\n",
        "    \"\"\"\n",
        "    Build input_ids using the tokenizer's chat_template when available.\n",
        "    Returns a dict of tensors on CPU.\n",
        "    \"\"\"\n",
        "    if getattr(tokenizer, \"apply_chat_template\", None) and getattr(tokenizer, \"chat_template\", None):\n",
        "        messages: List[Dict[str, Any]] = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "        out = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=add_generation_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return _as_model_inputs(out)\n",
        "    # Fallback: plain prompt\n",
        "    return _as_model_inputs(tokenizer(prompt, return_tensors=\"pt\"))\n",
        "\n",
        "def generate(model, tok, prompt: str, max_new_tokens: int = 128,\n",
        "             temperature: float = 0.7, top_p: float = 0.95,\n",
        "             do_sample: Optional[bool] = None) -> str:\n",
        "    if do_sample is None:\n",
        "        do_sample = temperature > 0.0\n",
        "\n",
        "    inputs = _chat_inputs(tok, prompt, add_generation_prompt=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id or tok.eos_token_id,\n",
        "        return_dict_in_generate=True,\n",
        "    )\n",
        "    seq = out.sequences  # [bsz, prompt_len + new_len]\n",
        "\n",
        "    # Decode ONLY the newly generated tokens (no role echoes)\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    new_ids = seq[:, prompt_len:]\n",
        "    return tok.decode(new_ids[0], skip_special_tokens=True).strip()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTK6gV2ODU7r",
        "outputId": "b5afbeed-fe1b-49c5-cf2a-f66c1f6046d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/vpscore/utils/generation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/scripts/infer_vps.py\n",
        "import os\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "from transformers.utils import logging as hf_logging\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.patch_hf import patch_model_with_vps\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.hooks import HookManager\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# Use canonical keys that CompositeVerifier expects\n",
        "DEFAULT_VERIFIER_WEIGHTS = {\n",
        "    \"numeric\": 1.0,\n",
        "    \"units\": 0.5,\n",
        "    \"algebraic\": 0.2,\n",
        "    \"self_consistency\": 0.3,\n",
        "}\n",
        "\n",
        "def build(cfg: VPSConfig):\n",
        "    # ---- robust fallbacks so missing config fields don't crash ----\n",
        "    seed = getattr(cfg, \"seed\", 1234)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Resolve dtype name -> torch dtype\n",
        "    dtype_name = getattr(cfg, \"dtype\", None) or getattr(cfg, \"torch_dtype_str\", \"float16\")\n",
        "    dtype = getattr(torch, dtype_name, torch.float16)\n",
        "\n",
        "    model_name = getattr(cfg, \"model_name\", None)\n",
        "    if not model_name:\n",
        "        raise ValueError(\"cfg.model_name is not set in VPSConfig\")\n",
        "\n",
        "    device_map = getattr(cfg, \"device_map\", \"auto\")\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.pad_token_id = tok.eos_token_id\n",
        "\n",
        "    # Use the official argument name\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype, device_map=device_map)\n",
        "\n",
        "    apply_to = getattr(cfg, \"apply_to\", None)\n",
        "    model = patch_model_with_vps(model, apply_to, cfg)\n",
        "\n",
        "    # Small introspection to confirm wrapping happened\n",
        "    wrapped = sum(1 for _, m in model.named_modules() if m.__class__.__name__ == \"VPSLinear\")\n",
        "    print(f\"[VPS] Wrapped layers: {wrapped}\")\n",
        "\n",
        "    hooks = HookManager()\n",
        "    hooks.attach(model)\n",
        "    return tok, model, hooks\n",
        "\n",
        "def _parse_args():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    # Notebook-safe defaults prevent argparse crashes\n",
        "    ap.add_argument(\"--prompt\", type=str, default=\"Compute 120/1.5 and give the answer with unit km/h.\")\n",
        "    ap.add_argument(\"--gold\", type=str, default=None)\n",
        "    ap.add_argument(\"--iters\", type=int, default=2)\n",
        "    # In notebooks, IPython may inject argv; parse_known keeps it safe\n",
        "    args, _ = ap.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "    args = _parse_args()\n",
        "    cfg = VPSConfig()\n",
        "\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Fall back if cfg.verifier_weights missing\n",
        "    verifier_weights = getattr(cfg, \"verifier_weights\", DEFAULT_VERIFIER_WEIGHTS)\n",
        "    verifier = CompositeVerifier(verifier_weights)\n",
        "\n",
        "    # Also fallbacks for gen params\n",
        "    max_new_tokens = getattr(cfg, \"max_new_tokens\", 128)\n",
        "    temperature = getattr(cfg, \"temperature\", 0.7)\n",
        "    top_p = getattr(cfg, \"top_p\", 0.9)\n",
        "\n",
        "    # ----------------- Iteration 0 (plain decode) -----------------\n",
        "    text = generate(model, tok, args.prompt, max_new_tokens, temperature, top_p)\n",
        "    print(f\"=== Iteration 0 ===\\n{text}\\n\")\n",
        "\n",
        "    if args.gold is None or args.iters < 2:\n",
        "        return\n",
        "\n",
        "    prev_loss = float(\"inf\")\n",
        "\n",
        "    for it in range(1, args.iters):\n",
        "        hooks.clear_buffers()\n",
        "\n",
        "        # Clear any L-BFGS memories (keeps your technique intact)\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"clear_lbfgs\"):\n",
        "                m.clear_lbfgs()\n",
        "\n",
        "        # Forward pass to compute token entropy for the adaptive policy\n",
        "        inputs = tok(args.prompt, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.enable_grad():\n",
        "            out = model(**inputs)\n",
        "            logits = out.logits\n",
        "            entropy = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "        # Feed entropy to policies\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                m.policy.set_token_entropy(float(entropy))\n",
        "\n",
        "        # Deterministic decode for verifier evaluation\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=min(64, max_new_tokens),\n",
        "            do_sample=False,\n",
        "            return_dict_in_generate=True,\n",
        "        )\n",
        "        pred_text = tok.decode(out_ids.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        # Multi-objective verifier\n",
        "        losses = verifier.compute_loss(pred_text, args.gold, model, tok, args.prompt)\n",
        "        vloss = losses[\"total\"]\n",
        "\n",
        "        # Feedback loop to policy\n",
        "        improved = vloss < prev_loss\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                m.policy.update_outcome(improved, prev_loss - vloss)\n",
        "        prev_loss = vloss\n",
        "\n",
        "        # Short CE surrogate to populate grads → VPS uses grad_h (via hooks)\n",
        "        gold_ids = tok(args.gold, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "        T = min(8, gold_ids.shape[0])\n",
        "        target = gold_ids[-T:].unsqueeze(0)  # [1, T]\n",
        "\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        with torch.enable_grad():\n",
        "            out2 = model(**inputs)\n",
        "            logits2 = out2.logits[:, -T:, :]  # [1, T, V]\n",
        "            ce = torch.nn.functional.cross_entropy(\n",
        "                logits2.reshape(-1, logits2.size(-1)),\n",
        "                target.reshape(-1)\n",
        "            )\n",
        "            ce.backward()\n",
        "\n",
        "        # VPS-enhanced generation\n",
        "        text2 = generate(model, tok, args.prompt, max_new_tokens, temperature, top_p)\n",
        "        print(f\"=== Iteration {it} (VPS enhanced) ===\")\n",
        "        print(f\"Losses: {losses}\")\n",
        "        print(f\"Output:\\n{text2}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVM3KVrxcut4",
        "outputId": "b658ec16-1d84-4c66-a59c-7617df94dd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/scripts/infer_vps.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/patch_hf.py\n",
        "from __future__ import annotations\n",
        "import torch.nn as nn\n",
        "from typing import List, Any\n",
        "from .vps_linear import VPSLinear\n",
        "\n",
        "NAMES = {\n",
        "    \"attn_q\": [\"q_proj\",\"wq\",\"query\",\"q\"],\n",
        "    \"attn_k\": [\"k_proj\",\"wk\",\"key\",\"k\"],\n",
        "    \"attn_v\": [\"v_proj\",\"wv\",\"value\",\"v\"],\n",
        "    \"attn_o\": [\"o_proj\",\"wo\",\"out_proj\",\"o\"],\n",
        "    \"mlp_up\": [\"up_proj\",\"w1\",\"fc_in\",\"dense_h_to_4h\",\"gate_proj\",\"w3\",\"gate\"],\n",
        "    \"mlp_down\":[\"down_proj\",\"w2\",\"fc_out\",\"dense_4h_to_h\"],\n",
        "}\n",
        "\n",
        "# alias -> bucket map\n",
        "ALIAS2BUCKET = {alias: bucket for bucket, aliases in NAMES.items() for alias in aliases}\n",
        "\n",
        "# Global registry for Q/K pairing across modules (keyed by parent module id)\n",
        "PAIR_REGISTRY = {}\n",
        "\n",
        "def _match_bucket(name: str, apply_to: List[str]):\n",
        "    if not apply_to:\n",
        "        return None\n",
        "    lname = name.lower()\n",
        "\n",
        "    # If user provided canonical buckets, honor those\n",
        "    for bucket, aliases in NAMES.items():\n",
        "        if bucket in apply_to and any(a in lname for a in aliases):\n",
        "            return bucket\n",
        "\n",
        "    # If user provided aliases (e.g., \"q_proj\"), map them to buckets\n",
        "    for wanted in apply_to:\n",
        "        w = wanted.lower()\n",
        "        bucket = ALIAS2BUCKET.get(w, None)\n",
        "        if bucket is not None:\n",
        "            # Check actual child name contains this alias\n",
        "            if any(w in lname for w in NAMES[bucket]):\n",
        "                return bucket\n",
        "    return None\n",
        "\n",
        "def _wrap(module: nn.Module, apply_to: List[str], vps_kwargs: dict):\n",
        "    for name, child in list(module.named_children()):\n",
        "        _wrap(child, apply_to, vps_kwargs)\n",
        "        if isinstance(child, nn.Linear):\n",
        "            bucket = _match_bucket(name, apply_to)\n",
        "            if bucket is not None:\n",
        "                vps = VPSLinear(child, **vps_kwargs)\n",
        "                setattr(module, name, vps)\n",
        "                # Register possible q/k pairs under the parent block\n",
        "                if bucket in [\"attn_q\", \"attn_k\"]:\n",
        "                    key = id(module)\n",
        "                    PAIR_REGISTRY.setdefault(key, {})[bucket] = vps\n",
        "\n",
        "def patch_model_with_vps(model: nn.Module, apply_to: List[str], *args: Any, **kwargs: Any):\n",
        "    \"\"\"\n",
        "    Accepts either:\n",
        "      (model, apply_to, cfg)   where cfg is VPSConfig\n",
        "    or legacy long-form parameter lists (kept for compatibility).\n",
        "    \"\"\"\n",
        "    softgrad_builder = kwargs.pop(\"softgrad_builder\", False)\n",
        "    policy_cfg = kwargs.pop(\"policy_cfg\", None)\n",
        "    qk_coupling_flag = False\n",
        "\n",
        "    if len(args) == 1 and not isinstance(args[0], (int, float, str, bool)):\n",
        "        cfg = args[0]\n",
        "        rank   = getattr(cfg, \"rank\", 2)\n",
        "        topk   = getattr(cfg, \"topk\", 32)\n",
        "        clamp  = getattr(cfg, \"clamp\", 0.2)\n",
        "        gamma  = getattr(cfg, \"gamma\", 0.5)\n",
        "        builder= getattr(cfg, \"builder\", \"hybrid\")\n",
        "        softgrad_builder = getattr(cfg, \"softgrad_builder\", softgrad_builder)\n",
        "        policy_cfg = cfg\n",
        "        qk_coupling_flag = bool(getattr(cfg, \"qk_coupling\", False))\n",
        "    else:\n",
        "        if len(args) >= 6:\n",
        "            rank, topk, clamp, gamma, builder = args[0:6]\n",
        "            if len(args) >= 12:\n",
        "                qk_coupling_flag = bool(args[7])\n",
        "            if len(args) >= 13 and isinstance(args[12], bool):\n",
        "                softgrad_builder = args[12]\n",
        "            if len(args) >= 14:\n",
        "                policy_cfg = args[13]\n",
        "        else:\n",
        "            raise TypeError(\"patch_model_with_vps: unsupported argument pattern. \"\n",
        "                            \"Pass VPSConfig as third arg OR full long-form params.\")\n",
        "\n",
        "    vps_kwargs = dict(\n",
        "        rank=rank,\n",
        "        topk=topk,\n",
        "        clamp=clamp,\n",
        "        gamma=gamma,\n",
        "        builder=builder,\n",
        "        softgrad_builder=softgrad_builder,\n",
        "        policy_cfg=policy_cfg,\n",
        "    )\n",
        "\n",
        "    _wrap(model, apply_to or [], vps_kwargs)\n",
        "\n",
        "    if qk_coupling_flag:\n",
        "        for pair in PAIR_REGISTRY.values():\n",
        "            q = pair.get(\"attn_q\")\n",
        "            k = pair.get(\"attn_k\")\n",
        "            if q is not None and k is not None:\n",
        "                q.is_Q = True; k.is_K = True\n",
        "                q._peer = k;   k._peer = q\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkt8dEKYczTL",
        "outputId": "0e77043a-c267-4707-a9b0-f3361ebb6a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/vpscore/patch_hf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/hooks.py\n",
        "import weakref\n",
        "import torch\n",
        "\n",
        "class HookManager:\n",
        "    def __init__(self):\n",
        "        self.handles = []\n",
        "        self.layers = weakref.WeakSet()\n",
        "        self.head_outputs = {}  # for multi-head exterior\n",
        "\n",
        "    def attach(self, model):\n",
        "        self.detach()\n",
        "        for name, m in model.named_modules():\n",
        "            if m.__class__.__name__ == \"VPSLinear\":\n",
        "                self.layers.add(m)\n",
        "                self.handles.append(m.register_forward_pre_hook(self._fwd_pre))\n",
        "                self.handles.append(m.register_forward_hook(self._fwd_post))\n",
        "                self.handles.append(m.register_full_backward_hook(self._bwd))\n",
        "            if \"attention\" in name.lower() and hasattr(m, \"num_heads\"):\n",
        "                self.handles.append(m.register_forward_hook(self._capture_heads))\n",
        "\n",
        "    def detach(self):\n",
        "        for h in self.handles:\n",
        "            try: h.remove()\n",
        "            except: pass\n",
        "        self.handles.clear()\n",
        "        self.layers = weakref.WeakSet()\n",
        "        self.head_outputs.clear()\n",
        "\n",
        "    @staticmethod\n",
        "    def _fwd_pre(module, inputs):\n",
        "        x = inputs[0]\n",
        "        module.last_x = x.detach()\n",
        "\n",
        "    @staticmethod\n",
        "    def _fwd_post(module, inputs, output):\n",
        "        # critical: keep activations for stats/policy/rotations\n",
        "        try:\n",
        "            module.last_h = output.detach()\n",
        "        except Exception:\n",
        "            module.last_h = None\n",
        "\n",
        "    @staticmethod\n",
        "    def _bwd(module, grad_input, grad_output):\n",
        "        g = grad_output[0]\n",
        "        module.last_grad_h = g.detach() if g is not None else None\n",
        "\n",
        "    def _capture_heads(self, module, inputs, output):\n",
        "        if hasattr(output, \"shape\") and len(output.shape) >= 3:\n",
        "            self.head_outputs[id(module)] = output.detach()\n",
        "\n",
        "    def clear_buffers(self):\n",
        "        for m in list(self.layers):\n",
        "            for name in (\"last_x\",\"last_h\",\"last_grad_h\"):\n",
        "                if hasattr(m, name): setattr(m, name, None)\n",
        "        self.head_outputs.clear()\n",
        "\n",
        "    def get_head_outputs(self, module_id):\n",
        "        return self.head_outputs.get(module_id, None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5j7itPbc2nZ",
        "outputId": "25a0fd00-34eb-4701-df9e-86d2fe6ae0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/vpscore/hooks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/utils/generation.py\n",
        "from typing import Optional, List, Dict, Any\n",
        "import torch\n",
        "\n",
        "def _as_model_inputs(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return {\"input_ids\": x, \"attention_mask\": torch.ones_like(x)}\n",
        "    elif isinstance(x, dict):\n",
        "        if \"attention_mask\" not in x and \"input_ids\" in x:\n",
        "            x = dict(x)\n",
        "            x[\"attention_mask\"] = torch.ones_like(x[\"input_ids\"])\n",
        "        return x\n",
        "    else:\n",
        "        t = torch.as_tensor(x, dtype=torch.long)\n",
        "        return {\"input_ids\": t, \"attention_mask\": torch.ones_like(t)}\n",
        "\n",
        "def _chat_inputs(tokenizer, prompt: str, add_generation_prompt: bool = True):\n",
        "    if getattr(tokenizer, \"apply_chat_template\", None) and getattr(tokenizer, \"chat_template\", None):\n",
        "        messages: List[Dict[str, Any]] = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "        out = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=add_generation_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return _as_model_inputs(out)\n",
        "    return _as_model_inputs(tokenizer(prompt, return_tensors=\"pt\"))\n",
        "\n",
        "def generate(model, tok, prompt: str, max_new_tokens: int = 128,\n",
        "             temperature: float = 0.7, top_p: float = 0.95,\n",
        "             do_sample: Optional[bool] = None) -> str:\n",
        "    if do_sample is None:\n",
        "        do_sample = temperature > 0.0\n",
        "\n",
        "    inputs = _chat_inputs(tok, prompt, add_generation_prompt=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        return_dict_in_generate=True,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id or tok.eos_token_id,\n",
        "    )\n",
        "    if do_sample:\n",
        "        gen_kwargs.update(temperature=temperature, top_p=top_p)\n",
        "\n",
        "    out = model.generate(**inputs, **gen_kwargs)\n",
        "    seq = out.sequences\n",
        "\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    new_ids = seq[:, prompt_len:]\n",
        "    return tok.decode(new_ids[0], skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7J8XVysc5Il",
        "outputId": "1e7dc328-c0e6-4796-b766-c7edb60d019c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/vpscore/utils/generation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/verifiers/composite_verifier.py\n",
        "import re\n",
        "import sympy as sp\n",
        "import torch\n",
        "from typing import Dict, Any, Optional\n",
        "import pint\n",
        "\n",
        "ureg = pint.UnitRegistry()\n",
        "\n",
        "_WARN_SIGNS = (\"E0000\",\"W0000\",\"tensorflow\",\"xla\",\"cuDNN\",\"cuBLAS\",\"computation_placer\",\"I tensorflow\")\n",
        "\n",
        "class CompositeVerifier:\n",
        "    \"\"\"Multi-objective verifier with numeric, units, algebraic, and self-consistency checks\"\"\"\n",
        "\n",
        "    def __init__(self, weights: Dict[str, float] = None):\n",
        "        # Back-compat mapping for user-provided keys\n",
        "        weights = weights or {}\n",
        "        mapped = {\n",
        "            \"numeric\":           weights.get(\"numeric\",           weights.get(\"exact_match\", 1.0)),\n",
        "            \"units\":             weights.get(\"units\",             weights.get(\"coherence\", 0.5)),\n",
        "            \"algebraic\":         weights.get(\"algebraic\",         0.2),\n",
        "            \"self_consistency\":  weights.get(\"self_consistency\",  weights.get(\"length_penalty\", 0.3)),\n",
        "        }\n",
        "        self.weights = mapped\n",
        "        # Prefer numbers after '=' and with units; fall back to any trailing number\n",
        "        self.eq_pattern = re.compile(r\"(?:=|is)\\s*(-?\\d+(?:\\.\\d+)?)\\s*(?:km/?h|m/s|s|ms|kg|m|cm|mm|%|deg|°)?\\b\", re.I)\n",
        "        self.num_pattern = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "    def _candidate_lines(self, text: str):\n",
        "        # filter out log spam lines\n",
        "        for line in reversed(text.splitlines()):\n",
        "            if any(sig in line for sig in _WARN_SIGNS):\n",
        "                continue\n",
        "            yield line.strip()\n",
        "\n",
        "    def extract_answer(self, text: str) -> Optional[sp.Basic]:\n",
        "        \"\"\"Extract numeric answer from text, preferring '= <num> [unit]' in the last meaningful lines.\"\"\"\n",
        "        for line in self._candidate_lines(text):\n",
        "            m = self.eq_pattern.search(line)\n",
        "            if m:\n",
        "                try:\n",
        "                    return sp.nsimplify(m.group(1))\n",
        "                except Exception:\n",
        "                    pass\n",
        "        # fallback: last number in clean lines\n",
        "        for line in self._candidate_lines(text):\n",
        "            m = self.num_pattern.search(line)\n",
        "            if m:\n",
        "                try:\n",
        "                    return sp.nsimplify(m.group(1))\n",
        "                except Exception:\n",
        "                    return None\n",
        "        return None\n",
        "\n",
        "    def check_units(self, text: str, expected_unit: str = None) -> float:\n",
        "        try:\n",
        "            quantities = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*([a-zA-Z]+)\", text)\n",
        "            if not quantities:\n",
        "                return 1.0\n",
        "            parsed = []\n",
        "            for val, unit in quantities:\n",
        "                try:\n",
        "                    q = ureg.Quantity(float(val), unit)\n",
        "                    parsed.append(q)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if len(parsed) < 2:\n",
        "                return 1.0\n",
        "            dims = [q.dimensionality for q in parsed]\n",
        "            return 0.0 if len(set(str(d) for d in dims)) == 1 else 0.5\n",
        "        except Exception:\n",
        "            return 1.0\n",
        "\n",
        "    def extract_expression(self, text: str) -> Optional[sp.Basic]:\n",
        "        expr_match = re.search(r\"(?:=|is)\\s*([^.]+)\", text)\n",
        "        if expr_match:\n",
        "            try:\n",
        "                return sp.sympify(expr_match.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return None\n",
        "\n",
        "    def check_algebraic_form(self, pred_text: str, gold_text: str) -> float:\n",
        "        try:\n",
        "            pred_expr = self.extract_expression(pred_text)\n",
        "            gold_expr = self.extract_expression(gold_text)\n",
        "            if pred_expr is None or gold_expr is None:\n",
        "                return 1.0\n",
        "            if sp.simplify(pred_expr - gold_expr) == 0:\n",
        "                return 0.0\n",
        "            if pred_expr.free_symbols == gold_expr.free_symbols:\n",
        "                return 0.3\n",
        "            return 1.0\n",
        "        except Exception:\n",
        "            return 1.0\n",
        "\n",
        "    def check_self_consistency(self, model, tokenizer, prompt: str,\n",
        "                               n_samples: int = 3, temperature: float = 0.7) -> float:\n",
        "        answers = []\n",
        "        for _ in range(n_samples):\n",
        "            with torch.no_grad():\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                outputs = model.generate(**inputs, max_new_tokens=128, temperature=temperature, do_sample=True)\n",
        "                text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                ans = self.extract_answer(text)\n",
        "                if ans is not None:\n",
        "                    answers.append(ans)\n",
        "        if len(answers) < 2:\n",
        "            return 1.0\n",
        "        numeric_answers = [float(sp.N(a)) for a in answers]\n",
        "        mean_val = sum(numeric_answers) / len(numeric_answers)\n",
        "        variance = sum((x - mean_val)**2 for x in numeric_answers) / len(numeric_answers)\n",
        "        return min(1.0, variance / (abs(mean_val) + 1e-6)**2)\n",
        "\n",
        "    def compute_loss(self, pred_text: str, gold_text: str,\n",
        "                     model=None, tokenizer=None, prompt: str = None) -> Dict[str, float]:\n",
        "        losses: Dict[str, float] = {}\n",
        "\n",
        "        pred_num = self.extract_answer(pred_text)\n",
        "        gold_num = self.extract_answer(gold_text)\n",
        "        if pred_num is not None and gold_num is not None:\n",
        "            try:\n",
        "                losses[\"numeric\"] = float((sp.N(pred_num) - sp.N(gold_num))**2)\n",
        "            except Exception:\n",
        "                losses[\"numeric\"] = 1.0\n",
        "        else:\n",
        "            losses[\"numeric\"] = 1.0\n",
        "\n",
        "        losses[\"units\"] = self.check_units(pred_text)\n",
        "        losses[\"algebraic\"] = self.check_algebraic_form(pred_text, gold_text)\n",
        "\n",
        "        if model is not None and tokenizer is not None and prompt is not None:\n",
        "            losses[\"self_consistency\"] = self.check_self_consistency(model, tokenizer, prompt)\n",
        "        else:\n",
        "            losses[\"self_consistency\"] = 0.0\n",
        "\n",
        "        total = sum(self.weights.get(k, 0.0) * v for k, v in losses.items())\n",
        "        losses[\"total\"] = total\n",
        "        return losses\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEe9RTF1c72B",
        "outputId": "eadce4ac-912a-46af-d6d6-97da69d243ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/vpscore/verifiers/composite_verifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/vps_linear.py\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from .builders import make_builder, HybridBuilder  # no HybridBuilderOut needed\n",
        "\n",
        "try:\n",
        "    from .math_utils import spectral_norm_clip as _spectral_norm_clip\n",
        "except Exception:\n",
        "    def _spectral_norm_clip(A: torch.Tensor, B: torch.Tensor, tau: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return A, B\n",
        "\n",
        "try:\n",
        "    from .ephemeral_lbfgs import EphemeralLBFGS\n",
        "except Exception:\n",
        "    class EphemeralLBFGS:\n",
        "        def __init__(self, m: int = 5): self.m = m\n",
        "        def update(self, s: torch.Tensor, y: torch.Tensor): return\n",
        "        def two_loop(self, g: torch.Tensor) -> torch.Tensor: return g\n",
        "\n",
        "try:\n",
        "    from .policy import VPSPolicy as _Policy\n",
        "except Exception:\n",
        "    class _Policy:\n",
        "        def __init__(self, cfg): self.cfg = cfg\n",
        "        def decide(self, module, h2d: torch.Tensor):\n",
        "            class P: pass\n",
        "            p = P(); p.rank = int(getattr(module.cfg, \"rank\", 2))\n",
        "            p.gamma = float(getattr(module.cfg, \"gamma\", 0.5))\n",
        "            p.order = int(getattr(module.cfg, \"order\", 1))\n",
        "            return p\n",
        "\n",
        "@dataclass\n",
        "class _VPSLinearCfg:\n",
        "    rank: int = 2\n",
        "    topk: int = 32\n",
        "    clamp: Optional[float] = None\n",
        "    gamma: float = 0.5\n",
        "    builder: str = \"hybrid\"\n",
        "    order: int = 1\n",
        "    qk_coupling: bool = True\n",
        "    tau: float = 0.8\n",
        "    lbfgs_enabled: bool = True\n",
        "    adaptive_rank: bool = True\n",
        "    adaptive_gamma: bool = True\n",
        "    alpha: float = 1e-3  # for SC builder\n",
        "    # --- policy expectations (add safe defaults) ---\n",
        "    enable_policy: bool = True\n",
        "    min_energy_threshold: float = 1e-6\n",
        "    rank_bounds: tuple = (1, 8)\n",
        "    topk_bounds: tuple = (8, 64)\n",
        "    gamma_bounds: tuple = (0.1, 1.0)\n",
        "    order_bounds: tuple = (1, 2)\n",
        "    adaptive_order: bool = True\n",
        "\n",
        "class VPSLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Adds a dynamic low-rank delta: y = W x + gamma * (x A) B^T,\n",
        "    with A = W^T V, B = W U. U,V are synthesized on the fly.\n",
        "    \"\"\"\n",
        "    def __init__(self, base: nn.Linear, **kwargs):\n",
        "        super().__init__()\n",
        "        assert isinstance(base, nn.Linear), \"VPSLinear expects nn.Linear as 'base'\"\n",
        "        self.base = base\n",
        "        self.cfg = _VPSLinearCfg(**{\n",
        "            **_VPSLinearCfg().__dict__,\n",
        "            **{k: v for k, v in kwargs.items() if k in _VPSLinearCfg().__dict__}\n",
        "        })\n",
        "        self.builder = make_builder(self.cfg.builder, self.cfg)\n",
        "        self.builder_name = self.cfg.builder\n",
        "        self.policy  = _Policy(self.cfg)\n",
        "        self.lbfgs   = EphemeralLBFGS(m=5) if self.cfg.lbfgs_enabled else None\n",
        "\n",
        "    @property\n",
        "    def clamp(self):\n",
        "        return self.cfg.clamp\n",
        "\n",
        "    def clear_lbfgs(self):\n",
        "        if self.lbfgs and hasattr(self.lbfgs, \"S\"):\n",
        "            try:\n",
        "                self.lbfgs.S.clear(); self.lbfgs.Y.clear()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return (f\"rank={self.cfg.rank}, topk={self.cfg.topk}, gamma={self.cfg.gamma}, \"\n",
        "                f\"builder='{self.cfg.builder}', order={self.cfg.order}, lbfgs={self.cfg.lbfgs_enabled}\")\n",
        "\n",
        "    def _flatten(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, ...]]:\n",
        "        if x.dim() == 2:\n",
        "            return x, x.shape\n",
        "        in_f = x.size(-1)\n",
        "        return x.reshape(-1, in_f), x.shape\n",
        "\n",
        "    def _unflatten(self, y2d: torch.Tensor, orig_shape: Tuple[int, ...]) -> torch.Tensor:\n",
        "        return y2d.reshape(*orig_shape[:-1], y2d.size(-1))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _decide_policy(self, h2d: torch.Tensor):\n",
        "        return self.policy.decide(self, h2d)\n",
        "\n",
        "    def _compute_delta(self, x2d: torch.Tensor, W: nn.Linear, A: torch.Tensor, B: torch.Tensor, order: int) -> torch.Tensor:\n",
        "        tmp = x2d @ A              # (N, r)\n",
        "        delta = tmp @ B.t()        # (N, out)\n",
        "        if order >= 2:\n",
        "            delta2 = (tmp @ B.t())\n",
        "            delta = delta + 0.5 * delta2\n",
        "        return delta\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        base_out = self.base(x)\n",
        "\n",
        "        x2d, _ = self._flatten(x)\n",
        "        h2d, _ = self._flatten(base_out.detach())\n",
        "\n",
        "        pol = self._decide_policy(h2d)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            U, V, in_f, out_f = self.builder(x2d, h2d, self.base, grad_h=None, target_step=None)\n",
        "            r = U.shape[1]\n",
        "            Wt = self.base.weight.t()                   # (in, out)\n",
        "            A = Wt @ V                                  # (in, r)\n",
        "            B = self.base.weight @ U                    # (out, r)\n",
        "            tau = float(getattr(self.cfg, \"tau\", 0.8))\n",
        "            A, B = _spectral_norm_clip(A, B, tau)\n",
        "\n",
        "        delta = self._compute_delta(x2d, self.base, A.to(x2d.dtype), B.to(x2d.dtype), pol.order)\n",
        "\n",
        "        if self.cfg.clamp is not None:\n",
        "            c = float(self.cfg.clamp)\n",
        "            delta = delta.clamp(min=-c, max=c)\n",
        "\n",
        "        if self.lbfgs is not None:\n",
        "            g = delta.detach().reshape(-1).to(torch.float32)\n",
        "            d = self.lbfgs.two_loop(g)\n",
        "            delta = delta + 1e-3 * d.reshape_as(delta).to(delta.dtype)\n",
        "\n",
        "        y2d = base_out.reshape(-1, base_out.size(-1))\n",
        "        y2d = y2d + float(pol.gamma) * delta\n",
        "        return self._unflatten(y2d, base_out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQKpB7LZc_Vb",
        "outputId": "5e630b2d-ede1-487b-de26-0e4c7f866c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/vpscore/vps_linear.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/ephemeral_lbfgs.py\n",
        "from __future__ import annotations\n",
        "import torch\n",
        "from collections import deque\n",
        "\n",
        "class EphemeralLBFGS:\n",
        "    \"\"\"\n",
        "    Tiny L-BFGS memory for preconditioning a 1D gradient-like vector.\n",
        "    - Keeps up to m curvature pairs (s_k, y_k)\n",
        "    - Provides two_loop(g) that returns an L-BFGS-preconditioned direction\n",
        "    Notes:\n",
        "      • This is *ephemeral*: we don't persist across runs; callers may or may not\n",
        "        populate (S, Y). If empty, two_loop(g) returns g (identity preconditioner).\n",
        "      • All math is done in float32 for stability, then cast back to g.dtype.\n",
        "    \"\"\"\n",
        "    def __init__(self, m: int = 5):\n",
        "        self.m = int(max(1, m))\n",
        "        self.S: deque[torch.Tensor] = deque(maxlen=self.m)\n",
        "        self.Y: deque[torch.Tensor] = deque(maxlen=self.m)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, s: torch.Tensor, y: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Add a curvature pair (s, y). Inputs can be any shape; we flatten to 1D.\n",
        "        A simple curvature check keeps only pairs with positive y·s.\n",
        "        \"\"\"\n",
        "        if s is None or y is None:\n",
        "            return\n",
        "        s = s.reshape(-1).detach()\n",
        "        y = y.reshape(-1).detach()\n",
        "        if s.numel() == 0 or y.numel() == 0:\n",
        "            return\n",
        "        # basic curvature check\n",
        "        ys = torch.dot(y.to(torch.float32), s.to(torch.float32))\n",
        "        if torch.isfinite(ys) and ys > 1e-8:\n",
        "            self.S.append(s.clone())\n",
        "            self.Y.append(y.clone())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def two_loop(self, g: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Standard L-BFGS two-loop recursion.\n",
        "        If no history is available, returns g unchanged.\n",
        "        \"\"\"\n",
        "        if g is None:\n",
        "            return g\n",
        "        # Work in float32 for robustness (esp. under fp16/bf16 models)\n",
        "        device = g.device\n",
        "        g32 = g.detach().reshape(-1).to(torch.float32)\n",
        "\n",
        "        m = len(self.S)\n",
        "        if m == 0:\n",
        "            return g  # identity preconditioner\n",
        "\n",
        "        # Prepare copies in float32 on the right device\n",
        "        S = [self.S[i].to(device=device, dtype=torch.float32) for i in range(m)]\n",
        "        Y = [self.Y[i].to(device=device, dtype=torch.float32) for i in range(m)]\n",
        "\n",
        "        rho = []\n",
        "        for i in range(m):\n",
        "            ys = torch.dot(Y[i], S[i])\n",
        "            # Guard against tiny/negative curvature\n",
        "            rho.append(1.0 / float(ys.item()) if ys > 1e-12 else 0.0)\n",
        "\n",
        "        # First loop: backward\n",
        "        q = g32.clone()\n",
        "        alpha = [0.0] * m\n",
        "        for i in range(m - 1, -1, -1):\n",
        "            if rho[i] == 0.0:\n",
        "                alpha[i] = 0.0\n",
        "                continue\n",
        "            alpha[i] = rho[i] * float(torch.dot(S[i], q).item())\n",
        "            q.add_(Y[i], alpha=-alpha[i])\n",
        "\n",
        "        # Initial H0 scaling: gamma0 = (s_{m-1}^T y_{m-1}) / (y_{m-1}^T y_{m-1})\n",
        "        y_last = Y[-1]\n",
        "        s_last = S[-1]\n",
        "        denom = float(torch.dot(y_last, y_last).item())\n",
        "        numer = float(torch.dot(s_last, y_last).item())\n",
        "        gamma0 = numer / denom if denom > 1e-12 else 1.0\n",
        "        r = q.mul(gamma0)\n",
        "\n",
        "        # Second loop: forward\n",
        "        for i in range(m):\n",
        "            if rho[i] == 0.0:\n",
        "                continue\n",
        "            beta = rho[i] * float(torch.dot(Y[i], r).item())\n",
        "            r.add_(S[i], alpha=(alpha[i] - beta))\n",
        "\n",
        "        # Cast back to original dtype/shape\n",
        "        return r.to(dtype=g.dtype).reshape_as(g)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFFMxcEheJa2",
        "outputId": "4a937f43-e6aa-469d-c94a-4c809c7dd9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/vpscore/ephemeral_lbfgs.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/scripts/eval_suite.py\n",
        "# Colab/T4-friendly evaluation suite:\n",
        "# - GSM8K (100), BBH Date Understanding (100), ARC-Challenge (100)\n",
        "# - Baseline (VPS off) vs VPS (on), identical decode (greedy)\n",
        "# - Reports: accuracy, latency, tokens/sec, VRAM deltas\n",
        "\n",
        "import argparse, time, math, re, random, statistics as stats\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import set_seed\n",
        "\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.hooks import HookManager\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "from vps.scripts.infer_vps import build  # reuse your build() exactly\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "NUM_RE = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "def _extract_number(s: str):\n",
        "    m = list(NUM_RE.finditer(s or \"\"))\n",
        "    return float(m[-1].group(1)) if m else None\n",
        "\n",
        "def _normalize(s: str):\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "def _extract_choice_letter(s: str):\n",
        "    s = s.strip()\n",
        "    # Prefer the last single upper letter A-E\n",
        "    m = re.findall(r\"\\b([A-E])\\b\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    # Or parse like \"answer: (C)\" / \"C)\"\n",
        "    m = re.findall(r\"\\(([A-E])\\)\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    return None\n",
        "\n",
        "def _answer_is_correct(task, pred, gold):\n",
        "    if task == \"gsm8k\":\n",
        "        pn, gn = _extract_number(pred), _extract_number(gold)\n",
        "        if pn is None or gn is None: return False\n",
        "        # tolerant to minor formatting; exact numeric match\n",
        "        return abs(pn - gn) < 1e-6\n",
        "    elif task == \"bbh_date\":\n",
        "        return _normalize(pred) == _normalize(gold)\n",
        "    elif task == \"arc_c\":\n",
        "        return _extract_choice_letter(pred) == str(gold).upper()\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def _format_prompt(task, x):\n",
        "    if task == \"gsm8k\":\n",
        "        q = x[\"question\"] if \"question\" in x else x[\"question_text\"]\n",
        "        return f\"Solve the problem. Return ONLY the final number.\\n\\nProblem: {q}\\nAnswer:\"\n",
        "    elif task == \"bbh_date\":\n",
        "        return f\"Answer the question correctly. Return ONLY the final answer string.\\n\\n{ x['input'] }\\nAnswer:\"\n",
        "    elif task == \"arc_c\":\n",
        "        q = x[\"question\"]\n",
        "        labels = x[\"choices\"][\"label\"]\n",
        "        texts  = x[\"choices\"][\"text\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, texts)])\n",
        "        return (\n",
        "          \"You are given a multiple-choice science question. \"\n",
        "          \"Return ONLY the correct option letter (A, B, C, D, or E).\\n\\n\"\n",
        "          f\"{q}\\n\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(task)\n",
        "\n",
        "def _gold_text(task, x):\n",
        "    if task == \"gsm8k\":\n",
        "        # gsm8k answer strings are like \"... #### 42\"\n",
        "        ans = x[\"answer\"]\n",
        "        m = re.search(r\"####\\s*(.+)\", ans)\n",
        "        return m.group(1).strip() if m else ans.strip()\n",
        "    elif task == \"bbh_date\":\n",
        "        # bbh fields have 'target' as gold\n",
        "        return str(x[\"target\"]).strip()\n",
        "    elif task == \"arc_c\":\n",
        "        return str(x[\"answerKey\"]).strip().upper()\n",
        "    else:\n",
        "        raise ValueError(task)\n",
        "\n",
        "# ---------- Iterative VPS step (mirrors infer_vps.py) ----------\n",
        "def vps_iterate_once(model, tok, hooks, prompt:str, gold_text:str, max_new_tokens:int):\n",
        "    \"\"\"\n",
        "    Do the same two-phase you used:\n",
        "      - compute entropy on prompt\n",
        "      - deterministic decode for verifier text\n",
        "      - verifier loss -> policy.update_outcome\n",
        "      - short CE surrogate backward vs gold tail to populate grad_h\n",
        "      - final VPS-enhanced generation (greedy)\n",
        "    \"\"\"\n",
        "    hooks.clear_buffers()\n",
        "\n",
        "    # entropy from prompt logits\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.enable_grad():\n",
        "        out = model(**inputs)\n",
        "        logits = out.logits\n",
        "        entropy = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            m.policy.set_token_entropy(entropy)\n",
        "\n",
        "    # deterministic decode for verifier evaluation\n",
        "    out_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=min(64, max_new_tokens),\n",
        "        do_sample=False,\n",
        "        return_dict_in_generate=True,\n",
        "    )\n",
        "    pred_text = tok.decode(out_ids.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # verifier\n",
        "    verifier = CompositeVerifier()\n",
        "    losses = verifier.compute_loss(pred_text, gold_text, model, tok, prompt)\n",
        "    vloss = losses[\"total\"]\n",
        "    prev_loss = float(\"inf\")\n",
        "    improved = vloss < prev_loss  # True\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            m.policy.update_outcome(improved, prev_loss - vloss)\n",
        "\n",
        "    # CE surrogate backward to populate grad_h\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    with torch.enable_grad():\n",
        "        out2 = model(**inputs)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            target.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "\n",
        "    # final VPS-enhanced generation (greedy)\n",
        "    final = generate(model, tok, prompt, max_new_tokens=max_new_tokens,\n",
        "                     temperature=0.0, top_p=1.0, do_sample=False)\n",
        "    return final, float(vloss)\n",
        "\n",
        "# ---------- Runner ----------\n",
        "def run_task(task: str, n: int, seed: int, vps_on: bool, cfg_overrides: dict):\n",
        "    set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Build model (we set fp16 for T4)\n",
        "    cfg = VPSConfig()\n",
        "    cfg.dtype = \"fp16\"\n",
        "    max_new = int(cfg_overrides.get(\"max_new_tokens\", cfg.max_new_tokens))\n",
        "    # Greedy for evaluation\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "\n",
        "    # Toggle VPS: gamma=0 => baseline\n",
        "    if not vps_on:\n",
        "        cfg.gamma = 0.0\n",
        "        cfg.adaptive_gamma = False\n",
        "\n",
        "    tok, model, hooks = build(cfg)\n",
        "    model.eval()\n",
        "\n",
        "    # dataset\n",
        "    if task == \"gsm8k\":\n",
        "        ds = load_dataset(\"gsm8k\", \"main\")[\"test\"]\n",
        "    elif task == \"bbh_date\":\n",
        "        ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\")[\"test\"]\n",
        "    elif task == \"arc_c\":\n",
        "        ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\")[\"test\"]\n",
        "    else:\n",
        "        raise ValueError(\"unknown task\")\n",
        "\n",
        "    # sample n items deterministically\n",
        "    idxs = list(range(len(ds)))\n",
        "    random.shuffle(idxs)\n",
        "    idxs = idxs[:n]\n",
        "\n",
        "    # metrics\n",
        "    correct = 0\n",
        "    latencies = []\n",
        "    tokens_gen = []\n",
        "\n",
        "    vram_start = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        ex = ds[idx]\n",
        "        prompt = _format_prompt(task, ex)\n",
        "        gold   = _gold_text(task, ex)\n",
        "\n",
        "        # Measure\n",
        "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "        t0 = time.time()\n",
        "\n",
        "        if vps_on:\n",
        "            pred, _ = vps_iterate_once(model, tok, hooks, prompt, gold, max_new)\n",
        "        else:\n",
        "            pred = generate(model, tok, prompt, max_new_tokens=max_new,\n",
        "                            temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        # crude token count (new tokens only)\n",
        "        in_ids = tok(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        out_ids = tok(pred, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        tokens_gen.append(int(out_ids.shape[1]))\n",
        "\n",
        "        ok = _answer_is_correct(task, pred, gold)\n",
        "        correct += int(ok)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"[{task}] {i}/{n} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s\")\n",
        "\n",
        "    vram_end = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "    acc = correct / n\n",
        "    t_mean = stats.mean(latencies)\n",
        "    tokps = (sum(tokens_gen) / sum(latencies)) if sum(latencies) > 0 else 0.0\n",
        "    vram_mb = (vram_end - vram_start) / (1024**2)\n",
        "\n",
        "    return {\n",
        "        \"task\": task,\n",
        "        \"n\": n,\n",
        "        \"acc\": acc,\n",
        "        \"latency_mean_s\": t_mean,\n",
        "        \"tokens_per_sec\": tokps,\n",
        "        \"vram_delta_mb\": vram_mb,\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=100)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=100)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=100)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=64)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    cfg_over = {\"max_new_tokens\": args.max_new_tokens}\n",
        "\n",
        "    report = {\"baseline\": {}, \"vps\": {}, \"delta\": {}}\n",
        "    results_base = []\n",
        "    results_vps = []\n",
        "\n",
        "    # BASELINE\n",
        "    for task, n in [(\"gsm8k\", args.n_gsm8k),\n",
        "                    (\"bbh_date\", args.n_bbh_date),\n",
        "                    (\"arc_c\", args.n_arc_c)]:\n",
        "        print(f\"\\n=== BASELINE (VPS OFF) :: {task} n={n} ===\")\n",
        "        res = run_task(task, n, args.seed, vps_on=False, cfg_overrides=cfg_over)\n",
        "        results_base.append(res)\n",
        "        print(res)\n",
        "\n",
        "    # VPS\n",
        "    for task, n in [(\"gsm8k\", args.n_gsm8k),\n",
        "                    (\"bbh_date\", args.n_bbh_date),\n",
        "                    (\"arc_c\", args.n_arc_c)]:\n",
        "        print(f\"\\n=== VPS ON :: {task} n={n} ===\")\n",
        "        res = run_task(task, n, args.seed, vps_on=True, cfg_overrides=cfg_over)\n",
        "        results_vps.append(res)\n",
        "        print(res)\n",
        "\n",
        "    # Summarize deltas\n",
        "    def _as_map(lst): return {r[\"task\"]: r for r in lst}\n",
        "    B = _as_map(results_base); V = _as_map(results_vps)\n",
        "\n",
        "    print(\"\\n========= SUMMARY =========\")\n",
        "    for task in [\"gsm8k\", \"bbh_date\", \"arc_c\"]:\n",
        "        b, v = B[task], V[task]\n",
        "        d_acc = (v[\"acc\"] - b[\"acc\"]) * 100.0\n",
        "        d_lat = (v[\"latency_mean_s\"] / max(1e-9, b[\"latency_mean_s\"]) - 1.0) * 100.0\n",
        "        d_tok = (v[\"tokens_per_sec\"] / max(1e-9, b[\"tokens_per_sec\"]) - 1.0) * 100.0\n",
        "        print(f\"{task:12s}  acc: {b['acc']:.3f} -> {v['acc']:.3f}  (Δ {d_acc:+.2f} pp) | \"\n",
        "              f\"latency: {b['latency_mean_s']:.2f}s -> {v['latency_mean_s']:.2f}s (Δ {d_lat:+.1f}%) | \"\n",
        "              f\"toks/s: {b['tokens_per_sec']:.1f} -> {v['tokens_per_sec']:.1f} (Δ {d_tok:+.1f}%)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atI6kRdUifU8",
        "outputId": "37130c0e-c427-4b5a-abc3-2795fce1f85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vps/scripts/eval_suite.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "\n",
        "# vps/__init__.py\n",
        "cat > vps/__init__.py <<'PY'\n",
        "# VPS package init\n",
        "# (keep this minimal; avoid notebook magics inside files)\n",
        "from . import vpscore\n",
        "__all__ = [\"vpscore\"]\n",
        "PY\n",
        "\n",
        "# vps/vpscore/__init__.py\n",
        "cat > vps/vpscore/__init__.py <<'PY'\n",
        "from .config import VPSConfig\n",
        "from .patch_hf import patch_model_with_vps\n",
        "from .vps_linear import VPSLinear\n",
        "PY\n",
        "\n",
        "# vps/vpscore/utils/__init__.py\n",
        "cat > vps/vpscore/utils/__init__.py <<'PY'\n",
        "from .generation import generate\n",
        "PY\n",
        "\n",
        "# vps/vpscore/verifiers/__init__.py\n",
        "cat > vps/vpscore/verifiers/__init__.py <<'PY'\n",
        "from .composite_verifier import CompositeVerifier\n",
        "PY\n",
        "\n",
        "# vps/vpscore/data/__init__.py\n",
        "cat > vps/vpscore/data/__init__.py <<'PY'\n",
        "# data pkg\n",
        "PY\n"
      ],
      "metadata": {
        "id": "hSysBYxFjcY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "mods = [m for m in list(sys.modules) if m == \"vps\" or m.startswith(\"vps.\")]\n",
        "for m in mods:\n",
        "    sys.modules.pop(m, None)\n",
        "print(\"Cleared:\", mods)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAzrAOlZjdU0",
        "outputId": "498bd142-522f-4c34-bf4e-fcf3959721bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vps/vpscore/patch_hf.py\n",
        "from __future__ import annotations\n",
        "import weakref\n",
        "import torch.nn as nn\n",
        "from typing import List, Any\n",
        "from .vps_linear import VPSLinear\n",
        "\n",
        "# Canonical buckets and their common alias names inside HF models\n",
        "NAMES = {\n",
        "    \"attn_q\": [\"q_proj\", \"wq\", \"query\", \"q\"],\n",
        "    \"attn_k\": [\"k_proj\", \"wk\", \"key\", \"k\"],\n",
        "    \"attn_v\": [\"v_proj\", \"wv\", \"value\", \"v\"],\n",
        "    \"attn_o\": [\"o_proj\", \"wo\", \"out_proj\", \"o\"],\n",
        "    \"mlp_up\": [\"up_proj\", \"w1\", \"fc_in\", \"dense_h_to_4h\", \"gate_proj\", \"w3\", \"gate\"],\n",
        "    \"mlp_down\": [\"down_proj\", \"w2\", \"fc_out\", \"dense_4h_to_h\"],\n",
        "}\n",
        "\n",
        "# Global registry for Q/K pairing across modules (keyed by parent block id)\n",
        "PAIR_REGISTRY = {}\n",
        "\n",
        "def _normalize_apply_to(apply_to: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Accept both bucket keys (e.g., 'attn_q') and raw aliases (e.g., 'q_proj').\n",
        "    Return a list of canonical bucket keys that should be applied.\n",
        "    \"\"\"\n",
        "    if not apply_to:\n",
        "        return []\n",
        "    want = set()\n",
        "    a2 = [s.lower() for s in apply_to]\n",
        "    for key, aliases in NAMES.items():\n",
        "        if key in a2 or any(alias in a2 for alias in aliases):\n",
        "            want.add(key)\n",
        "    return list(want)\n",
        "\n",
        "def _match_bucket(child_name: str, wanted_buckets: List[str]):\n",
        "    \"\"\"\n",
        "    Return the canonical bucket key for this named child if it should be wrapped.\n",
        "    \"\"\"\n",
        "    lname = child_name.lower()\n",
        "    for key in wanted_buckets:\n",
        "        aliases = NAMES.get(key, [])\n",
        "        if any(alias in lname for alias in aliases):\n",
        "            return key\n",
        "    return None\n",
        "\n",
        "def _wrap(module: nn.Module, wanted_buckets: List[str], vps_kwargs: dict, counter: dict):\n",
        "    \"\"\"\n",
        "    Recursively replace selected nn.Linear children with VPSLinear.\n",
        "    Also record Q/K modules per parent block for optional coupling.\n",
        "    \"\"\"\n",
        "    for name, child in list(module.named_children()):\n",
        "        # Recurse first\n",
        "        _wrap(child, wanted_buckets, vps_kwargs, counter)\n",
        "\n",
        "        # Replace target Linear layers\n",
        "        if isinstance(child, nn.Linear):\n",
        "            bucket = _match_bucket(name, wanted_buckets)\n",
        "            if bucket is not None:\n",
        "                vps = VPSLinear(child, **vps_kwargs)\n",
        "                setattr(module, name, vps)  # install wrapper\n",
        "                counter[\"wrapped\"] += 1\n",
        "\n",
        "                # Track possible Q/K pairs under the parent block\n",
        "                if bucket in (\"attn_q\", \"attn_k\"):\n",
        "                    key = id(module)\n",
        "                    PAIR_REGISTRY.setdefault(key, {})[bucket] = vps\n",
        "\n",
        "def patch_model_with_vps(model: nn.Module, apply_to: List[str], *args: Any, **kwargs: Any):\n",
        "    \"\"\"\n",
        "    Flexible wrapper: accepts either\n",
        "      A) long-form positional:\n",
        "         (model, apply_to, rank, topk, clamp, gamma, builder, [softgrad_builder=False], policy_cfg=None)\n",
        "         or extended:\n",
        "         (model, apply_to, rank, topk, clamp, gamma, builder, order, qk_coupling, tau,\n",
        "          lbfgs_enabled, adaptive_rank, adaptive_gamma, [softgrad_builder=False], policy_cfg=None)\n",
        "\n",
        "      B) cfg-form:\n",
        "         (model, apply_to, cfg)   where cfg is VPSConfig\n",
        "    \"\"\"\n",
        "    # Defaults\n",
        "    softgrad_builder = kwargs.pop(\"softgrad_builder\", False)\n",
        "    policy_cfg = kwargs.pop(\"policy_cfg\", None)\n",
        "    qk_coupling_flag = False\n",
        "\n",
        "    # ---- Parse inputs ----\n",
        "    if len(args) == 1 and not isinstance(args[0], (int, float, str, bool)):\n",
        "        # cfg-form\n",
        "        cfg = args[0]\n",
        "        rank   = getattr(cfg, \"rank\", 2)\n",
        "        topk   = getattr(cfg, \"topk\", 32)\n",
        "        clamp  = getattr(cfg, \"clamp\", None)\n",
        "        gamma  = getattr(cfg, \"gamma\", 0.5)\n",
        "        builder= getattr(cfg, \"builder\", \"hybrid\")\n",
        "        softgrad_builder = getattr(cfg, \"softgrad_builder\", softgrad_builder)\n",
        "        policy_cfg = cfg\n",
        "        qk_coupling_flag = bool(getattr(cfg, \"qk_coupling\", False))\n",
        "    else:\n",
        "        # long-form(s)\n",
        "        if len(args) >= 6:\n",
        "            rank, topk, clamp, gamma, builder = args[0:6]\n",
        "            if len(args) >= 12:\n",
        "                # args[6] is order (unused here), args[7] is qk_coupling\n",
        "                qk_coupling_flag = bool(args[7])\n",
        "            if len(args) >= 13 and isinstance(args[12], bool):\n",
        "                softgrad_builder = args[12]\n",
        "            if len(args) >= 14:\n",
        "                policy_cfg = args[13]\n",
        "        else:\n",
        "            raise TypeError(\n",
        "                \"patch_model_with_vps: unsupported arguments. \"\n",
        "                \"Pass VPSConfig as third arg OR full long-form params.\"\n",
        "            )\n",
        "\n",
        "    vps_kwargs = dict(\n",
        "        rank=rank,\n",
        "        topk=topk,\n",
        "        clamp=clamp,\n",
        "        gamma=gamma,\n",
        "        builder=builder,\n",
        "        softgrad_builder=softgrad_builder,\n",
        "        policy_cfg=policy_cfg,\n",
        "    )\n",
        "\n",
        "    wanted_buckets = _normalize_apply_to(apply_to or [])\n",
        "    counter = {\"wrapped\": 0}\n",
        "\n",
        "    # Actually swap in VPSLinear for selected Linear layers\n",
        "    _wrap(model, wanted_buckets, vps_kwargs, counter)\n",
        "\n",
        "    # Finalize Q/K pairing if enabled — use WEAKREFS to avoid module graph cycles\n",
        "    if qk_coupling_flag:\n",
        "        for pair in PAIR_REGISTRY.values():\n",
        "            q = pair.get(\"attn_q\")\n",
        "            k = pair.get(\"attn_k\")\n",
        "            if q is not None and k is not None:\n",
        "                # flags are plain attrs\n",
        "                q.is_Q = True\n",
        "                k.is_K = True\n",
        "                # store weak references (NOT modules) to avoid child-module cycles\n",
        "                q._peer_ref = weakref.ref(k)\n",
        "                k._peer_ref = weakref.ref(q)\n",
        "                # ensure any accidental strong refs don't linger\n",
        "                if hasattr(q, \"_peer\"): delattr(q, \"_peer\")\n",
        "                if hasattr(k, \"_peer\"): delattr(k, \"_peer\")\n",
        "\n",
        "    try:\n",
        "        print(f\"[VPS] Wrapped layers: {counter['wrapped']}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmGC1ISNkMRq",
        "outputId": "77a7da43-280f-47db-f5cb-4e975ccd7519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vps/vpscore/patch_hf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any previously-imported broken modules\n",
        "import sys\n",
        "for m in [m for m in list(sys.modules) if m == \"vps\" or m.startswith(\"vps.\")]:\n",
        "    sys.modules.pop(m, None)\n",
        "print(\"Cleared vps imports\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNmbw7ObkNOx",
        "outputId": "aab1069a-8f5b-41be-b595-2b828eeaf1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared vps imports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=/content:$PYTHONPATH TF_CPP_MIN_LOG_LEVEL=3 \\\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 40 --n_bbh_date 40 --n_arc_c 40 \\\n",
        "  --max_new_tokens 48 --seed 1234 \\\n",
        "  --print_every 1 --self_consistency_weight 0.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9DO5GXhkPn6",
        "outputId": "0f52f518-9bd1-4962-9f6e-b6694770d61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: eval_suite.py [-h] [--n_gsm8k N_GSM8K] [--n_bbh_date N_BBH_DATE]\n",
            "                     [--n_arc_c N_ARC_C] [--seed SEED]\n",
            "                     [--max_new_tokens MAX_NEW_TOKENS]\n",
            "eval_suite.py: error: unrecognized arguments: --print_every 1 --self_consistency_weight 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cat > vps/scripts/eval_suite.py <<'PY'\n",
        "import argparse, time, re, random, statistics as stats, sys\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import set_seed\n",
        "\n",
        "from vps.scripts.infer_vps import build          # reuse your build()\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.hooks import HookManager\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "\n",
        "NUM_RE = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "def _extract_number(s: str):\n",
        "    m = list(NUM_RE.finditer(s or \"\"))\n",
        "    return float(m[-1].group(1)) if m else None\n",
        "\n",
        "def _normalize(s: str):\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "def _extract_choice_letter(s: str):\n",
        "    s = s.strip()\n",
        "    m = re.findall(r\"\\b([A-E])\\b\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    m = re.findall(r\"\\(([A-E])\\)\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    return None\n",
        "\n",
        "def _answer_is_correct(task, pred, gold):\n",
        "    if task == \"gsm8k\":\n",
        "        pn, gn = _extract_number(pred), _extract_number(gold)\n",
        "        return (pn is not None and gn is not None and abs(pn - gn) < 1e-6)\n",
        "    elif task == \"bbh_date\":\n",
        "        return _normalize(pred) == _normalize(gold)\n",
        "    elif task == \"arc_c\":\n",
        "        return _extract_choice_letter(pred) == str(gold).upper()\n",
        "    return False\n",
        "\n",
        "def _format_prompt(task, x):\n",
        "    if task == \"gsm8k\":\n",
        "        q = x[\"question\"] if \"question\" in x else x[\"question_text\"]\n",
        "        return f\"Solve the problem. Return ONLY the final number.\\n\\nProblem: {q}\\nAnswer:\"\n",
        "    if task == \"bbh_date\":\n",
        "        return f\"Answer correctly. Return ONLY the final answer string.\\n\\n{ x['input'] }\\nAnswer:\"\n",
        "    if task == \"arc_c\":\n",
        "        q = x[\"question\"]\n",
        "        labels = x[\"choices\"][\"label\"]\n",
        "        texts  = x[\"choices\"][\"text\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, texts)])\n",
        "        return (\"You are given a multiple-choice science question. \"\n",
        "                \"Return ONLY the correct option letter (A, B, C, D, or E).\\n\\n\"\n",
        "                f\"{q}\\n\\n{opts}\\n\\nAnswer:\")\n",
        "    raise ValueError(task)\n",
        "\n",
        "def _gold_text(task, x):\n",
        "    if task == \"gsm8k\":\n",
        "        ans = x[\"answer\"]\n",
        "        m = re.search(r\"####\\s*(.+)\", ans)\n",
        "        return m.group(1).strip() if m else ans.strip()\n",
        "    if task == \"bbh_date\":\n",
        "        return str(x[\"target\"]).strip()\n",
        "    if task == \"arc_c\":\n",
        "        return str(x[\"answerKey\"]).strip().upper()\n",
        "    raise ValueError(task)\n",
        "\n",
        "def vps_iterate_once(model, tok, hooks, prompt:str, gold_text:str, max_new_tokens:int):\n",
        "    hooks.clear_buffers()\n",
        "\n",
        "    # entropy from prompt logits\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.enable_grad():\n",
        "        out = model(**inputs)\n",
        "        logits = out.logits\n",
        "        entropy = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            m.policy.set_token_entropy(entropy)\n",
        "\n",
        "    # deterministic decode for verifier text\n",
        "    out_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=min(64, max_new_tokens),\n",
        "        do_sample=False,\n",
        "        return_dict_in_generate=True,\n",
        "    )\n",
        "    pred_text = tok.decode(out_ids.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # light verifier (no self-consistency to keep it fast)\n",
        "    verifier = CompositeVerifier({\"numeric\":1.0,\"units\":0.5,\"algebraic\":0.2,\"self_consistency\":0.0})\n",
        "    _ = verifier.compute_loss(pred_text, gold_text)\n",
        "\n",
        "    # short CE surrogate to populate grads\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    with torch.enable_grad():\n",
        "        out2 = model(**inputs)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            target.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "\n",
        "    # final VPS-enhanced generation (greedy)\n",
        "    final = generate(model, tok, prompt, max_new_tokens=max_new_tokens,\n",
        "                     temperature=0.0, top_p=1.0, do_sample=False)\n",
        "    return final\n",
        "\n",
        "def run_phase(tasks, n_map, seed, vps_on, max_new_tokens, print_every=10):\n",
        "    set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Build once per phase\n",
        "    cfg = VPSConfig()\n",
        "    cfg.dtype = \"fp16\"\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    if not vps_on:\n",
        "        cfg.gamma = 0.0\n",
        "        cfg.adaptive_gamma = False\n",
        "\n",
        "    print(f\"\\n[Phase] {'VPS ON' if vps_on else 'BASELINE (VPS OFF)'} — building model...\", flush=True)\n",
        "    tok, model, hooks = build(cfg)\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for task in tasks:\n",
        "        n = int(n_map.get(task, 0))\n",
        "        if n <= 0:\n",
        "            print(f\"\\n=== {('VPS' if vps_on else 'BASELINE')} :: {task} n=0 — skipping ===\", flush=True)\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n=== {('VPS' if vps_on else 'BASELINE')} :: {task} n={n} ===\", flush=True)\n",
        "\n",
        "        # dataset load\n",
        "        if task == \"gsm8k\":\n",
        "            ds = load_dataset(\"gsm8k\", \"main\")[\"test\"]\n",
        "        elif task == \"bbh_date\":\n",
        "            ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\")[\"test\"]\n",
        "        elif task == \"arc_c\":\n",
        "            ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\")[\"test\"]\n",
        "        else:\n",
        "            raise ValueError(task)\n",
        "\n",
        "        print(f\"[{task}] dataset size={len(ds)}; sampling n={n}\", flush=True)\n",
        "\n",
        "        # sample n items\n",
        "        idxs = list(range(len(ds)))\n",
        "        random.shuffle(idxs)\n",
        "        idxs = idxs[:n]\n",
        "\n",
        "        correct = 0\n",
        "        latencies = []\n",
        "        tokens_gen = []\n",
        "        ema_dt = None\n",
        "\n",
        "        for i, idx in enumerate(idxs, 1):\n",
        "            ex = ds[idx]\n",
        "            prompt = _format_prompt(task, ex)\n",
        "            gold   = _gold_text(task, ex)\n",
        "\n",
        "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "            t0 = time.time()\n",
        "\n",
        "            if vps_on:\n",
        "                pred = vps_iterate_once(model, tok, hooks, prompt, gold, max_new_tokens)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens=max_new_tokens,\n",
        "                                temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "            dt = time.time() - t0\n",
        "            latencies.append(dt)\n",
        "            ema_dt = dt if ema_dt is None else (0.9*ema_dt + 0.1*dt)\n",
        "\n",
        "            out_ids = tok(pred, return_tensors=\"pt\")[\"input_ids\"]\n",
        "            tokens_gen.append(int(out_ids.shape[1]))\n",
        "\n",
        "            ok = _answer_is_correct(task, pred, gold)\n",
        "            correct += int(ok)\n",
        "\n",
        "            if (i % print_every == 0) or (i in (1, n)):\n",
        "                rem = n - i\n",
        "                eta = rem * (ema_dt or dt)\n",
        "                mins = int(eta // 60); secs = int(eta % 60)\n",
        "                print(f\"[{task}] {i}/{n} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {mins}m{secs}s\",\n",
        "                      flush=True)\n",
        "\n",
        "        acc = correct / n\n",
        "        t_mean = stats.mean(latencies) if latencies else 0.0\n",
        "        tokps = (sum(tokens_gen) / sum(latencies)) if sum(latencies) > 0 else 0.0\n",
        "\n",
        "        res = {\n",
        "            \"task\": task,\n",
        "            \"n\": n,\n",
        "            \"acc\": acc,\n",
        "            \"latency_mean_s\": t_mean,\n",
        "            \"tokens_per_sec\": tokps,\n",
        "        }\n",
        "        print(res, flush=True)\n",
        "        results.append(res)\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=100)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=100)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=100)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=64)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=10)  # NEW but optional\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    tasks = [\"gsm8k\", \"bbh_date\", \"arc_c\"]\n",
        "    n_map = {\"gsm8k\": args.n_gsm8k, \"bbh_date\": args.n_bbh_date, \"arc_c\": args.n_arc_c}\n",
        "\n",
        "    # Phase 1: Baseline\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens,\n",
        "                  print_every=args.print_every)\n",
        "\n",
        "    # Phase 2: VPS\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens,\n",
        "                  print_every=args.print_every)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "4UW7yLNVci4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "m = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "print(\"Downloading model + tokenizer to local cache...\", flush=True)\n",
        "AutoTokenizer.from_pretrained(m, use_fast=True)\n",
        "AutoModelForCausalLM.from_pretrained(m, device_map=\"cpu\")\n",
        "print(\"Done. Subsequent runs will start instantly.\", flush=True)\n",
        "PY\n",
        "\n"
      ],
      "metadata": {
        "id": "RCNAsQ4kcjzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de3c11a-1e42-473a-909c-997f3e4957c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%bash\n",
        "set -e\n",
        "python - <<'PY'\n",
        "import sys, os\n",
        "os.environ[\"PYTHONUNBUFFERED\"]=\"1\"\n",
        "print(\"[diag] importing eval_suite...\", flush=True)\n",
        "from vps.scripts.eval_suite import run_phase\n",
        "print(\"[diag] import OK. Running 2 GSM8K items baseline...\", flush=True)\n",
        "_ = run_phase([\"gsm8k\"], {\"gsm8k\": 2}, seed=1234, vps_on=False, max_new_tokens=24, print_every=1)\n",
        "print(\"[diag] baseline OK. Now VPS ON (2 items)...\", flush=True)\n",
        "_ = run_phase([\"gsm8k\"], {\"gsm8k\": 2}, seed=1234, vps_on=True, max_new_tokens=24, print_every=1)\n",
        "print(\"[diag] done.\", flush=True)\n",
        "PY\n"
      ],
      "metadata": {
        "id": "Cx0xRexgiTir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4155474-abaa-43c5-f3fe-faeb6be8c7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "# ensure \"vps/scripts\" is a package\n",
        "mkdir -p vps/scripts\n",
        "if [ ! -f vps/scripts/__init__.py ]; then\n",
        "  touch vps/scripts/__init__.py\n",
        "fi\n",
        "echo \"ok\"\n"
      ],
      "metadata": {
        "id": "JxH9iOERjqzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd03670-fbb4-4b34-d7e2-f711161a3234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "cat > vps/scripts/infer_vps.py <<'PY'\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ---- robust imports: package-first, legacy fallback ----\n",
        "try:\n",
        "    from vps.vpscore.config import VPSConfig\n",
        "    from vps.vpscore.patch_hf import patch_model_with_vps\n",
        "    from vps.vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "    from vps.vpscore.utils.generation import generate\n",
        "    from vps.vpscore.hooks import HookManager\n",
        "    from vps.vpscore.math_utils import compute_token_entropy\n",
        "except Exception:\n",
        "    # allow running with PYTHONPATH=/content/vps\n",
        "    from vpscore.config import VPSConfig\n",
        "    from vpscore.patch_hf import patch_model_with_vps\n",
        "    from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "    from vpscore.utils.generation import generate\n",
        "    from vpscore.hooks import HookManager\n",
        "    from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "DEFAULT_VERIFIER_WEIGHTS = {\n",
        "    \"exact_match\": 0.8,\n",
        "    \"coherence\": 0.2,\n",
        "    \"length_penalty\": 0.0,\n",
        "}\n",
        "\n",
        "def build(cfg: VPSConfig):\n",
        "    # ---- robust fallbacks so missing config fields don't crash ----\n",
        "    seed = getattr(cfg, \"seed\", 1234)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    dtype_name = getattr(cfg, \"dtype\", \"bf16\")\n",
        "    _dtype_map = {\"bf16\": torch.bfloat16, \"fp16\": torch.float16, \"fp32\": torch.float32}\n",
        "    dtype = _dtype_map.get(dtype_name, torch.bfloat16)\n",
        "\n",
        "    model_name = getattr(cfg, \"model_name\", None)\n",
        "    if not model_name:\n",
        "        raise ValueError(\"cfg.model_name is not set in VPSConfig\")\n",
        "\n",
        "    device_map = getattr(cfg, \"device_map\", \"auto\")\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.pad_token_id = tok.eos_token_id\n",
        "\n",
        "    # Use dtype= (not torch_dtype=) to avoid the deprecation warning.\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, dtype=dtype, device_map=device_map)\n",
        "\n",
        "    # Your patcher: the variant we standardized earlier takes (model, apply_to, cfg)\n",
        "    apply_to = getattr(cfg, \"apply_to\", None)\n",
        "    model = patch_model_with_vps(model, apply_to, cfg)\n",
        "\n",
        "    hooks = HookManager()\n",
        "    hooks.attach(model)\n",
        "    return tok, model, hooks\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--prompt\", type=str, required=True)\n",
        "    ap.add_argument(\"--gold\", type=str, default=None)\n",
        "    ap.add_argument(\"--iters\", type=int, default=3)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    cfg = VPSConfig()\n",
        "\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Fall back if cfg.verifier_weights missing\n",
        "    verifier_weights = getattr(cfg, \"verifier_weights\", DEFAULT_VERIFIER_WEIGHTS)\n",
        "    verifier = CompositeVerifier(verifier_weights)\n",
        "\n",
        "    # Also fallbacks for gen params\n",
        "    max_new_tokens = getattr(cfg, \"max_new_tokens\", 128)\n",
        "    temperature = getattr(cfg, \"temperature\", 0.7)\n",
        "    top_p = getattr(cfg, \"top_p\", 0.9)\n",
        "\n",
        "    # ----------------- Iteration 0 (plain decode) -----------------\n",
        "    text = generate(model, tok, args.prompt, max_new_tokens, temperature, top_p)\n",
        "    print(f\"=== Iteration 0 ===\\n{text}\\n\")\n",
        "\n",
        "    if args.gold is None or args.iters < 2:\n",
        "        return\n",
        "\n",
        "    prev_loss = float(\"inf\")\n",
        "\n",
        "    for it in range(1, args.iters):\n",
        "        hooks.clear_buffers()\n",
        "\n",
        "        # Clear any L-BFGS memories (keeps your technique intact)\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"clear_lbfgs\"):\n",
        "                m.clear_lbfgs()\n",
        "\n",
        "        # Forward pass to compute token entropy for the adaptive policy\n",
        "        inputs = tok(args.prompt, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.enable_grad():\n",
        "            out = model(**inputs)\n",
        "            logits = out.logits\n",
        "            entropy = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "        # Feed entropy to policies (again, preserving your mechanism)\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                m.policy.set_token_entropy(entropy)\n",
        "\n",
        "        # Deterministic decode for verifier evaluation\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=min(64, max_new_tokens),\n",
        "            do_sample=False,\n",
        "            return_dict_in_generate=True,\n",
        "        )\n",
        "        pred_text = tok.decode(out_ids.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        # Multi-objective verifier (unchanged logic)\n",
        "        losses = verifier.compute_loss(pred_text, args.gold, model, tok, args.prompt)\n",
        "        vloss = losses[\"total\"]\n",
        "\n",
        "        # Feedback loop to policy\n",
        "        improved = vloss < prev_loss\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                m.policy.update_outcome(improved, prev_loss - vloss)\n",
        "        prev_loss = vloss\n",
        "\n",
        "        # Short CE surrogate to populate grads → VPS uses grad_h\n",
        "        gold_ids = tok(args.gold, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "        T = min(8, gold_ids.shape[0])\n",
        "        target = gold_ids[-T:].unsqueeze(0)  # [1, T]\n",
        "\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        with torch.enable_grad():\n",
        "            out2 = model(**inputs)\n",
        "            logits2 = out2.logits[:, -T:, :]  # [1, T, V]\n",
        "            ce = torch.nn.functional.cross_entropy(\n",
        "                logits2.reshape(-1, logits2.size(-1)),\n",
        "                target.reshape(-1)\n",
        "            )\n",
        "            ce.backward()\n",
        "\n",
        "        # VPS-enhanced generation\n",
        "        text2 = generate(model, tok, args.prompt, max_new_tokens, temperature, top_p)\n",
        "        print(f\"=== Iteration {it} (VPS enhanced) ===\")\n",
        "        print(f\"Losses: {losses}\")\n",
        "        print(f\"Output:\\n{text2}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n",
        "echo \"infer_vps.py rewritten\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJzCTI7gjui7",
        "outputId": "4e0b989d-f211-4c48-e047-fe5f0d8c3788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "infer_vps.py rewritten\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%bash\n",
        "python - <<'PY'\n",
        "from pathlib import Path, re\n",
        "p = Path('vps/vpscore/config.py')\n",
        "s = p.read_text()\n",
        "# Use 3B for more headroom (still fits on T4)\n",
        "s = s.replace('Qwen/Qwen2.5-1.5B-Instruct','Qwen/Qwen2.5-3B-Instruct')\n",
        "# Make VPS slightly more expressive + stable for MC tasks\n",
        "s = s.replace('rank: int = 2','rank: int = 4')\\\n",
        "     .replace('topk: int = 32','topk: int = 64')\\\n",
        "     .replace('order: int = 1','order: int = 2')\\\n",
        "     .replace('gamma: float = 0.5','gamma: float = 0.6')\\\n",
        "     .replace('temperature: float = 0.2','temperature: float = 0.0')  # greedy for MC stability\n",
        "# Ensure clamp is on (prevents rare spikes)\n",
        "s = s.replace('clamp: Optional[float] = None','clamp: Optional[float] = 0.2')\n",
        "p.write_text(s)\n",
        "print(\"VPS config tuned for MC/short reasoning ✅\")\n",
        "PY\n"
      ],
      "metadata": {
        "id": "GvUu4j8IlLZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3d9a16-3543-4291-8c85-fc7018fae7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPS config tuned for MC/short reasoning ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "# Colab/T4-friendly evaluation with progress + OOM-safe VPS pass.\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your build() and utilities exactly\n",
        "from vps.scripts.infer_vps import build             # creates (tok, model, hooks)\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: toggle VPS on / off\n",
        "# -----------------------------\n",
        "def set_vps_enabled(model: torch.nn.Module, enabled: bool):\n",
        "    \"\"\"\n",
        "    Make the VPS delta inert for baseline by forcing gamma=0 and disabling adaptivity.\n",
        "    We do NOT remove wrappers; we just neutralize them. This keeps your patching intact.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            # Hard-disable effect for baseline\n",
        "            if not enabled:\n",
        "                if hasattr(m, \"cfg\"):\n",
        "                    m.cfg.adaptive_gamma = False\n",
        "                    m.cfg.gamma = 0.0\n",
        "                if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                    # keep policy object but pin its gamma via cfg above\n",
        "                    m.policy.cfg = m.cfg\n",
        "            else:\n",
        "                # Re-enable whatever the layer already had (do nothing)\n",
        "                if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                    m.policy.cfg = m.cfg\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# OOM-safe single VPS \"iteration\" (no grads)\n",
        "# ------------------------------------------\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold: str | None, max_new_tokens: int):\n",
        "    \"\"\"\n",
        "    A light VPS pass that:\n",
        "      1) does a forward pass (no grad) to get token entropy,\n",
        "      2) feeds entropy to per-layer policies (so they can adjust),\n",
        "      3) runs greedy generation with VPS active.\n",
        "    NOTE: we intentionally do NOT build any backward graph here (prevents OOM).\n",
        "    \"\"\"\n",
        "    hooks.clear_buffers()\n",
        "\n",
        "    # Step 1: entropy from a cheap forward (no cache -> lower mem)\n",
        "    with torch.no_grad():\n",
        "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        out = model(**inputs, use_cache=False, return_dict=True)\n",
        "        logits = out.logits\n",
        "        ent = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "    # Step 2: feed entropy to policies\n",
        "    ent_val = float(ent) if torch.is_tensor(ent) else float(ent)\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(ent_val)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Step 3: VPS-influenced generation (greedy for MC stability)\n",
        "    pred_text = generate(\n",
        "        model, tok, prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.0, top_p=1.0, do_sample=False\n",
        "    )\n",
        "    return pred_text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ARC-Challenge prompt helpers\n",
        "# -----------------------------\n",
        "def format_arc_prompt(q: str, choices: List[str]) -> str:\n",
        "    letters = \"ABCDE\"\n",
        "    lines = [f\"Question: {q}\", \"Options:\"]\n",
        "    for i, c in enumerate(choices):\n",
        "        lines.append(f\"({letters[i]}) {c}\")\n",
        "    lines.append(\"Answer:\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def extract_arc_choice_letter(text: str) -> str | None:\n",
        "    # Find last capital A–E in the output (common HF eval trick)\n",
        "    for ch in reversed(text.strip()):\n",
        "        if ch in \"ABCDE\":\n",
        "            return ch\n",
        "    return None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# BBH date prompt helpers\n",
        "# -----------------------------\n",
        "def format_bbh_date_prompt(q: str) -> str:\n",
        "    # The BBH prompts are usually already formatted; we just ensure a clear cue.\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# GSM8K prompt helpers (short)\n",
        "# -----------------------------\n",
        "def format_gsm8k_prompt(q: str) -> str:\n",
        "    # Keep short question → short answer; no CoT for speed/consistency.\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Task runners\n",
        "# -----------------------------\n",
        "def run_arc_c(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[arc_c] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"question\"]\n",
        "        choices = item[\"choices\"][\"text\"]\n",
        "        labels = item[\"choices\"][\"label\"]\n",
        "        gold_letter = item[\"answerKey\"].strip()\n",
        "\n",
        "        prompt = format_arc_prompt(q, choices)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gold_letter, max_new_tokens)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        # decode letter\n",
        "        got = extract_arc_choice_letter(pred) or \"\"\n",
        "        if got == gold_letter:\n",
        "            correct += 1\n",
        "\n",
        "        # token estimate (approx decode length)\n",
        "        gen_token_est += len(tok.encode(pred))  # rough but consistent\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            # naive ETA based on last example duration\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[arc_c] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"arc_c\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "def run_bbh_date(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[bbh_date] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"input\"]\n",
        "        gold = item[\"target\"].strip()\n",
        "\n",
        "        prompt = format_bbh_date_prompt(q)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gold, max_new_tokens=24)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        got = pred.strip().splitlines()[-1].strip()\n",
        "        if got == gold:\n",
        "            correct += 1\n",
        "\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[bbh_date] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"bbh_date\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "def run_gsm8k(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    # Use the short \"main\" split (1319 samples)\n",
        "    ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[gsm8k] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    # Tiny numeric extractor (last number)\n",
        "    import re\n",
        "    num_re = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "    def extract_num(s: str):\n",
        "        m = list(num_re.finditer(s))\n",
        "        return m[-1].group(1) if m else None\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"question\"]\n",
        "        gold_text = item[\"answer\"]\n",
        "        # gold is \"#### 123\" format; pull the number\n",
        "        gnum = extract_num(gold_text)\n",
        "\n",
        "        prompt = format_gsm8k_prompt(q)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gnum, max_new_tokens)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        pnum = extract_num(pred)\n",
        "        if pnum is not None and gnum is not None:\n",
        "            try:\n",
        "                correct += float(pnum) == float(gnum)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[gsm8k] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"gsm8k\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Phase runner (build once)\n",
        "# -----------------------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str, int], seed: int, vps_on: bool,\n",
        "              max_new_tokens: int, print_every: int) -> List[Dict]:\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\")\n",
        "\n",
        "    # Build model (your build() already patches; we neutralize for baseline below)\n",
        "    cfg_overrides = None  # keep your defaults\n",
        "    tok, model, hooks = build(cfg_overrides)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(0)\n",
        "\n",
        "    # Make baseline inert\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    model.eval()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"[Phase] model ready.\")\n",
        "\n",
        "    results: List[Dict] = []\n",
        "\n",
        "    for task in tasks_order:\n",
        "        n = int(n_map.get(task, 0))\n",
        "\n",
        "        if n <= 0:\n",
        "            print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n={n} ===\")\n",
        "\n",
        "        if task == \"arc_c\":\n",
        "            res = run_arc_c(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        elif task == \"bbh_date\":\n",
        "            res = run_bbh_date(model, tok, hooks, n, print_every, max_new_tokens=min(24, max_new_tokens))\n",
        "        elif task == \"gsm8k\":\n",
        "            res = run_gsm8k(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        else:\n",
        "            print(f\"[warn] unknown task '{task}', skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(res)\n",
        "        results.append(res)\n",
        "\n",
        "        # light GC between tasks\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=100)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=10)\n",
        "    return ap.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    # Recommended on Colab to reduce fragmentation\n",
        "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "    tasks_order = []\n",
        "    n_map = {}\n",
        "\n",
        "    if args.n_gsm8k >= 0:\n",
        "        tasks_order.append(\"gsm8k\")\n",
        "        n_map[\"gsm8k\"] = args.n_gsm8k\n",
        "    if args.n_bbh_date >= 0:\n",
        "        tasks_order.append(\"bbh_date\")\n",
        "        n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    if args.n_arc_c >= 0:\n",
        "        tasks_order.append(\"arc_c\")\n",
        "        n_map[\"arc_c\"] = args.n_arc_c\n",
        "\n",
        "    # BASELINE (VPS OFF)\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "\n",
        "    # VPS ON\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "NIgOksrzmYx7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "8bfebf19-eabc-4ff9-8fa4-7336faf37b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--n_gsm8k N_GSM8K]\n",
            "                                [--n_bbh_date N_BBH_DATE] [--n_arc_c N_ARC_C]\n",
            "                                [--seed SEED]\n",
            "                                [--max_new_tokens MAX_NEW_TOKENS]\n",
            "                                [--print_every PRINT_EVERY]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-f94432f4-3a92-45ce-9723-3e91da0b75d6.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "# Colab/T4-friendly evaluation with progress + OOM-safe VPS pass.\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "from typing import Dict, List\n",
        "\n",
        "# --- Make top-level 'vpscore' imports work no matter how it's launched ---\n",
        "# This points to the repo's /content/vps folder (parent of this scripts/).\n",
        "try:\n",
        "    import pathlib\n",
        "    PROJ_ROOT = str(pathlib.Path(__file__).resolve().parents[1])  # .../vps\n",
        "except Exception:\n",
        "    PROJ_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
        "if PROJ_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJ_ROOT)\n",
        "os.environ[\"PYTHONPATH\"] = PROJ_ROOT + \":\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your build() and utilities exactly\n",
        "from vps.scripts.infer_vps import build             # creates (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: toggle VPS on / off\n",
        "# -----------------------------\n",
        "def set_vps_enabled(model: torch.nn.Module, enabled: bool):\n",
        "    \"\"\"\n",
        "    Make the VPS delta inert for baseline by forcing gamma=0 and disabling adaptivity.\n",
        "    We do NOT remove wrappers; we just neutralize them.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not enabled:\n",
        "                if hasattr(m, \"cfg\"):\n",
        "                    m.cfg.adaptive_gamma = False\n",
        "                    m.cfg.gamma = 0.0\n",
        "                if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                    m.policy.cfg = m.cfg\n",
        "            else:\n",
        "                if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                    m.policy.cfg = m.cfg\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# OOM-safe single VPS \"iteration\" (no grads)\n",
        "# ------------------------------------------\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold: str | None, max_new_tokens: int):\n",
        "    \"\"\"\n",
        "    Light VPS pass that:\n",
        "      1) forward pass (no grad) to get token entropy,\n",
        "      2) feed entropy to per-layer policies (adaptive tuning),\n",
        "      3) greedy generation with VPS active.\n",
        "    No backward() → avoids OOM on T4.\n",
        "    \"\"\"\n",
        "    hooks.clear_buffers()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        out = model(**inputs, use_cache=False, return_dict=True)\n",
        "        logits = out.logits\n",
        "        ent = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "    ent_val = float(ent) if torch.is_tensor(ent) else float(ent)\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(ent_val)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    pred_text = generate(\n",
        "        model, tok, prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.0, top_p=1.0, do_sample=False\n",
        "    )\n",
        "    return pred_text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ARC-Challenge helpers\n",
        "# -----------------------------\n",
        "def format_arc_prompt(q: str, choices: List[str]) -> str:\n",
        "    letters = \"ABCDE\"\n",
        "    lines = [f\"Question: {q}\", \"Options:\"]\n",
        "    for i, c in enumerate(choices):\n",
        "        lines.append(f\"({letters[i]}) {c}\")\n",
        "    lines.append(\"Answer:\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def extract_arc_choice_letter(text: str) -> str | None:\n",
        "    for ch in reversed(text.strip()):\n",
        "        if ch in \"ABCDE\":\n",
        "            return ch\n",
        "    return None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# BBH date helpers\n",
        "# -----------------------------\n",
        "def format_bbh_date_prompt(q: str) -> str:\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# GSM8K helpers\n",
        "# -----------------------------\n",
        "def format_gsm8k_prompt(q: str) -> str:\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Task runners\n",
        "# -----------------------------\n",
        "def run_arc_c(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[arc_c] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"question\"]\n",
        "        choices = item[\"choices\"][\"text\"]\n",
        "        gold_letter = item[\"answerKey\"].strip()\n",
        "\n",
        "        prompt = format_arc_prompt(q, choices)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gold_letter, max_new_tokens)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        got = extract_arc_choice_letter(pred) or \"\"\n",
        "        if got == gold_letter:\n",
        "            correct += 1\n",
        "\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[arc_c] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"arc_c\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "def run_bbh_date(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[bbh_date] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"input\"]\n",
        "        gold = item[\"target\"].strip()\n",
        "\n",
        "        prompt = format_bbh_date_prompt(q)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gold, max_new_tokens=24)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        got = pred.strip().splitlines()[-1].strip()\n",
        "        if got == gold:\n",
        "            correct += 1\n",
        "\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[bbh_date] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"bbh_date\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "def run_gsm8k(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[gsm8k] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    import re\n",
        "    num_re = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "    def extract_num(s: str):\n",
        "        m = list(num_re.finditer(s))\n",
        "        return m[-1].group(1) if m else None\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"question\"]\n",
        "        gold_text = item[\"answer\"]\n",
        "        gnum = extract_num(gold_text)\n",
        "\n",
        "        prompt = format_gsm8k_prompt(q)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gnum, max_new_tokens)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        pnum = extract_num(pred)\n",
        "        if pnum is not None and gnum is not None:\n",
        "            try:\n",
        "                correct += float(pnum) == float(gnum)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[gsm8k] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"gsm8k\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Phase runner (build once)\n",
        "# -----------------------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str, int], seed: int, vps_on: bool,\n",
        "              max_new_tokens: int, print_every: int) -> List[Dict]:\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\")\n",
        "\n",
        "    # Build model using your builder and default VPSConfig\n",
        "    tok, model, hooks = build(VPSConfig())\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(0)\n",
        "\n",
        "    # Neutralize or enable VPS effect\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    model.eval()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"[Phase] model ready.\")\n",
        "\n",
        "    results: List[Dict] = []\n",
        "\n",
        "    for task in tasks_order:\n",
        "        n = int(n_map.get(task, 0))\n",
        "\n",
        "        if n <= 0:\n",
        "            print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n={n} ===\")\n",
        "\n",
        "        if task == \"arc_c\":\n",
        "            res = run_arc_c(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        elif task == \"bbh_date\":\n",
        "            res = run_bbh_date(model, tok, hooks, n, print_every, max_new_tokens=min(24, max_new_tokens))\n",
        "        elif task == \"gsm8k\":\n",
        "            res = run_gsm8k(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        else:\n",
        "            print(f\"[warn] unknown task '{task}', skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(res)\n",
        "        results.append(res)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=100)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=10)\n",
        "    return ap.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "    tasks_order = []\n",
        "    n_map = {}\n",
        "\n",
        "    if args.n_gsm8k >= 0:\n",
        "        tasks_order.append(\"gsm8k\")\n",
        "        n_map[\"gsm8k\"] = args.n_gsm8k\n",
        "    if args.n_bbh_date >= 0:\n",
        "        tasks_order.append(\"bbh_date\")\n",
        "        n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    if args.n_arc_c >= 0:\n",
        "        tasks_order.append(\"arc_c\")\n",
        "        n_map[\"arc_c\"] = args.n_arc_c\n",
        "\n",
        "    # BASELINE (VPS OFF)\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "\n",
        "    # VPS ON\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "nyyim5HBs3_E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "e682c2de-33a0-4e71-d43c-fec31b138014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2796711507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mPROJ_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# .../vps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2796711507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mPROJ_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# .../vps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mPROJ_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mPROJ_ROOT\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROJ_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "# Colab/T4-friendly evaluation with progress + OOM-safe VPS pass.\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "from typing import Dict, List\n",
        "\n",
        "# --- Robust path setup: works when run as a file or via stdin/heredoc ---\n",
        "try:\n",
        "    import pathlib\n",
        "    _has_file = \"__file__\" in globals()\n",
        "    if _has_file:\n",
        "        HERE = pathlib.Path(__file__).resolve()                  # .../vps/scripts/eval_suite.py\n",
        "        PKG_ROOT = str(HERE.parents[1])                          # .../vps\n",
        "        REPO_ROOT = str(HERE.parents[2])                         # .../\n",
        "    else:\n",
        "        # e.g., executed via stdin in Colab; fall back to CWD\n",
        "        CWD = pathlib.Path(os.getcwd()).resolve()\n",
        "        # If a \"vps\" dir exists here, treat it as package root\n",
        "        if (CWD / \"vpscore\").exists() or (CWD / \"scripts\").exists():\n",
        "            PKG_ROOT = str(CWD)                                  # already inside .../vps\n",
        "            REPO_ROOT = str(CWD.parent)\n",
        "        elif (CWD / \"vps\").exists():\n",
        "            PKG_ROOT = str((CWD / \"vps\").resolve())              # .../vps\n",
        "            REPO_ROOT = str(CWD.resolve())                       # .../\n",
        "        else:\n",
        "            # last resort: just use cwd for both\n",
        "            PKG_ROOT = str(CWD)\n",
        "            REPO_ROOT = str(CWD)\n",
        "except Exception:\n",
        "    # absolute last resort if anything above fails\n",
        "    PKG_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"vps\"))\n",
        "    REPO_ROOT = os.path.abspath(os.getcwd())\n",
        "\n",
        "# Put BOTH on sys.path so that 'vps.scripts.*' and 'vpscore.*' work\n",
        "for p in (REPO_ROOT, PKG_ROOT):\n",
        "    if p and p not in sys.path:\n",
        "        sys.path.insert(0, p)\n",
        "os.environ[\"PYTHONPATH\"] = REPO_ROOT + \":\" + PKG_ROOT + \":\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your build() and utilities exactly\n",
        "from vps.scripts.infer_vps import build             # creates (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: toggle VPS on / off\n",
        "# -----------------------------\n",
        "def set_vps_enabled(model: torch.nn.Module, enabled: bool):\n",
        "    \"\"\"\n",
        "    Make the VPS delta inert for baseline by forcing gamma=0 and disabling adaptivity.\n",
        "    We do NOT remove wrappers; we just neutralize them.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not enabled:\n",
        "                if hasattr(m, \"cfg\"):\n",
        "                    m.cfg.adaptive_gamma = False\n",
        "                    m.cfg.gamma = 0.0\n",
        "                if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                    m.policy.cfg = m.cfg\n",
        "            else:\n",
        "                if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                    m.policy.cfg = m.cfg\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# OOM-safe single VPS \"iteration\" (no grads)\n",
        "# ------------------------------------------\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold: str | None, max_new_tokens: int):\n",
        "    \"\"\"\n",
        "    Light VPS pass that:\n",
        "      1) forward pass (no grad) to get token entropy,\n",
        "      2) feed entropy to per-layer policies (adaptive tuning),\n",
        "      3) greedy generation with VPS active.\n",
        "    No backward() → avoids OOM on T4.\n",
        "    \"\"\"\n",
        "    hooks.clear_buffers()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        out = model(**inputs, use_cache=False, return_dict=True)\n",
        "        logits = out.logits\n",
        "        ent = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "    ent_val = float(ent) if torch.is_tensor(ent) else float(ent)\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(ent_val)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    pred_text = generate(\n",
        "        model, tok, prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.0, top_p=1.0, do_sample=False\n",
        "    )\n",
        "    return pred_text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ARC-Challenge helpers\n",
        "# -----------------------------\n",
        "def format_arc_prompt(q: str, choices: List[str]) -> str:\n",
        "    letters = \"ABCDE\"\n",
        "    lines = [f\"Question: {q}\", \"Options:\"]\n",
        "    for i, c in enumerate(choices):\n",
        "        lines.append(f\"({letters[i]}) {c}\")\n",
        "    lines.append(\"Answer:\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def extract_arc_choice_letter(text: str) -> str | None:\n",
        "    for ch in reversed(text.strip()):\n",
        "        if ch in \"ABCDE\":\n",
        "            return ch\n",
        "    return None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# BBH date helpers\n",
        "# -----------------------------\n",
        "def format_bbh_date_prompt(q: str) -> str:\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# GSM8K helpers\n",
        "# -----------------------------\n",
        "def format_gsm8k_prompt(q: str) -> str:\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Task runners\n",
        "# -----------------------------\n",
        "def run_arc_c(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[arc_c] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"question\"]\n",
        "        choices = item[\"choices\"][\"text\"]\n",
        "        gold_letter = item[\"answerKey\"].strip()\n",
        "\n",
        "        prompt = format_arc_prompt(q, choices)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gold_letter, max_new_tokens)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        got = extract_arc_choice_letter(pred) or \"\"\n",
        "        if got == gold_letter:\n",
        "            correct += 1\n",
        "\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[arc_c] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"arc_c\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "def run_bbh_date(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[bbh_date] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"input\"]\n",
        "        gold = item[\"target\"].strip()\n",
        "\n",
        "        prompt = format_bbh_date_prompt(q)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gold, max_new_tokens=24)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        got = pred.strip().splitlines()[-1].strip()\n",
        "        if got == gold:\n",
        "            correct += 1\n",
        "\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[bbh_date] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"bbh_date\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "def run_gsm8k(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total))\n",
        "    random.shuffle(idx)\n",
        "    idx = idx[:n] if n > 0 else []\n",
        "\n",
        "    correct = 0\n",
        "    latencies: List[float] = []\n",
        "    gen_token_est: int = 0\n",
        "\n",
        "    print(f\"[gsm8k] dataset size={total}; sampling n={len(idx)}\")\n",
        "\n",
        "    import re\n",
        "    num_re = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "    def extract_num(s: str):\n",
        "        m = list(num_re.finditer(s))\n",
        "        return m[-1].group(1) if m else None\n",
        "\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        q = item[\"question\"]\n",
        "        gold_text = item[\"answer\"]\n",
        "        gnum = extract_num(gold_text)\n",
        "\n",
        "        prompt = format_gsm8k_prompt(q)\n",
        "\n",
        "        t0 = time.time()\n",
        "        pred = vps_iterate_once(model, tok, hooks, prompt, gnum, max_new_tokens)\n",
        "        dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "\n",
        "        pnum = extract_num(pred)\n",
        "        if pnum is not None and gnum is not None:\n",
        "            try:\n",
        "                correct += float(pnum) == float(gnum)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            acc_so_far = correct / i if i > 0 else 0.0\n",
        "            rem = len(idx) - i\n",
        "            eta_s = int(dt * rem)\n",
        "            print(f\"[gsm8k] {i}/{len(idx)} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "\n",
        "    acc = (correct / len(idx)) if len(idx) else 0.0\n",
        "    mean_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0\n",
        "    tps = float(gen_token_est / sum(latencies)) if latencies and gen_token_est > 0 else 0.0\n",
        "\n",
        "    return {\"task\": \"gsm8k\", \"n\": len(idx), \"acc\": acc, \"latency_mean_s\": mean_lat, \"tokens_per_sec\": tps}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Phase runner (build once)\n",
        "# -----------------------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str, int], seed: int, vps_on: bool,\n",
        "              max_new_tokens: int, print_every: int) -> List[Dict]:\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\")\n",
        "\n",
        "    # Build model using your builder and default VPSConfig\n",
        "    tok, model, hooks = build(VPSConfig())\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(0)\n",
        "\n",
        "    # Neutralize or enable VPS effect\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    model.eval()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"[Phase] model ready.\")\n",
        "\n",
        "    results: List[Dict] = []\n",
        "\n",
        "    for task in tasks_order:\n",
        "        n = int(n_map.get(task, 0))\n",
        "\n",
        "        if n <= 0:\n",
        "            print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n={n} ===\")\n",
        "\n",
        "        if task == \"arc_c\":\n",
        "            res = run_arc_c(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        elif task == \"bbh_date\":\n",
        "            res = run_bbh_date(model, tok, hooks, n, print_every, max_new_tokens=min(24, max_new_tokens))\n",
        "        elif task == \"gsm8k\":\n",
        "            res = run_gsm8k(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        else:\n",
        "            print(f\"[warn] unknown task '{task}', skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(res)\n",
        "        results.append(res)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=100)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=10)\n",
        "    return ap.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "    tasks_order = []\n",
        "    n_map = {}\n",
        "\n",
        "    if args.n_gsm8k >= 0:\n",
        "        tasks_order.append(\"gsm8k\")\n",
        "        n_map[\"gsm8k\"] = args.n_gsm8k\n",
        "    if args.n_bbh_date >= 0:\n",
        "        tasks_order.append(\"bbh_date\")\n",
        "        n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    if args.n_arc_c >= 0:\n",
        "        tasks_order.append(\"arc_c\")\n",
        "        n_map[\"arc_c\"] = args.n_arc_c\n",
        "\n",
        "    # BASELINE (VPS OFF)\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "\n",
        "    # VPS ON\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qhmqn3ffuJXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "f8ea7f02-23cd-4eec-a837-4ea0a15e2d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--n_gsm8k N_GSM8K]\n",
            "                                [--n_bbh_date N_BBH_DATE] [--n_arc_c N_ARC_C]\n",
            "                                [--seed SEED]\n",
            "                                [--max_new_tokens MAX_NEW_TOKENS]\n",
            "                                [--print_every PRINT_EVERY]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-f94432f4-3a92-45ce-9723-3e91da0b75d6.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "# Colab/T4-friendly evaluation with progress + OOM-safe VPS pass.\n",
        "from __future__ import annotations\n",
        "\n",
        "import os, sys, time, random, argparse\n",
        "from typing import Dict, List\n",
        "\n",
        "# --- Robust path setup: works when run as a file or via stdin/heredoc ---\n",
        "try:\n",
        "    import pathlib\n",
        "    if \"__file__\" in globals():\n",
        "        HERE = pathlib.Path(__file__).resolve()          # .../vps/scripts/eval_suite.py\n",
        "        PKG_ROOT = str(HERE.parents[1])                  # .../vps\n",
        "        REPO_ROOT = str(HERE.parents[2])                 # .../\n",
        "    else:\n",
        "        CWD = pathlib.Path(os.getcwd()).resolve()\n",
        "        if (CWD / \"vpscore\").exists() or (CWD / \"scripts\").exists():\n",
        "            PKG_ROOT = str(CWD)\n",
        "            REPO_ROOT = str(CWD.parent)\n",
        "        elif (CWD / \"vps\").exists():\n",
        "            PKG_ROOT = str((CWD / \"vps\").resolve())\n",
        "            REPO_ROOT = str(CWD.resolve())\n",
        "        else:\n",
        "            PKG_ROOT = str(CWD)\n",
        "            REPO_ROOT = str(CWD)\n",
        "except Exception:\n",
        "    PKG_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"vps\"))\n",
        "    REPO_ROOT = os.path.abspath(os.getcwd())\n",
        "\n",
        "for p in (REPO_ROOT, PKG_ROOT):\n",
        "    if p and p not in sys.path:\n",
        "        sys.path.insert(0, p)\n",
        "os.environ[\"PYTHONPATH\"] = REPO_ROOT + \":\" + PKG_ROOT + \":\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your existing pieces\n",
        "from vps.scripts.infer_vps import build             # creates (tok, model, hooks)\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "\n",
        "# ----------------------------- helpers -----------------------------\n",
        "def set_vps_enabled(model: torch.nn.Module, enabled: bool):\n",
        "    \"\"\"Neutralize VPS delta by setting gamma=0 for baseline; keep wrappers intact.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if hasattr(m, \"cfg\"):\n",
        "                if not enabled:\n",
        "                    m.cfg.adaptive_gamma = False\n",
        "                    m.cfg.gamma = 0.0\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                m.policy.cfg = m.cfg\n",
        "\n",
        "\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold: str | None, max_new_tokens: int):\n",
        "    \"\"\"OOM-safe VPS pass: entropy→policy only, no backward().\"\"\"\n",
        "    hooks.clear_buffers()\n",
        "    with torch.no_grad():\n",
        "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        out = model(**inputs, use_cache=False, return_dict=True)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    ent_val = float(ent) if torch.is_tensor(ent) else float(ent)\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(ent_val)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return generate(model, tok, prompt, max_new_tokens=max_new_tokens,\n",
        "                    temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "\n",
        "def format_arc_prompt(q: str, choices: List[str]) -> str:\n",
        "    letters = \"ABCDE\"\n",
        "    lines = [f\"Question: {q}\", \"Options:\"]\n",
        "    for i, c in enumerate(choices):\n",
        "        lines.append(f\"({letters[i]}) {c}\")\n",
        "    lines.append(\"Answer:\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def extract_arc_choice_letter(text: str) -> str | None:\n",
        "    for ch in reversed(text.strip()):\n",
        "        if ch in \"ABCDE\":\n",
        "            return ch\n",
        "    return None\n",
        "\n",
        "def format_bbh_date_prompt(q: str) -> str:\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "def format_gsm8k_prompt(q: str) -> str:\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "\n",
        "def run_arc_c(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total)); random.shuffle(idx); idx = idx[:n] if n > 0 else []\n",
        "    correct = 0; latencies = []; gen_token_est = 0\n",
        "    print(f\"[arc_c] dataset size={total}; sampling n={len(idx)}\")\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        prompt = format_arc_prompt(item[\"question\"], item[\"choices\"][\"text\"])\n",
        "        t0 = time.time(); pred = vps_iterate_once(model, tok, hooks, prompt, item[\"answerKey\"], max_new_tokens); dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "        if (extract_arc_choice_letter(pred) or \"\") == item[\"answerKey\"].strip():\n",
        "            correct += 1\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            rem = len(idx) - i; eta_s = int(dt * rem)\n",
        "            print(f\"[arc_c] {i}/{len(idx)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "    acc = correct/len(idx) if idx else 0.0\n",
        "    mean_lat = sum(latencies)/len(latencies) if latencies else 0.0\n",
        "    tps = gen_token_est/sum(latencies) if latencies and gen_token_est>0 else 0.0\n",
        "    return {\"task\":\"arc_c\",\"n\":len(idx),\"acc\":acc,\"latency_mean_s\":mean_lat,\"tokens_per_sec\":tps}\n",
        "\n",
        "def run_bbh_date(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total)); random.shuffle(idx); idx = idx[:n] if n > 0 else []\n",
        "    correct = 0; latencies = []; gen_token_est = 0\n",
        "    print(f\"[bbh_date] dataset size={total}; sampling n={len(idx)}\")\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        prompt = format_bbh_date_prompt(item[\"input\"])\n",
        "        t0 = time.time(); pred = vps_iterate_once(model, tok, hooks, prompt, item[\"target\"].strip(), max_new_tokens=24); dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "        got = pred.strip().splitlines()[-1].strip()\n",
        "        if got == item[\"target\"].strip():\n",
        "            correct += 1\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            rem = len(idx) - i; eta_s = int(dt * rem)\n",
        "            print(f\"[bbh_date] {i}/{len(idx)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "    acc = correct/len(idx) if idx else 0.0\n",
        "    mean_lat = sum(latencies)/len(latencies) if latencies else 0.0\n",
        "    tps = gen_token_est/sum(latencies) if latencies and gen_token_est>0 else 0.0\n",
        "    return {\"task\":\"bbh_date\",\"n\":len(idx),\"acc\":acc,\"latency_mean_s\":mean_lat,\"tokens_per_sec\":tps}\n",
        "\n",
        "def run_gsm8k(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total)); random.shuffle(idx); idx = idx[:n] if n > 0 else []\n",
        "    correct = 0; latencies = []; gen_token_est = 0\n",
        "    print(f\"[gsm8k] dataset size={total}; sampling n={len(idx)}\")\n",
        "    import re\n",
        "    num_re = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "    def extract_num(s: str):\n",
        "        m = list(num_re.finditer(s)); return m[-1].group(1) if m else None\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        gnum = extract_num(item[\"answer\"])\n",
        "        prompt = format_gsm8k_prompt(item[\"question\"])\n",
        "        t0 = time.time(); pred = vps_iterate_once(model, tok, hooks, prompt, gnum, max_new_tokens); dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "        pnum = extract_num(pred)\n",
        "        if pnum is not None and gnum is not None:\n",
        "            try:\n",
        "                if float(pnum) == float(gnum):\n",
        "                    correct += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            rem = len(idx) - i; eta_s = int(dt * rem)\n",
        "            print(f\"[gsm8k] {i}/{len(idx)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "    acc = correct/len(idx) if idx else 0.0\n",
        "    mean_lat = sum(latencies)/len(latencies) if latencies else 0.0\n",
        "    tps = gen_token_est/sum(latencies) if latencies and gen_token_est>0 else 0.0\n",
        "    return {\"task\":\"gsm8k\",\"n\":len(idx),\"acc\":acc,\"latency_mean_s\":mean_lat,\"tokens_per_sec\":tps}\n",
        "\n",
        "\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str, int], seed: int, vps_on: bool,\n",
        "              max_new_tokens: int, print_every: int) -> List[Dict]:\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\")\n",
        "    tok, model, hooks = build()  # uses your VPSConfig defaults internally\n",
        "    if torch.cuda.is_available(): torch.cuda.set_device(0)\n",
        "    set_vps_enabled(model, enabled=vps_on); model.eval()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    print(\"[Phase] model ready.\")\n",
        "    results: List[Dict] = []\n",
        "    for task in tasks_order:\n",
        "        n = int(n_map.get(task, 0))\n",
        "        if n <= 0:\n",
        "            print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "            continue\n",
        "        print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n={n} ===\")\n",
        "        if task == \"arc_c\":\n",
        "            res = run_arc_c(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        elif task == \"bbh_date\":\n",
        "            res = run_bbh_date(model, tok, hooks, n, print_every, max_new_tokens=min(24, max_new_tokens))\n",
        "        elif task == \"gsm8k\":\n",
        "            res = run_gsm8k(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        else:\n",
        "            print(f\"[warn] unknown task '{task}', skipping.\"); continue\n",
        "        print(res); results.append(res)\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    return results\n",
        "\n",
        "\n",
        "# ----------------------------- CLI -----------------------------\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser(prog=\"eval_suite.py\")\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=10)\n",
        "    # IMPORTANT: tolerate unknown args injected by Jupyter/Colab (-f kernel.json)\n",
        "    args, _unknown = ap.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "    os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "    args = parse_args()\n",
        "    tasks_order = []; n_map = {}\n",
        "    tasks_order.append(\"gsm8k\");   n_map[\"gsm8k\"]   = args.n_gsm8k\n",
        "    tasks_order.append(\"bbh_date\"); n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    tasks_order.append(\"arc_c\");   n_map[\"arc_c\"]   = args.n_arc_c\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "xnw3BGUpussA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "7743fba0-736e-47c0-828b-cdc73ab13e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase] BASELINE (VPS OFF) — building model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "build() missing 1 required positional argument: 'cfg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3664348414.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3664348414.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mtasks_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbh_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbh_date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_bbh_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mtasks_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arc_c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m   \u001b[0mn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arc_c\"\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_arc_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     _ = run_phase(tasks_order, n_map, args.seed, vps_on=False,\n\u001b[0m\u001b[1;32m    226\u001b[0m                   max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n\u001b[1;32m    227\u001b[0m     _ = run_phase(tasks_order, n_map, args.seed, vps_on=True,\n",
            "\u001b[0;32m/tmp/ipython-input-3664348414.py\u001b[0m in \u001b[0;36mrun_phase\u001b[0;34m(tasks_order, n_map, seed, vps_on, max_new_tokens, print_every)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mphase_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"VPS ON\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvps_on\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"BASELINE (VPS OFF)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[Phase] {phase_name} — building model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# uses your VPSConfig defaults internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mset_vps_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvps_on\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: build() missing 1 required positional argument: 'cfg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "# Colab/T4-friendly evaluation with progress + OOM-safe VPS pass.\n",
        "from __future__ import annotations\n",
        "\n",
        "import os, sys, time, random, argparse\n",
        "from typing import Dict, List\n",
        "\n",
        "# --- Robust path setup: works when run as a file or via stdin/heredoc ---\n",
        "try:\n",
        "    import pathlib\n",
        "    if \"__file__\" in globals():\n",
        "        HERE = pathlib.Path(__file__).resolve()          # .../vps/scripts/eval_suite.py\n",
        "        PKG_ROOT = str(HERE.parents[1])                  # .../vps\n",
        "        REPO_ROOT = str(HERE.parents[2])                 # .../\n",
        "    else:\n",
        "        CWD = pathlib.Path(os.getcwd()).resolve()\n",
        "        if (CWD / \"vpscore\").exists() or (CWD / \"scripts\").exists():\n",
        "            PKG_ROOT = str(CWD)\n",
        "            REPO_ROOT = str(CWD.parent)\n",
        "        elif (CWD / \"vps\").exists():\n",
        "            PKG_ROOT = str((CWD / \"vps\").resolve())\n",
        "            REPO_ROOT = str(CWD.resolve())\n",
        "        else:\n",
        "            PKG_ROOT = str(CWD)\n",
        "            REPO_ROOT = str(CWD)\n",
        "except Exception:\n",
        "    PKG_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"vps\"))\n",
        "    REPO_ROOT = os.path.abspath(os.getcwd())\n",
        "\n",
        "for p in (REPO_ROOT, PKG_ROOT):\n",
        "    if p and p not in sys.path:\n",
        "        sys.path.insert(0, p)\n",
        "os.environ[\"PYTHONPATH\"] = REPO_ROOT + \":\" + PKG_ROOT + \":\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your existing pieces\n",
        "from vps.scripts.infer_vps import build             # creates (tok, model, hooks)\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "from vpscore.config import VPSConfig                # <-- NEW: import the config\n",
        "\n",
        "\n",
        "# ----------------------------- helpers -----------------------------\n",
        "def set_vps_enabled(model: torch.nn.Module, enabled: bool):\n",
        "    \"\"\"Neutralize VPS delta by setting gamma=0 for baseline; keep wrappers intact.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if hasattr(m, \"cfg\"):\n",
        "                if not enabled:\n",
        "                    m.cfg.adaptive_gamma = False\n",
        "                    m.cfg.gamma = 0.0\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                m.policy.cfg = m.cfg\n",
        "\n",
        "\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold: str | None, max_new_tokens: int):\n",
        "    \"\"\"OOM-safe VPS pass: entropy→policy only, no backward().\"\"\"\n",
        "    hooks.clear_buffers()\n",
        "    with torch.no_grad():\n",
        "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        out = model(**inputs, use_cache=False, return_dict=True)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    ent_val = float(ent) if torch.is_tensor(ent) else float(ent)\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(ent_val)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return generate(model, tok, prompt, max_new_tokens=max_new_tokens,\n",
        "                    temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "\n",
        "def format_arc_prompt(q: str, choices: List[str]) -> str:\n",
        "    letters = \"ABCDE\"\n",
        "    lines = [f\"Question: {q}\", \"Options:\"]\n",
        "    for i, c in enumerate(choices):\n",
        "        lines.append(f\"({letters[i]}) {c}\")\n",
        "    lines.append(\"Answer:\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def extract_arc_choice_letter(text: str) -> str | None:\n",
        "    for ch in reversed(text.strip()):\n",
        "        if ch in \"ABCDE\":\n",
        "            return ch\n",
        "    return None\n",
        "\n",
        "def format_bbh_date_prompt(q: str) -> str:\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "def format_gsm8k_prompt(q: str) -> str:\n",
        "    return f\"{q.strip()}\\nAnswer:\"\n",
        "\n",
        "\n",
        "def run_arc_c(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total)); random.shuffle(idx); idx = idx[:n] if n > 0 else []\n",
        "    correct = 0; latencies = []; gen_token_est = 0\n",
        "    print(f\"[arc_c] dataset size={total}; sampling n={len(idx)}\")\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        prompt = format_arc_prompt(item[\"question\"], item[\"choices\"][\"text\"])\n",
        "        t0 = time.time(); pred = vps_iterate_once(model, tok, hooks, prompt, item[\"answerKey\"], max_new_tokens); dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "        if (extract_arc_choice_letter(pred) or \"\") == item[\"answerKey\"].strip():\n",
        "            correct += 1\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            rem = len(idx) - i; eta_s = int(dt * rem)\n",
        "            print(f\"[arc_c] {i}/{len(idx)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "    acc = correct/len(idx) if idx else 0.0\n",
        "    mean_lat = sum(latencies)/len(latencies) if latencies else 0.0\n",
        "    tps = gen_token_est/sum(latencies) if latencies and gen_token_est>0 else 0.0\n",
        "    return {\"task\":\"arc_c\",\"n\":len(idx),\"acc\":acc,\"latency_mean_s\":mean_lat,\"tokens_per_sec\":tps}\n",
        "\n",
        "def run_bbh_date(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total)); random.shuffle(idx); idx = idx[:n] if n > 0 else []\n",
        "    correct = 0; latencies = []; gen_token_est = 0\n",
        "    print(f\"[bbh_date] dataset size={total}; sampling n={len(idx)}\")\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        prompt = format_bbh_date_prompt(item[\"input\"])\n",
        "        t0 = time.time(); pred = vps_iterate_once(model, tok, hooks, prompt, item[\"target\"].strip(), max_new_tokens=24); dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "        got = pred.strip().splitlines()[-1].strip()\n",
        "        if got == item[\"target\"].strip():\n",
        "            correct += 1\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            rem = len(idx) - i; eta_s = int(dt * rem)\n",
        "            print(f\"[bbh_date] {i}/{len(idx)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "    acc = correct/len(idx) if idx else 0.0\n",
        "    mean_lat = sum(latencies)/len(latencies) if latencies else 0.0\n",
        "    tps = gen_token_est/sum(latencies) if latencies and gen_token_est>0 else 0.0\n",
        "    return {\"task\":\"bbh_date\",\"n\":len(idx),\"acc\":acc,\"latency_mean_s\":mean_lat,\"tokens_per_sec\":tps}\n",
        "\n",
        "def run_gsm8k(model, tok, hooks, n: int, print_every: int, max_new_tokens: int) -> Dict:\n",
        "    ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    total = len(ds)\n",
        "    idx = list(range(total)); random.shuffle(idx); idx = idx[:n] if n > 0 else []\n",
        "    correct = 0; latencies = []; gen_token_est = 0\n",
        "    print(f\"[gsm8k] dataset size={total}; sampling n={len(idx)}\")\n",
        "    import re\n",
        "    num_re = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "    def extract_num(s: str):\n",
        "        m = list(num_re.finditer(s)); return m[-1].group(1) if m else None\n",
        "    for i, k in enumerate(idx, 1):\n",
        "        item = ds[int(k)]\n",
        "        gnum = extract_num(item[\"answer\"])\n",
        "        prompt = format_gsm8k_prompt(item[\"question\"])\n",
        "        t0 = time.time(); pred = vps_iterate_once(model, tok, hooks, prompt, gnum, max_new_tokens); dt = time.time() - t0\n",
        "        latencies.append(dt)\n",
        "        pnum = extract_num(pred)\n",
        "        if pnum is not None and gnum is not None:\n",
        "            try:\n",
        "                if float(pnum) == float(gnum):\n",
        "                    correct += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "        gen_token_est += len(tok.encode(pred))\n",
        "        if print_every > 0 and (i % print_every == 0 or i == len(idx)):\n",
        "            rem = len(idx) - i; eta_s = int(dt * rem)\n",
        "            print(f\"[gsm8k] {i}/{len(idx)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {eta_s//60}m{eta_s%60}s\")\n",
        "    acc = correct/len(idx) if idx else 0.0\n",
        "    mean_lat = sum(latencies)/len(latencies) if latencies else 0.0\n",
        "    tps = gen_token_est/sum(latencies) if latencies and gen_token_est>0 else 0.0\n",
        "    return {\"task\":\"gsm8k\",\"n\":len(idx),\"acc\":acc,\"latency_mean_s\":mean_lat,\"tokens_per_sec\":tps}\n",
        "\n",
        "\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str, int], seed: int, vps_on: bool,\n",
        "              max_new_tokens: int, print_every: int) -> List[Dict]:\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\")\n",
        "    cfg = VPSConfig()                          # <-- NEW\n",
        "    tok, model, hooks = build(cfg)             # <-- NEW: pass cfg into build()\n",
        "    if torch.cuda.is_available(): torch.cuda.set_device(0)\n",
        "    set_vps_enabled(model, enabled=vps_on); model.eval()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    print(\"[Phase] model ready.\")\n",
        "    results: List[Dict] = []\n",
        "    for task in tasks_order:\n",
        "        n = int(n_map.get(task, 0))\n",
        "        if n <= 0:\n",
        "            print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "            continue\n",
        "        print(f\"\\n=== {'VPS' if vps_on else 'BASELINE'} :: {task} n={n} ===\")\n",
        "        if task == \"arc_c\":\n",
        "            res = run_arc_c(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        elif task == \"bbh_date\":\n",
        "            res = run_bbh_date(model, tok, hooks, n, print_every, max_new_tokens=min(24, max_new_tokens))\n",
        "        elif task == \"gsm8k\":\n",
        "            res = run_gsm8k(model, tok, hooks, n, print_every, max_new_tokens=max_new_tokens)\n",
        "        else:\n",
        "            print(f\"[warn] unknown task '{task}', skipping.\"); continue\n",
        "        print(res); results.append(res)\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    return results\n",
        "\n",
        "\n",
        "# ----------------------------- CLI -----------------------------\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser(prog=\"eval_suite.py\")\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=10)\n",
        "    # IMPORTANT: tolerate unknown args injected by Jupyter/Colab (-f kernel.json)\n",
        "    args, _unknown = ap.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "    os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "    args = parse_args()\n",
        "    tasks_order = []; n_map = {}\n",
        "    tasks_order.append(\"gsm8k\");   n_map[\"gsm8k\"]   = args.n_gsm8k\n",
        "    tasks_order.append(\"bbh_date\"); n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    tasks_order.append(\"arc_c\");   n_map[\"arc_c\"]   = args.n_arc_c\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "    _ = run_phase(tasks_order, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "D2arzgGqvQ1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872,
          "referenced_widgets": [
            "4294d18ac6a3405eb62979e680d735fc",
            "7e04baf8884b4dff905dc18f532c6484",
            "fcec185afbd34161b7f4d03e86575eb2",
            "7da8a410eec44c639fa6778a1915ac3b",
            "e45f967632f044fa975c48352796bdbc",
            "1746a6ed0b3b4f4a9ced254877a7c315",
            "22209d46931942a082489cdb5b27e083",
            "377da03c912c4e5fa87b6af480022c42",
            "6678bf2247404745a35a5502a4e3387a",
            "d232ae3fc46d43f69ef8e08e3518aacc",
            "4cacaba9bade467aab980ac416e50dc6",
            "d69258d58d9e488c9409f790c526b0d9",
            "cbf849314eb84553ad052b3dde8680de",
            "bfd296affdae4f67b5d724a9f960a931",
            "018bc31e12824b56ade1a897a206c1b7",
            "176f159c207245c786bca20f99605af6",
            "f5ba926e48e045408fc8ee34758e2b20",
            "2218b3a3e5754806b1af2a9b1a1c0d4a",
            "e882b34cfb4e4a7eb4640ab5e9e2abc3",
            "45bbd16f4189499898e2b636f2e6969c",
            "b2d2e94076f3448e9ba0dd446eea5dc0",
            "88fae17b825b4300a39f7b03981573e1",
            "42ce2de625fc4400907fb6351b689f37",
            "f9c5a8ec9a61445f9b3a795dce6a2db1",
            "bd8f74fefaa24b468e1c04cebe7a6da9",
            "4b7ef64cdcb647e6a1e54016c5d2583c",
            "a9c8cdca4a3841bab8947c1e14663e7b",
            "229275b49b85413fb28d90aa4d0688c3",
            "10d44cb0cd944ed8a0b24bde05d9fee9",
            "8340d6e0dd6d4e64933c046b70b4f714",
            "81989633538a4c55811a174d83a164ca",
            "5bbd981b51014e74b43a6c267f8febd8",
            "29ba1767b67245689429496c812e92fa",
            "27a6c0c9f28b474d9c5d3f0497def112",
            "ed628350e5464d18be970b988ea3d70d",
            "99cbc6f885bb4ea7aad03cc9527f0b32",
            "0fb3a336f94841b7a8cbdb3fb07d9f0e",
            "32bb66c5f0494e9d9ed64c8567cde961",
            "22cdd79ef9974baba2be5108f79fe937",
            "54b7a64a78804017a1e005d398e69953",
            "d74cc0d4de024654a79135a96ae45530",
            "73aab4cd577045c9800067c23077748b",
            "8d22d365b5d84c32854575b4f0e87e9c",
            "6331e126595c4f04ba0f8c334f16e0fe",
            "77d5b3c023d44dea9e62c4ea5310ebb3",
            "61b9a1778a2944b79a3363d44ecddded",
            "442c6330ad7d46d5b727cee3ffeda489",
            "0721058631c24396b49d7de481ff79f4",
            "fed346e50ac44f6ea81bdfd668582e84",
            "c393832963ea48328008dbdbd6c83080",
            "43a763c15c954d508a759df42c061b2d",
            "d21ccb4c3b2247189ed8a8e105016a37",
            "17eae4ad7d2842a59764ab05146ed09e",
            "d9d0449642ce44a983065909247dc46f",
            "ee99390fb583496b8bbcffde92660f04",
            "59e0181269fb4ab2b83c254153250e37",
            "9efe034300064746ae37b84f3b59c84f",
            "f8cadf5f8a6949ea82a817547ef5bb9a",
            "d977d0a60d834dc3b05f294198343b43",
            "d0f4a3cf2a294cda8c09db32965185d4",
            "f372c8f9bb40463f86934e04ea253d3b",
            "dc45a7bb355240b7a8a927eeaf54ea58",
            "97c611d40ae84911adbebddc653ec650",
            "f34c286df3a44ad8854c0f70dc2a1efc",
            "b69c6026265e49c186d366180c121108",
            "d9a4cc4b23d649d0bbc92a4af7f8ff42",
            "e9cbb7ea04e144108fb972d5c844955d",
            "d011a54f817f461baf0cf02734f8fb90",
            "4312e9b9117640779dfdb858ff05a9f9",
            "d041e118e834410c9b41518ce8ac54aa",
            "924012184e3a4b339dcb4346ed020a76",
            "a0a175099f30448ea7f9a5c4402b6d34",
            "e0f51678e71a428a9445c4387c7fa23a",
            "7c91372dc21946f690165ec99a457bf9",
            "10bb643a49954ca58b0746c4df17d1a0",
            "2da921bb7dd442c9957e635c96bd649f",
            "1717de0776814db2b058fb857d99b412",
            "bc2f4cd7dace438fb7cca1b1cb02db53",
            "2923f92aac59498dbb400a5885ea509f",
            "20bb6b53e18548ebadb0360fdacec9f4",
            "8df97dfd4f4049dfb889bc92722b6776",
            "fc3e9c5a70174a2797b0b14fd546ac57",
            "2c79450a41d94521bb0efaf601da94c5",
            "00a3bc48b7ce45e98fd2d629c51f8901",
            "97b895467d5a447692ff496a01fd430d",
            "42a2152c0f5e47cf833dda4581911d9f",
            "a24e20f426d24de984c401f3228814f9",
            "446dae01d955418cadb795ef146f3a74",
            "9cd67346c9fe4a47bcc55a69e5d5da70",
            "f29ff75b695f4c6aa95dd74e2ddc3cbf",
            "d18316e39dfa407bbc18bbda96e5753f",
            "d5d61ea61c75475d848eb6cfc994f7f4",
            "b927a4c970a74df7bddd0426b73ac472",
            "de6ac68a802c4f65aafa5e60de986b51",
            "87b41f4efc8944679495e6f6bf34d1ff",
            "131b03a17b6c4df5b9c4ee7ef9b5de1f",
            "9005e02151144e61b87e0a0dc94f0d92",
            "42f6da8c6b1c4f91aba99aebdd67839d",
            "b0b2329620da4c649c9a7adcd80a1e3f",
            "ef5418f3606140d0bcbb6aa309a3faad",
            "dff09c873bad445db54a87aef921f80a",
            "101d053f6af94172b500b1bcb505745f",
            "6579da96fade46daaf63b8687d8b340d",
            "f665558d1e814ea78ab5fdbd1835dc32",
            "8050526e302940a3aa608f6fc00a9550",
            "b001149b1cd3407ba892ac0f941c7e67",
            "c4f88b3de6084ba6a40753828fb8e62d",
            "63e8317a8cec41f0ad301034f2ef01d9",
            "6787f44e93044d7baefbc4bc6b0d012c",
            "11b53621361f45bf99e6d1170c865226",
            "c8303876aec84b8c9814ede476f85282",
            "3b7b1142111d46c787c54bafb2826b06",
            "f5926070356d4dad87a3972258c39242",
            "77f0956678a04897a1e47fe500c73431",
            "7c41917d9f8e4ed4af2287043d39fc2a",
            "861baed6850a4b13a4c77bac86b07e6a",
            "ec2591a573cb49229785bc3317cecac6",
            "8f32846feb8b421e8d9d028c926029d6",
            "1b1dfe820e7342b1b40c2a7681e59c33",
            "e9d76e293c584e8eae32461b332b580e",
            "7816e2bad72849819f4ba4bbb5589523",
            "70718e66c47d4ece9fd55c50af74b640",
            "8e1984410a4a4f3ba38a75aa80746eaf",
            "a91482fa3c9c46c9834c85eb24e9b757",
            "91df2aceeaee4238988bfb64b234f3b5",
            "a776f6a51a064605b2136fdacfa28af1",
            "27c220d86db8495bbbea7be0751cc1cd",
            "dd480fd2709948f2b3ced98cb1c4aa32",
            "dc58f68cc90d457d95028abf576529f1",
            "2dd93d15230142adaf881565f2ac3543",
            "7c1915bed13c4baab8241fefd02a2bed",
            "f2dbb2302c6e4261907fbb32fdf185be"
          ]
        },
        "outputId": "d4f4395c-501a-4d6f-b210-260b09c8ce0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase] BASELINE (VPS OFF) — building model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_headers.py:154: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  cached_token = get_token()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4294d18ac6a3405eb62979e680d735fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d69258d58d9e488c9409f790c526b0d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42ce2de625fc4400907fb6351b689f37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27a6c0c9f28b474d9c5d3f0497def112"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77d5b3c023d44dea9e62c4ea5310ebb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59e0181269fb4ab2b83c254153250e37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9cbb7ea04e144108fb972d5c844955d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc2f4cd7dace438fb7cca1b1cb02db53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cd67346c9fe4a47bcc55a69e5d5da70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef5418f3606140d0bcbb6aa309a3faad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8303876aec84b8c9814ede476f85282"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VPS] Wrapped layers: 252\n",
            "[Phase] model ready.\n",
            "\n",
            "=== BASELINE :: gsm8k n=0 — skipping ===\n",
            "\n",
            "=== BASELINE :: bbh_date n=0 — skipping ===\n",
            "\n",
            "=== BASELINE :: arc_c n=0 — skipping ===\n",
            "\n",
            "[Phase] VPS ON — building model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70718e66c47d4ece9fd55c50af74b640"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VPS] Wrapped layers: 252\n",
            "[Phase] model ready.\n",
            "\n",
            "=== VPS :: gsm8k n=0 — skipping ===\n",
            "\n",
            "=== VPS :: bbh_date n=0 — skipping ===\n",
            "\n",
            "=== VPS :: arc_c n=0 — skipping ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "# Make your packages importable\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "# Show logs immediately (stdout + stderr)\n",
        "export PYTHONUNBUFFERED=1\n",
        "export TF_CPP_MIN_LOG_LEVEL=3\n",
        "export TRANSFORMERS_VERBOSITY=info\n",
        "\n",
        "# Run Python with faulthandler and line-buffered stdio\n",
        "stdbuf -oL -eL python -X faulthandler -u - <<'PY'\n",
        "import os, sys, time, threading\n",
        "# Ensure import paths even inside Python\n",
        "for p in [\"/content\", \"/content/vps\"]:\n",
        "    if p not in sys.path: sys.path.insert(0, p)\n",
        "\n",
        "alive = True\n",
        "def heartbeat():\n",
        "    t = 0\n",
        "    while alive:\n",
        "        print(f\"[hb] alive {t}s\", flush=True)\n",
        "        time.sleep(1)\n",
        "        t += 1\n",
        "\n",
        "hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "hb.start()\n",
        "\n",
        "try:\n",
        "    print(\"[step] quick imports...\", flush=True)\n",
        "    from datasets import load_dataset\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "    print(\"[ok] imports ready\", flush=True)\n",
        "\n",
        "    print(\"[step] cache tiny dataset slice (ARC-Challenge 2 items)...\", flush=True)\n",
        "    ds = load_dataset(\"ai2_arc\",\"ARC-Challenge\", split=\"validation[:2]\")\n",
        "    print(f\"[ok] dataset cached: {len(ds)} items\", flush=True)\n",
        "\n",
        "    MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    print(f\"[step] load tokenizer: {MODEL}\", flush=True)\n",
        "    tok = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "    print(\"[ok] tokenizer ready\", flush=True)\n",
        "\n",
        "    print(\"[step] load base model (this is the slowest part; HF logs will appear)...\", flush=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\")\n",
        "    print(\"[ok] base model in memory\", flush=True)\n",
        "\n",
        "    print(\"[step] import your eval harness (run_phase)...\", flush=True)\n",
        "    from vps.scripts.eval_suite import run_phase\n",
        "    print(\"[ok] eval harness ready\", flush=True)\n",
        "\n",
        "    # Tiny baseline run (prints per-item progress)\n",
        "    print(\"\\n[phase] BASELINE (no VPS) :: ARC-C n=1\", flush=True)\n",
        "    res_base = run_phase([\"arc_c\"], {\"arc_c\": 1}, seed=1234, vps_on=False, max_new_tokens=24, print_every=1)\n",
        "    print(\"[result] baseline:\", res_base, flush=True)\n",
        "\n",
        "    # Tiny VPS run (prints per-item progress)\n",
        "    print(\"\\n[phase] VPS ON :: ARC-C n=1\", flush=True)\n",
        "    res_vps = run_phase([\"arc_c\"], {\"arc_c\": 1}, seed=1234, vps_on=True, max_new_tokens=24, print_every=1)\n",
        "    print(\"[result] vps:\", res_vps, flush=True)\n",
        "\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(\"[error] exception during run:\", repr(e), flush=True)\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    alive = False\n",
        "    time.sleep(0.2)\n",
        "    print(\"[done] diagnostic run finished.\", flush=True)\n",
        "PY\n",
        "\n"
      ],
      "metadata": {
        "id": "6Ij5lF2Zv4HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, math, random, gc\n",
        "import argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# --- make both \"vpscore\" and \"vps.scripts\" importable no matter how we launch ---\n",
        "THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "PROJ_ROOT = os.path.abspath(os.path.join(THIS_DIR, \"..\"))         # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))      # .../\n",
        "for p in (PROJ_ROOT, PROJ_PARENT):\n",
        "    if p not in sys.path: sys.path.insert(0, p)\n",
        "\n",
        "# Helps with allocator fragmentation on Colab T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from vps.scripts.infer_vps import build                         # your build(cfg)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "\n",
        "\n",
        "# ------------------------- small helpers -------------------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_inputs_to_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(0)==1 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def build_cfg(args, vps_on: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    # model / dtype\n",
        "    cfg.model_name = args.model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = args.dtype\n",
        "    cfg.torch_dtype_str = args.dtype\n",
        "    # generation\n",
        "    cfg.max_new_tokens = args.max_new_tokens\n",
        "    cfg.temperature = 0.0  # deterministic for eval\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    # VPS knobs (kept same as your defaults, but allow attn-only)\n",
        "    if args.attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    # make sure qk coupling stays enabled (your default)\n",
        "    cfg.qk_coupling = True\n",
        "    # lbfgs is on by default; you can disable with --no_lbfgs if you add it later\n",
        "    return cfg\n",
        "\n",
        "\n",
        "# ------------------------- task loaders -------------------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Return list of (prompt, gold_text) for ARC-Challenge validation.\"\"\"\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\",\"ARC-Challenge\", split=\"validation\")\n",
        "    # Stable sample\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        # simple instruction prompt\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        gold = gold_label  # compare on letter\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "\n",
        "# ------------------------- VPS iteration -------------------------\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    verifier = CompositeVerifier()\n",
        "\n",
        "    # ---- Iteration 0 (plain decode, no grad) ----\n",
        "    pred0 = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    # Prepare inputs once\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_inputs_to_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    # ---- Lightweight policy signal: token entropy (no grad) ----\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        logits = out.logits\n",
        "        ent = compute_token_entropy(logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            m.policy.set_token_entropy(float(ent))\n",
        "\n",
        "    # ---- CE surrogate on tiny tail to populate grads for VPS ----\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)  # [1,T]\n",
        "\n",
        "    # Important for memory: no cache + grad ckpt on VPS runs\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]  # [1,T,V]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "    # ---- VPS-enhanced decode (deterministic) ----\n",
        "    pred1 = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "    return pred1\n",
        "\n",
        "\n",
        "# ------------------------- phases -------------------------\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Optionally gate VPS at runtime (if you add flags later).\"\"\"\n",
        "    # Your VPSLinear path is injected already; generation will use it.\n",
        "    # Nothing to toggle here unless you add a runtime switch inside VPSLinear.\n",
        "\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, hooks, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        if vps_on:\n",
        "            pred = vps_iterate_once(model, tok, hooks, prompt, gold, max_new_tokens, tail_tokens)\n",
        "        else:\n",
        "            pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        # ARC-C grading: compare letter\n",
        "        got = None\n",
        "        for ch in [\"A\",\"B\",\"C\",\"D\",\"E\",\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
        "            if f\"{ch}\" in pred:\n",
        "                got = ch.upper(); break\n",
        "        is_ok = (got == gold.upper())\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\")\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    # tokens_per_sec is approximate without tokenizer length; keep simple\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\")\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(argparse.Namespace(\n",
        "        model_name=model_name, dtype=dtype,\n",
        "        max_new_tokens=max_new_tokens, attn_only=attn_only\n",
        "    ), vps_on=vps_on)\n",
        "\n",
        "    # Build model\n",
        "    tok, model, hooks = build(cfg)\n",
        "    if vps_on:\n",
        "        # memory saving tricks for the CE step\n",
        "        try:\n",
        "            model.gradient_checkpointing_enable()\n",
        "        except Exception:\n",
        "            pass\n",
        "        set_vps_enabled(model, True)\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\")\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, hooks, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "    finally:\n",
        "        # free memory before leaving the phase\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ------------------------- CLI -------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=64, help=\"number of prompt tokens used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O to save VRAM\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    tasks = []\n",
        "    n_map = {}\n",
        "    if args.n_gsm8k:   tasks.append(\"gsm8k\");     n_map[\"gsm8k\"] = args.n_gsm8k  # (not implemented in this trimmed file)\n",
        "    if args.n_bbh_date:tasks.append(\"bbh_date\");  n_map[\"bbh_date\"] = args.n_bbh_date  # (not implemented here)\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    # BASELINE\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    # VPS\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "kRtbVERvNMaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, math, random, gc\n",
        "import argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# --- make both \"vpscore\" and \"vps.scripts\" importable no matter how we launch ---\n",
        "THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "PROJ_ROOT = os.path.abspath(os.path.join(THIS_DIR, \"..\"))         # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))      # .../\n",
        "for p in (PROJ_ROOT, PROJ_PARENT):\n",
        "    if p not in sys.path: sys.path.insert(0, p)\n",
        "\n",
        "# Helps with allocator fragmentation on Colab T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from vps.scripts.infer_vps import build                         # your build(cfg)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "\n",
        "\n",
        "# ------------------------- small helpers -------------------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_inputs_to_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(0)==1 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def build_cfg(args, vps_on: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    # model / dtype\n",
        "    cfg.model_name = args.model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = args.dtype\n",
        "    cfg.torch_dtype_str = args.dtype\n",
        "    # generation\n",
        "    cfg.max_new_tokens = args.max_new_tokens\n",
        "    cfg.temperature = 0.0  # deterministic for eval\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    # VPS knobs (kept same as your defaults, but allow attn-only)\n",
        "    if args.attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    # make sure qk coupling stays enabled (your default)\n",
        "    cfg.qk_coupling = True\n",
        "    # lbfgs is on by default; you can disable with --no_lbfgs if you add it later\n",
        "    return cfg\n",
        "\n",
        "\n",
        "# ------------------------- task loaders -------------------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Return list of (prompt, gold_text) for ARC-Challenge validation.\"\"\"\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\",\"ARC-Challenge\", split=\"validation\")\n",
        "    # Stable sample\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        # simple instruction prompt\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        gold = gold_label  # compare on letter\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "\n",
        "# ------------------------- VPS iteration -------------------------\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    verifier = CompositeVerifier()\n",
        "\n",
        "    # ---- Iteration 0 (plain decode, no grad) ----\n",
        "    pred0 = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    # Prepare inputs once\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_inputs_to_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    # ---- Lightweight policy signal: token entropy (no grad) ----\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        logits = out.logits\n",
        "        ent = compute_token_entropy(logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            m.policy.set_token_entropy(float(ent))\n",
        "\n",
        "    # ---- CE surrogate on tiny tail to populate grads for VPS ----\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)  # [1,T]\n",
        "\n",
        "    # Important for memory: no cache + grad ckpt on VPS runs\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]  # [1,T,V]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "    # ---- VPS-enhanced decode (deterministic) ----\n",
        "    pred1 = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "    return pred1\n",
        "\n",
        "\n",
        "# ------------------------- phases -------------------------\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Optionally gate VPS at runtime (if you add flags later).\"\"\"\n",
        "    # Your VPSLinear path is injected already; generation will use it.\n",
        "    # Nothing to toggle here unless you add a runtime switch inside VPSLinear.\n",
        "\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, hooks, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        if vps_on:\n",
        "            pred = vps_iterate_once(model, tok, hooks, prompt, gold, max_new_tokens, tail_tokens)\n",
        "        else:\n",
        "            pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        # ARC-C grading: compare letter\n",
        "        got = None\n",
        "        for ch in [\"A\",\"B\",\"C\",\"D\",\"E\",\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
        "            if f\"{ch}\" in pred:\n",
        "                got = ch.upper(); break\n",
        "        is_ok = (got == gold.upper())\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\")\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    # tokens_per_sec is approximate without tokenizer length; keep simple\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\")\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(argparse.Namespace(\n",
        "        model_name=model_name, dtype=dtype,\n",
        "        max_new_tokens=max_new_tokens, attn_only=attn_only\n",
        "    ), vps_on=vps_on)\n",
        "\n",
        "    # Build model\n",
        "    tok, model, hooks = build(cfg)\n",
        "    if vps_on:\n",
        "        # memory saving tricks for the CE step\n",
        "        try:\n",
        "            model.gradient_checkpointing_enable()\n",
        "        except Exception:\n",
        "            pass\n",
        "        set_vps_enabled(model, True)\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\")\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, hooks, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "    finally:\n",
        "        # free memory before leaving the phase\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ------------------------- CLI -------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=64, help=\"number of prompt tokens used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O to save VRAM\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    tasks = []\n",
        "    n_map = {}\n",
        "    if args.n_gsm8k:   tasks.append(\"gsm8k\");     n_map[\"gsm8k\"] = args.n_gsm8k  # (not implemented in this trimmed file)\n",
        "    if args.n_bbh_date:tasks.append(\"bbh_date\");  n_map[\"bbh_date\"] = args.n_bbh_date  # (not implemented here)\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    # BASELINE\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    # VPS\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "9DLiYxLHNXlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, math, random, gc, argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# -------- make both \"vpscore\" and \"vps.scripts\" importable everywhere --------\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))  # script mode\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())                # notebook/REPL\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))       # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))      # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if os.path.isdir(cand) and cand not in sys.path:\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# allocator hygiene on Colab T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# reuse your build() and utilities\n",
        "from vps.scripts.infer_vps import build\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "\n",
        "\n",
        "# ------------------------- small helpers -------------------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_inputs_to_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(0)==1 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def build_cfg(args, vps_on: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    # model / dtype\n",
        "    cfg.model_name = args.model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = args.dtype\n",
        "    cfg.torch_dtype_str = args.dtype\n",
        "    # generation\n",
        "    cfg.max_new_tokens = args.max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    # patch selection\n",
        "    if args.attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "\n",
        "# ------------------------- task loaders -------------------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\",\"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "\n",
        "# ------------------------- VPS iteration -------------------------\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    # Iter-0 (deterministic)\n",
        "    _ = CompositeVerifier()  # kept for parity; not used here directly\n",
        "    pred0 = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_inputs_to_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        logits = out.logits\n",
        "        ent = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            m.policy.set_token_entropy(float(ent))\n",
        "\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)  # [1,T]\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "    # Iter-1 (deterministic with VPS deltas active)\n",
        "    pred1 = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "    return pred1\n",
        "\n",
        "\n",
        "# ------------------------- phases -------------------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, hooks, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        if vps_on:\n",
        "            pred = vps_iterate_once(model, tok, hooks, prompt, gold, max_new_tokens, tail_tokens)\n",
        "        else:\n",
        "            pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        # ARC-C: extract letter\n",
        "        got = None\n",
        "        for ch in [\"A\",\"B\",\"C\",\"D\",\"E\",\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
        "            if ch in pred:\n",
        "                got = ch.upper(); break\n",
        "        is_ok = (got == gold.upper())\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\")\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\")\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(argparse.Namespace(\n",
        "        model_name=model_name, dtype=dtype,\n",
        "        max_new_tokens=max_new_tokens, attn_only=attn_only\n",
        "    ), vps_on=vps_on)\n",
        "\n",
        "    tok, model, hooks = build(cfg)\n",
        "    if vps_on:\n",
        "        try: model.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\")\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, hooks, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ------------------------- CLI -------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=64, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    tasks = []\n",
        "    n_map = {}\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if args.n_gsm8k:   print(\"[warn] gsm8k loader not enabled in this lite file — set n_gsm8k=0\")\n",
        "    if args.n_bbh_date:print(\"[warn] bbh_date loader not enabled in this lite file — set n_bbh_date=0\")\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "d6XqRY2SOs3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ---------- make both \"vpscore\" and \"vps.scripts\" importable (script or notebook) ----------\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))    # script mode\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())                  # notebook/REPL\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))  # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\")) # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if os.path.isdir(cand) and cand not in sys.path:\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator hygiene on Colab T4 (helps OOM/fragmentation)\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your project’s build + utils\n",
        "from vps.scripts.infer_vps import build\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "from vpscore.verifiers.composite_verifier import CompositeVerifier\n",
        "\n",
        "\n",
        "# ------------------------- small helpers -------------------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_inputs_to_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(0)==1 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]  # attention only (OOM safer)\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "\n",
        "# ------------------------- task loader(s) -------------------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\",\"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "\n",
        "# ------------------------- VPS iteration (1 step) -------------------------\n",
        "def vps_iterate_once(model, tok, hooks, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    # Iter-0 (deterministic) — not used for scoring here, but keeps your flow\n",
        "    _ = CompositeVerifier()\n",
        "    _pred0 = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_inputs_to_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        logits = out.logits\n",
        "        ent = compute_token_entropy(logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            m.policy.set_token_entropy(float(ent))\n",
        "\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)  # [1,T]\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "    # Iter-1 (deterministic with VPS)\n",
        "    pred1 = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "    return pred1\n",
        "\n",
        "\n",
        "# ------------------------- run one task -------------------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, hooks, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\")\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        if vps_on:\n",
        "            pred = vps_iterate_once(model, tok, hooks, prompt, gold, max_new_tokens, tail_tokens)\n",
        "        else:\n",
        "            pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        # ARC-C scoring: check for option letter\n",
        "        got = None\n",
        "        for ch in [\"A\",\"B\",\"C\",\"D\",\"E\",\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
        "            if ch in pred:\n",
        "                got = ch.upper(); break\n",
        "        is_ok = (got == gold.upper())\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "\n",
        "# ------------------------- whole phase -------------------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens, attn_only=attn_only)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # tiny mem wins for VPS phase\n",
        "    if vps_on:\n",
        "        try: model.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, hooks, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ------------------------- CLI -------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=64, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    # IMPORTANT: ignore unknown args (e.g., Colab’s `-f <kernel.json>`)\n",
        "    args, _unknown = ap.parse_known_args()\n",
        "\n",
        "    tasks = []\n",
        "    n_map = {}\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if args.n_gsm8k:   print(\"[warn] gsm8k loader not enabled in this lite file — set n_gsm8k=0\")\n",
        "    if args.n_bbh_date:print(\"[warn] bbh_date loader not enabled in this lite file — set n_bbh_date=0\")\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "q8TVU9dsPa9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_arc_c 5 \\\n",
        "  --max_new_tokens 24 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --attn_only \\\n",
        "  --tail_tokens 32\n"
      ],
      "metadata": {
        "id": "pfayhBnKPdY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ---------- Make imports work whether launched as a script or inside a notebook ----------\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))    # script mode\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())                  # notebook/REPL\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))   # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))  # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator hygiene on Colab T4 (helps fragmentation/OOM)\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your project constructs\n",
        "from vps.scripts.infer_vps import build           # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ------------------------- small helpers -------------------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_inputs_to_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(0)==1 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"\n",
        "    Baseline toggle: when disabled, set gamma=0 so VPS adds no delta.\n",
        "    Keeps wrapping intact (so code paths stay identical).\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            if enabled:\n",
        "                m.cfg.gamma = float(getattr(m, \"_saved_gamma\", 0.5))\n",
        "            else:\n",
        "                m.cfg.gamma = 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype              # your build() maps this to torch dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]  # attention only (lower memory)\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "# ------------------------- loader(s) -------------------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\",\"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "# ------------------------- VPS iteration (1 step) -------------------------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    # Iter-0 (deterministic) — keeps your original flow; not used for scoring\n",
        "    _ = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_inputs_to_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # tiny CE on the last few gold tokens to populate grads → VPS uses grad_h internally\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)  # [1, T]\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "    # VPS-enhanced decode (deterministic)\n",
        "    return generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "# ------------------------- run one task -------------------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        if vps_on:\n",
        "            pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, tail_tokens)\n",
        "        else:\n",
        "            pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        # ARC-C scoring: best-effort letter extraction\n",
        "        got = None\n",
        "        for ch in [\"A\",\"B\",\"C\",\"D\",\"E\",\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
        "            if ch in pred:\n",
        "                got = ch.upper(); break\n",
        "        is_ok = (got == gold.upper())\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "# ------------------------- whole phase -------------------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens, attn_only=attn_only)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Toggle VPS effect for 'baseline'\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    # small memory wins\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "            print(json.dumps(res), flush=True)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------------------------- CLI -------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=32, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    # NOTE: parse_known_args so Colab's \"-f <kernel.json>\" won't crash\n",
        "    args, _unknown = ap.parse_known_args()\n",
        "\n",
        "    tasks = []\n",
        "    n_map = {}\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if args.n_gsm8k:   print(\"[warn] gsm8k loader not enabled in this lite file — set n_gsm8k=0\")\n",
        "    if args.n_bbh_date:print(\"[warn] bbh_date loader not enabled in this lite file — set n_bbh_date=0\")\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "kW32AAdcP47h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_arc_c 5 \\\n",
        "  --max_new_tokens 24 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --attn_only \\\n",
        "  --tail_tokens 32\n"
      ],
      "metadata": {
        "id": "E4_kUjl9QL-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ---------- Make imports work whether launched as a script or inside a notebook ----------\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))    # script mode\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())                  # notebook/REPL\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))   # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))  # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator hygiene (reduces OOM on Colab T4)\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your project constructs\n",
        "from vps.scripts.infer_vps import build           # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ------------------------- small helpers -------------------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_inputs_to_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(0)==1 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Baseline toggle: when disabled, set gamma=0 so VPS adds no delta.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(getattr(m._saved_gamma, \"__float__\", lambda: m._saved_gamma)()) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype              # build() maps this to torch dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]  # attention only (lower memory)\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "# ------------------------- loader(s) -------------------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\",\"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "# ------------------------- VPS iteration (1 step) -------------------------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    # Iter-0 (deterministic) — keeps your original flow; not used for scoring\n",
        "    _ = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_inputs_to_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # tiny CE on the last few gold tokens to populate grads → VPS uses grad_h internally\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)  # [1, T]\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "    # VPS-enhanced decode (deterministic)\n",
        "    return generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "# ------------------------- run one task -------------------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        if vps_on:\n",
        "            pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, tail_tokens)\n",
        "        else:\n",
        "            pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        # ARC-C scoring: best-effort letter extraction\n",
        "        got = None\n",
        "        for ch in [\"A\",\"B\",\"C\",\"D\",\"E\",\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
        "            if ch in pred:\n",
        "                got = ch.upper(); break\n",
        "        is_ok = (got == gold.upper())\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "# ------------------------- whole phase -------------------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens, attn_only=attn_only)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Toggle VPS effect for 'baseline'\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    # small memory wins\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "            print(json.dumps(res), flush=True)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------------------------- CLI -------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=32, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    # parse_known_args → ignores Colab’s \"-f <kernel.json>\"\n",
        "    args, _unknown = ap.parse_known_args()\n",
        "\n",
        "    tasks = []\n",
        "    n_map = {}\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if args.n_gsm8k:   print(\"[warn] gsm8k loader not enabled in this lite file — set n_gsm8k=0\")\n",
        "    if args.n_bbh_date:print(\"[warn] bbh_date loader not enabled in this lite file — set n_bbh_date=0\")\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gha1eqT6RTQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_arc_c 5 \\\n",
        "  --max_new_tokens 24 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --attn_only \\\n",
        "  --tail_tokens 32\n"
      ],
      "metadata": {
        "id": "3R60l_wORWfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/eval_suite.py\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ---------- Make imports work in both script and notebook ----------\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))    # script mode\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())                  # notebook/REPL\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))   # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))  # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator hygiene (reduces OOM on Colab T4)\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reuse your project constructs\n",
        "from vps.scripts.infer_vps import build           # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ------------------------- small helpers -------------------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_inputs_to_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(0)==1 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Baseline toggle: when disabled, set gamma=0 so VPS adds no delta.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype              # build() maps this to torch dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]  # attention only (lower memory)\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "# ------------------------- loader(s) -------------------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\",\"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "# ------------------------- VPS iteration (1 step) -------------------------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    # Iter-0 (deterministic) — keeps your original flow; not used for scoring\n",
        "    _ = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_inputs_to_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # tiny CE on the last few gold tokens to populate grads → VPS uses grad_h internally\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)  # [1, T]\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "    # VPS-enhanced decode (deterministic)\n",
        "    return generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "# ------------------------- run one task -------------------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        if vps_on:\n",
        "            pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, tail_tokens)\n",
        "        else:\n",
        "            pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        # ARC-C scoring: best-effort letter extraction\n",
        "        got = None\n",
        "        for ch in [\"A\",\"B\",\"C\",\"D\",\"E\",\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
        "            if ch in pred:\n",
        "                got = ch.upper(); break\n",
        "        is_ok = (got == gold.upper())\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "# ------------------------- whole phase -------------------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens, attn_only=attn_only)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Toggle VPS effect for 'baseline'\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    # small memory wins\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "            print(json.dumps(res), flush=True)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------------------------- CLI -------------------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=32, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    # parse_known_args → ignores Colab’s \"-f <kernel.json>\"\n",
        "    args, _unknown = ap.parse_known_args()\n",
        "\n",
        "    tasks = []\n",
        "    n_map = {}\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if args.n_gsm8k:   print(\"[warn] gsm8k loader not enabled in this lite file — set n_gsm8k=0\")\n",
        "    if args.n_bbh_date:print(\"[warn] bbh_date loader not enabled in this lite file — set n_bbh_date=0\")\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "PgfRvlLvSGNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "# Show the first ~140 lines (should contain model_name/dtype/attn_only/tail_tokens)\n",
        "sed -n '1,140p' vps/scripts/eval_suite.py | nl | sed -n '1,140p'\n",
        "echo \"---- grep flags ----\"\n",
        "grep -nE 'model_name|dtype|attn_only|tail_tokens|parse_known_args' vps/scripts/eval_suite.py || true\n"
      ],
      "metadata": {
        "id": "il7xmE7YSM9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_arc_c 5 \\\n",
        "  --max_new_tokens 24 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --attn_only \\\n",
        "  --tail_tokens 32\n"
      ],
      "metadata": {
        "id": "ykIEp8bgSTf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > vps/scripts/eval_suite.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# --- Make imports work in Colab or as a script ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))   # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))  # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator = fewer OOMs on T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Reuse your project pieces ---\n",
        "from vps.scripts.infer_vps import build            # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"For baseline: keep VPS modules present but turn their effect off.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype              # your build() maps this to torch dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]   # attention only\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "# --------------- data (ARC-C only in this lite file) ---------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "# --------------- one VPS iteration (safe on T4) ---------------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    # Iter 0 (greedy) to get context; ignore output\n",
        "    _ = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # tiny CE on last few gold tokens to populate grads\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "    # VPS-enhanced decode (greedy)\n",
        "    return generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "# --------------- run one task ---------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, tail_tokens)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            # graceful fallback: free, shrink tail, try once more (or baseline)\n",
        "            free_cuda()\n",
        "            if vps_on and tail_tokens > 8:\n",
        "                pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, tail_tokens // 2)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        got = None\n",
        "        for ch in [\"A\",\"B\",\"C\",\"D\",\"E\",\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
        "            if ch in pred:\n",
        "                got = ch.upper(); break\n",
        "        is_ok = (got == gold.upper())\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "# --------------- whole phase ---------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens, attn_only=attn_only)\n",
        "    tok, model, hooks = build(cfg)\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "            print(json.dumps(res), flush=True)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# --------------- CLI ---------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=32, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    args, _unknown = ap.parse_known_args()  # tolerate Colab's -f kernel.json\n",
        "\n",
        "    tasks, n_map = [], {}\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if args.n_gsm8k:   print(\"[warn] gsm8k loader not in this lite file — set n_gsm8k=0\")\n",
        "    if args.n_bbh_date:print(\"[warn] bbh_date loader not in this lite file — set n_bbh_date=0\")\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "2jgq4cEnTLXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_arc_c 5 \\\n",
        "  --max_new_tokens 24 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --attn_only \\\n",
        "  --tail_tokens 32\n"
      ],
      "metadata": {
        "id": "KFeoKJUqTPOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_arc_c 50 \\\n",
        "  --n_gsm8k 0 \\\n",
        "  --n_bbh_date 0 \\\n",
        "  --max_new_tokens 24 \\\n",
        "  --print_every 5\n"
      ],
      "metadata": {
        "id": "VgZphTkiX8Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_bbh_date 40 \\\n",
        "  --n_arc_c 0 \\\n",
        "  --n_gsm8k 0 \\\n",
        "  --max_new_tokens 16 \\\n",
        "  --print_every 5\n"
      ],
      "metadata": {
        "id": "g8sKty-iY1kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_bbh_date 40 \\\n",
        "  --n_gsm8k 0 \\\n",
        "  --n_arc_c 0 \\\n",
        "  --max_new_tokens 16 \\\n",
        "  --print_every 5\n"
      ],
      "metadata": {
        "id": "NqVUal_HZE17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > vps/scripts/eval_suite.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse, re, datetime as dt\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# --- Make imports work in Colab or as a script ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))   # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))  # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator = fewer OOMs on T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Reuse your project pieces ---\n",
        "from vps.scripts.infer_vps import build            # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "NUM_RE = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"For baseline: keep VPS modules present but turn their effect off.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype              # your build() maps this to torch dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]   # attention only\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "# --------------- data loaders ---------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    # ARC has \"test\" split for Challenge; some mirrors provide \"validation\" too.\n",
        "    try:\n",
        "        ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "    except Exception:\n",
        "        ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the following multiple-choice science question.\\n\"\n",
        "            \"Respond with the BEST option letter only (A, B, C, D, or E).\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((\"arc_c\", prompt, str(gold_label).upper()))\n",
        "    return items\n",
        "\n",
        "def load_gsm8k(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    # Use \"main\" config; fall back to train if test not present.\n",
        "    try:\n",
        "        ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    except Exception:\n",
        "        ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"] if \"question\" in ex else ex.get(\"question_text\", \"\")\n",
        "        # gold: extract final number after \"####\"\n",
        "        ans = ex[\"answer\"]\n",
        "        m = re.search(r\"####\\s*(.+)\", ans)\n",
        "        gold = (m.group(1) if m else ans).strip()\n",
        "        prompt = (\n",
        "            \"Solve the problem. Return ONLY the final number, no words.\\n\\n\"\n",
        "            f\"Problem: {q}\\nAnswer:\"\n",
        "        )\n",
        "        items.append((\"gsm8k\", prompt, gold))\n",
        "    return items\n",
        "\n",
        "def _bbh_try_load():\n",
        "    tried = []\n",
        "    for name, subset, split in [\n",
        "        (\"lukaemon/bbh\", \"date_understanding\", \"test\"),\n",
        "        (\"google/bigbench-hard\", \"date_understanding\", \"test\"),\n",
        "        (\"maveriq/bbh\", \"date_understanding\", \"test\"),\n",
        "    ]:\n",
        "        try:\n",
        "            ds = load_dataset(name, subset, split=split)\n",
        "            return ds, f\"{name}/{subset}:{split}\"\n",
        "        except Exception as e:\n",
        "            tried.append(f\"{name}/{subset}:{split} -> {type(e).__name__}\")\n",
        "    return None, tried\n",
        "\n",
        "def _weekday_name(d: dt.date):\n",
        "    return [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"][d.weekday()]\n",
        "\n",
        "def _synthetic_bbh_date(n, seed=1234):\n",
        "    rng = random.Random(seed)\n",
        "    rows = []\n",
        "    for _ in range(n):\n",
        "        y = rng.randint(2001, 2030)\n",
        "        m = rng.randint(1, 12)\n",
        "        # choose a valid day for the month\n",
        "        for _try in range(5):\n",
        "            d = rng.randint(1, 31)\n",
        "            try:\n",
        "                base = dt.date(y, m, d); break\n",
        "            except: continue\n",
        "        k = rng.choice([-400,-90,-31,-30,-7,-1,1,2,3,7,30,31,90,400])\n",
        "        target = base + dt.timedelta(days=k)\n",
        "        if rng.random() < 0.6:\n",
        "            prompt = f\"If the date is {base.isoformat()}, what day of the week is it {k} days later? Answer only the weekday.\"\n",
        "            answer = _weekday_name(target)\n",
        "        else:\n",
        "            prompt = f\"Starting from {base.strftime('%B %d, %Y')}, what is the exact date {k} days later? Answer as YYYY-MM-DD.\"\n",
        "            answer = target.isoformat()\n",
        "        rows.append({\"input\": prompt, \"target\": answer})\n",
        "    from datasets import Dataset\n",
        "    return Dataset.from_list(rows)\n",
        "\n",
        "def load_bbh_date(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds, msg = _bbh_try_load()\n",
        "    if ds is None:\n",
        "        print(f\"[warn] could not fetch BBH Date from HF ({msg}); using synthetic {n} items.\", flush=True)\n",
        "        ds = _synthetic_bbh_date(n, seed)\n",
        "    else:\n",
        "        ds = ds.shuffle(seed=seed).select(range(min(n, len(ds))))\n",
        "    items = []\n",
        "    for ex in ds:\n",
        "        prompt = (\n",
        "            \"Answer the question. Return ONLY the final weekday name or ISO date (YYYY-MM-DD).\\n\\n\"\n",
        "            f\"{ex['input']}\\nAnswer:\"\n",
        "        )\n",
        "        gold = str(ex.get(\"target\", ex.get(\"label\", \"\"))).strip()\n",
        "        items.append((\"bbh_date\", prompt, gold))\n",
        "    return items\n",
        "\n",
        "# --------------- parsing + correctness ---------------\n",
        "def _extract_last_number(s: str):\n",
        "    m = list(NUM_RE.finditer(s or \"\"))\n",
        "    return m[-1].group(1) if m else None\n",
        "\n",
        "def _extract_choice_letter(s: str):\n",
        "    s = (s or \"\").strip()\n",
        "    m = re.findall(r\"\\b([A-E])\\b\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    m = re.findall(r\"\\(([A-E])\\)\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    # fallback: first letter in text beginning with A-E:\n",
        "    m = re.findall(r\"\\b([A-E])[.)\\s]\", s.upper())\n",
        "    return m[0] if m else None\n",
        "\n",
        "def _normalize(s: str):\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "def is_correct(task: str, pred: str, gold: str) -> bool:\n",
        "    if task == \"arc_c\":\n",
        "        got = _extract_choice_letter(pred)\n",
        "        return (got == gold.upper())\n",
        "    if task == \"gsm8k\":\n",
        "        pn = _extract_last_number(pred)\n",
        "        gn = _extract_last_number(gold)\n",
        "        try:\n",
        "            return pn is not None and gn is not None and abs(float(pn) - float(gn)) < 1e-6\n",
        "        except:\n",
        "            return False\n",
        "    if task == \"bbh_date\":\n",
        "        return _normalize(pred) == _normalize(gold)\n",
        "    return False\n",
        "\n",
        "# --------------- one VPS iteration (safe on T4) ---------------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int) -> str:\n",
        "    # Iter 0 (greedy) to warm caches; ignore output\n",
        "    _ = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                # ensure plain float (some policies expect scalar)\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # tiny CE on last few gold tokens to populate grads\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    try:\n",
        "        out2 = model(**tail_inputs)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            target.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "    finally:\n",
        "        model.config.use_cache = old_cache\n",
        "\n",
        "    # VPS-enhanced decode (greedy)\n",
        "    return generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "# --------------- run one task ---------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (_, prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, tail_tokens)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            # graceful fallback: free, shrink tail, try once more (or baseline)\n",
        "            free_cuda()\n",
        "            if vps_on and tail_tokens > 8:\n",
        "                pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, tail_tokens // 2)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        ok = is_correct(task, pred, gold)\n",
        "        correct += int(ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    lat = sum(times)/n\n",
        "    result = {\"task\": task, \"n\": n, \"acc\": correct / n, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "    return result\n",
        "\n",
        "# --------------- whole phase ---------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens, attn_only=attn_only)\n",
        "    tok, model, hooks = build(cfg)\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            elif task == \"gsm8k\":\n",
        "                items = load_gsm8k(n, seed)\n",
        "            elif task == \"bbh_date\":\n",
        "                items = load_bbh_date(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens, vps_on, tail_tokens)\n",
        "            results.append(res)\n",
        "            print(json.dumps(res), flush=True)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# --------------- CLI ---------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=32, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    args, _unknown = ap.parse_known_args()  # tolerate Colab's -f kernel.json\n",
        "\n",
        "    tasks, n_map = [], {}\n",
        "    if args.n_gsm8k:   tasks.append(\"gsm8k\");     n_map[\"gsm8k\"] = args.n_gsm8k\n",
        "    if args.n_bbh_date:tasks.append(\"bbh_date\");  n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n",
        "\n"
      ],
      "metadata": {
        "id": "21DLxo1HZmF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTHONUNBUFFERED=1\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_bbh_date 5 \\\n",
        "  --n_gsm8k 5 \\\n",
        "  --max_new_tokens 12 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 8 \\\n",
        "  --attn_only \\\n",
        "  --seed 1234\n"
      ],
      "metadata": {
        "id": "5i2raDXzafmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTHONUNBUFFERED=1\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 25 \\\n",
        "  --n_bbh_date 0 \\\n",
        "  --max_new_tokens 16 \\\n",
        "  --print_every 5 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 8 \\\n",
        "  --attn_only \\\n",
        "  --seed 1234\n"
      ],
      "metadata": {
        "id": "maH6JbXcbwp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/eval_suite.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, re, json, time, random, gc, argparse\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ---------- Make imports work both in Colab and as a script ----------\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))   # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))  # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# ---------- Friendly defaults for Colab/T4 ----------\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ---------- Reuse your project pieces ----------\n",
        "from vps.scripts.infer_vps import build            # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Keep VPS modules present but turn their effect off for baseline.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            if enabled:\n",
        "                m.cfg.gamma = float(getattr(m, \"_saved_gamma\", 0.5))\n",
        "                m.cfg.adaptive_gamma = True\n",
        "            else:\n",
        "                m.cfg.gamma = 0.0\n",
        "                m.cfg.adaptive_gamma = False\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype              # your build() maps this to torch dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]   # attention only\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "# --------------- tiny parsers/scorers ---------------\n",
        "NUM_RE = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "def _extract_number(s: str):\n",
        "    m = list(NUM_RE.finditer(s or \"\"))\n",
        "    return float(m[-1].group(1)) if m else None\n",
        "\n",
        "def _normalize(s: str) -> str:\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "def _extract_choice_letter(s: str):\n",
        "    s = (s or \"\").strip()\n",
        "    m = re.findall(r\"\\b([A-E])\\b\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    m = re.findall(r\"\\(([A-E])\\)\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    return None\n",
        "\n",
        "# --------------- prompt formatters ---------------\n",
        "def format_prompt_gsm8k(x):\n",
        "    q = x[\"question\"] if \"question\" in x else x.get(\"question_text\", \"\")\n",
        "    # Allow brief reasoning but enforce a clean final line\n",
        "    return (\n",
        "        \"Solve the math problem. You may use a brief scratchpad, \"\n",
        "        \"but end with a single line 'Final: <number>'.\\n\\n\"\n",
        "        f\"Problem: {q}\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "def format_prompt_arc_c(x):\n",
        "    q = x[\"question\"]; labels = x[\"choices\"][\"label\"]; texts = x[\"choices\"][\"text\"]\n",
        "    opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, texts)])\n",
        "    return (\n",
        "        \"You are given a multiple-choice science question. \"\n",
        "        \"Return ONLY the correct option letter (A, B, C, D, or E).\\n\\n\"\n",
        "        f\"{q}\\n\\n{opts}\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "def format_prompt_bbh_date(x):\n",
        "    return (\n",
        "        \"Answer the task correctly. Respond with the exact expected output string.\\n\\n\"\n",
        "        f\"{x['input']}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "# --------------- dataset loaders ---------------\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        items.append((format_prompt_arc_c(ex), str(ex[\"answerKey\"]).strip().upper()))\n",
        "    return items\n",
        "\n",
        "def load_gsm8k(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        # gold is after \"#### \"\n",
        "        ans = ex[\"answer\"]\n",
        "        m = re.search(r\"####\\s*(.+)\", ans)\n",
        "        gold = m.group(1).strip() if m else ans.strip()\n",
        "        items.append((format_prompt_gsm8k(ex), gold))\n",
        "    return items\n",
        "\n",
        "def load_bbh_date(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    # BBH community mirror\n",
        "    ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        items.append((format_prompt_bbh_date(ex), str(ex[\"target\"]).strip()))\n",
        "    return items\n",
        "\n",
        "# --------------- one VPS iteration (T4-safe, supports >1 iters) ---------------\n",
        "def vps_iterate(model, tok, prompt: str, gold_text: str,\n",
        "                max_new_tokens: int, tail_tokens: int, iters:int = 1) -> str:\n",
        "    pred = \"\"\n",
        "    for _ in range(max(1, iters)):\n",
        "        # warm-up forward on tail to get entropy\n",
        "        base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            out = model(**tail_inputs)\n",
        "            ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "        # pass entropy to policies if present\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                try:\n",
        "                    m.policy.set_token_entropy(float(ent))\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        # short CE on last few gold tokens to populate grads\n",
        "        gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "        T = min(8, gold_ids.shape[0])\n",
        "        target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "        old_cache = getattr(model.config, \"use_cache\", True)\n",
        "        model.config.use_cache = False\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        out2 = model(**tail_inputs)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            target.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "        model.config.use_cache = old_cache\n",
        "\n",
        "        # VPS-enhanced decode (greedy)\n",
        "        pred = generate(model, tok, prompt, max_new_tokens,\n",
        "                        temperature=0.0, top_p=1.0, do_sample=False)\n",
        "    return pred\n",
        "\n",
        "# --------------- run one task ---------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int, vps_iters:int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                pred = vps_iterate(model, tok, prompt, gold, max_new_tokens, tail_tokens, iters=vps_iters)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            # graceful fallback: free, shrink tail, try once more (or baseline)\n",
        "            free_cuda()\n",
        "            if vps_on and tail_tokens > 8:\n",
        "                pred = vps_iterate(model, tok, prompt, gold, max_new_tokens, tail_tokens // 2, iters=max(1, vps_iters//2))\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        # scoring\n",
        "        if task == \"gsm8k\":\n",
        "            pn, gn = _extract_number(pred), _extract_number(gold)\n",
        "            is_ok = (pn is not None and gn is not None and abs(pn - gn) < 1e-6)\n",
        "        elif task == \"bbh_date\":\n",
        "            is_ok = (_normalize(pred) == _normalize(gold))\n",
        "        elif task == \"arc_c\":\n",
        "            got = _extract_choice_letter(pred)\n",
        "            is_ok = (str(got).upper() == str(gold).upper())\n",
        "        else:\n",
        "            is_ok = False\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    lat = sum(times)/n\n",
        "    result = {\n",
        "        \"task\": task, \"n\": n,\n",
        "        \"acc\": correct / n,\n",
        "        \"latency_mean_s\": lat,\n",
        "        \"tokens_per_sec\": max(1e-9, 1.0/lat)\n",
        "    }\n",
        "    return result\n",
        "\n",
        "# --------------- whole phase ---------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool, vps_iters:int) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens, attn_only=attn_only)\n",
        "    tok, model, hooks = build(cfg)   # your build(cfg)\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()  # helps T4 OOM\n",
        "    except Exception:\n",
        "        pass\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            elif task == \"gsm8k\":\n",
        "                items = load_gsm8k(n, seed)\n",
        "            elif task == \"bbh_date\":\n",
        "                items = load_bbh_date(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens, vps_on, tail_tokens, vps_iters)\n",
        "            results.append(res)\n",
        "            print(json.dumps(res), flush=True)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# --------------- CLI ---------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=48)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=16, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    ap.add_argument(\"--vps_iters\", type=int, default=1, help=\"number of VPS refinement iters per sample\")\n",
        "    args, _unknown = ap.parse_known_args()  # tolerate Colab's -f kernel.json\n",
        "\n",
        "    tasks, n_map = [], {}\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if args.n_gsm8k:   tasks.append(\"gsm8k\");     n_map[\"gsm8k\"] = args.n_gsm8k\n",
        "    if args.n_bbh_date:tasks.append(\"bbh_date\");  n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only, vps_iters=args.vps_iters)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only, vps_iters=args.vps_iters)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "Uy3VU7fzdwB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "export HF_HUB_ENABLE_PROGRESS_BARS=1\n",
        "export TRANSFORMERS_VERBOSITY=info\n",
        "export HF_HUB_DISABLE_TELEMETRY=1\n",
        "\n",
        "# heartbeat\n",
        "( while :; do echo \"[hb] alive $(date +%T)\"; sleep 1; done ) &\n",
        "HB=$!\n",
        "\n",
        "# hard timeout so it can't hang forever\n",
        "timeout 300s stdbuf -oL -eL python -X faulthandler -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 1 \\\n",
        "  --n_bbh_date 1 \\\n",
        "  --max_new_tokens 16 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 8 || STATUS=$?\n",
        "\n",
        "kill $HB || true\n",
        "exit ${STATUS:-0}\n",
        "\n"
      ],
      "metadata": {
        "id": "2zFPsatkd00c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 10 \\\n",
        "  --n_bbh_date 10 \\\n",
        "  --max_new_tokens 24 \\\n",
        "  --print_every 5 \\\n",
        "  --model_name Qwen/Qwen2.5-1.5B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 12\n"
      ],
      "metadata": {
        "id": "8IxuG_8jjzqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "export TOKENIZERS_PARALLELISM=false\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 6 \\\n",
        "  --n_bbh_date 6 \\\n",
        "  --max_new_tokens 48 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 16 \\\n",
        "  --attn_only\n"
      ],
      "metadata": {
        "id": "xTvwArzwmnTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/eval_suite.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse, threading\n",
        "from typing import Dict, List, Tuple\n",
        "# ---------------- path setup (works in Colab or as plain script) ----------------\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))   # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))  # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator settings – help fragmentation on T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ----- import from your project (REAL VPS path) -----\n",
        "from vps.scripts.infer_vps import build             # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ---------------- misc utils ----------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool, gamma: float):\n",
        "    \"\"\"\n",
        "    Baseline: keep VPS modules present but neutralize by setting gamma=0.\n",
        "    VPS ON: restore configured gamma.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                # remember the module's original gamma if present; or use CLI gamma\n",
        "                g = float(getattr(getattr(m, \"cfg\", None), \"gamma\", gamma))\n",
        "                m._saved_gamma = g\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int,\n",
        "              attn_only: bool, gamma: float, adaptive_gamma: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name       = model_name\n",
        "    cfg.device_map       = \"auto\"\n",
        "    cfg.dtype            = dtype          # your build() maps this to torch dtype\n",
        "    cfg.torch_dtype_str  = dtype\n",
        "    cfg.max_new_tokens   = max_new_tokens\n",
        "    cfg.temperature      = 0.0\n",
        "    cfg.top_p            = 1.0\n",
        "    cfg.top_k            = 0\n",
        "    cfg.gamma            = float(gamma)\n",
        "    cfg.adaptive_gamma   = bool(adaptive_gamma)\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]   # target attention-only\n",
        "    # Encourage Q/K coupling (often helps)\n",
        "    try:\n",
        "        cfg.qk_coupling = True\n",
        "    except Exception:\n",
        "        pass\n",
        "    return cfg\n",
        "\n",
        "# ---------------- Data loaders ----------------\n",
        "def load_gsm8k(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")  # 1319 examples\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        ans = ex[\"answer\"]\n",
        "        # gold = number after '####'\n",
        "        g = ans.split(\"####\")[-1].strip()\n",
        "        prompt = (\n",
        "            \"Solve the problem carefully. Return ONLY the final number.\\n\\n\"\n",
        "            f\"Problem: {q}\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, g))\n",
        "    return items\n",
        "\n",
        "def load_bbh_date(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    \"\"\"\n",
        "    Try lukaemon/bbh (common mirror). Fallback prints a warning and returns [].\n",
        "    \"\"\"\n",
        "    if n <= 0: return []\n",
        "    try:\n",
        "        ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    except Exception:\n",
        "        print(\"[warn] Could not load BBH date_understanding; skipping.\", flush=True)\n",
        "        return []\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        inp = ex.get(\"inputs\") or ex.get(\"input\") or \"\"\n",
        "        tgt = (ex.get(\"targets\") or ex.get(\"target\") or \"\").strip()\n",
        "        prompt = (\n",
        "            \"Answer the question. Return ONLY the final answer string (no punctuation).\\n\\n\"\n",
        "            f\"{inp}\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, tgt))\n",
        "    return items\n",
        "\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    if n <= 0: return []\n",
        "    # 'validation' has 1172 and is fine for quick runs\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = str(ex[\"answerKey\"]).strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the multiple-choice question. Return ONLY the correct option letter.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "# ---------------- Answer checkers ----------------\n",
        "def _extract_number(s: str):\n",
        "    import re\n",
        "    m = list(re.finditer(r\"(-?\\d+(?:\\.\\d+)?)\", s or \"\"))\n",
        "    return float(m[-1].group(1)) if m else None\n",
        "\n",
        "def is_correct(task:str, pred:str, gold:str)->bool:\n",
        "    if task == \"gsm8k\":\n",
        "        pn, gn = _extract_number(pred), _extract_number(gold)\n",
        "        return (pn is not None and gn is not None and abs(pn - gn) < 1e-6)\n",
        "    if task == \"bbh_date\":\n",
        "        return (pred or \"\").strip().lower() == (gold or \"\").strip().lower()\n",
        "    if task == \"arc_c\":\n",
        "        import re\n",
        "        m = re.findall(r\"\\b([A-E])\\b\", (pred or \"\").upper())\n",
        "        choice = m[-1] if m else None\n",
        "        return choice == str(gold).upper()\n",
        "    return False\n",
        "\n",
        "# ---------------- VPS iteration (strong but T4-safe) ----------------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_text: str,\n",
        "                     max_new_tokens: int, tail_tokens: int, iters:int=1) -> str:\n",
        "    \"\"\"\n",
        "    1) First greedy decode to establish context (no sampling).\n",
        "    2) For 'iters' times: tail-slice forward, compute entropy, set policy; short CE on last 12 gold tokens; backward.\n",
        "    3) Final greedy decode with VPS influence.\n",
        "    \"\"\"\n",
        "    # Step 1: context decode (ignored)\n",
        "    _ = generate(model, tok, prompt, max_new_tokens=min(64, max_new_tokens),\n",
        "                 temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    try:\n",
        "        for k in range(max(1, iters)):\n",
        "            tail_inputs = slice_tail(base_inputs, tail=max(8, tail_tokens))\n",
        "            with torch.no_grad():\n",
        "                out = model(**tail_inputs)\n",
        "                ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "            # pass token entropy to policies\n",
        "            for m in model.modules():\n",
        "                if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                    try:\n",
        "                        m.policy.set_token_entropy(float(ent))\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "            # small CE on last few gold tokens (stronger than 8, but still safe)\n",
        "            gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "            T = min(12, gold_ids.shape[0])\n",
        "            target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "            model.config.use_cache = False\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            out2 = model(**tail_inputs)\n",
        "            logits2 = out2.logits[:, -T:, :]\n",
        "            ce = torch.nn.functional.cross_entropy(\n",
        "                logits2.reshape(-1, logits2.size(-1)),\n",
        "                target.reshape(-1)\n",
        "            )\n",
        "            ce.backward()\n",
        "    finally:\n",
        "        model.config.use_cache = old_cache\n",
        "\n",
        "    # Step 3: final greedy decode (VPS effect applied)\n",
        "    return generate(model, tok, prompt, max_new_tokens=max_new_tokens,\n",
        "                    temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "# ---------------- runner ----------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int, iters:int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, tail_tokens, iters=iters)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            if vps_on and tail_tokens > 8:\n",
        "                # auto-fallback: shrink tail\n",
        "                pred = vps_iterate_once(model, tok, prompt, gold, max_new_tokens, max(8, tail_tokens//2), iters=1)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        ok = is_correct(task, pred, gold)\n",
        "        correct += int(ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool, gamma: float, adaptive_gamma: bool,\n",
        "              iters:int) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens,\n",
        "                    attn_only=attn_only, gamma=gamma, adaptive_gamma=adaptive_gamma)\n",
        "    tok, model, hooks = build(cfg)\n",
        "    # Baseline neutralizes VPS by setting gamma=0 at modules; VPS ON restores\n",
        "    set_vps_enabled(model, enabled=vps_on, gamma=gamma)\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"gsm8k\":\n",
        "                items = load_gsm8k(n, seed)\n",
        "            elif task == \"bbh_date\":\n",
        "                items = load_bbh_date(n, seed)\n",
        "            elif task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens, vps_on, tail_tokens, iters)\n",
        "            results.append(res)\n",
        "            print(json.dumps(res), flush=True)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "    return results\n",
        "\n",
        "# ---------------- heartbeat (always prints) ----------------\n",
        "def start_heartbeat():\n",
        "    stop = {\"flag\": False}\n",
        "    def _hb():\n",
        "        while not stop[\"flag\"]:\n",
        "            print(f\"[hb] alive {time.strftime('%H:%M:%S')}\", flush=True)\n",
        "            time.sleep(1.0)\n",
        "    th = threading.Thread(target=_hb, daemon=True)\n",
        "    th.start()\n",
        "    return stop\n",
        "\n",
        "# ---------------- CLI ----------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=48)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=16, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.35, help=\"VPS strength (higher = stronger effect)\")\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\", help=\"scale gamma with token entropy\")\n",
        "    ap.add_argument(\"--iters\", type=int, default=1, help=\"number of VPS CE iterations (>=1); keep small on T4\")\n",
        "    args, _unknown = ap.parse_known_args()  # tolerate Colab's -f kernel.json\n",
        "\n",
        "    tasks, n_map = [], {}\n",
        "    if args.n_gsm8k:   tasks.append(\"gsm8k\");     n_map[\"gsm8k\"]   = args.n_gsm8k\n",
        "    if args.n_bbh_date:tasks.append(\"bbh_date\");  n_map[\"bbh_date\"]= args.n_bbh_date\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"]   = args.n_arc_c\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    hb = start_heartbeat()\n",
        "\n",
        "    try:\n",
        "        _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                      max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                      model_name=args.model_name, dtype=args.dtype,\n",
        "                      tail_tokens=args.tail_tokens, attn_only=args.attn_only,\n",
        "                      gamma=args.gamma, adaptive_gamma=args.adaptive_gamma,\n",
        "                      iters=1)  # baseline ignores VPS CE anyway\n",
        "\n",
        "        _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                      max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                      model_name=args.model_name, dtype=args.dtype,\n",
        "                      tail_tokens=args.tail_tokens, attn_only=args.attn_only,\n",
        "                      gamma=args.gamma, adaptive_gamma=args.adaptive_gamma,\n",
        "                      iters=args.iters)\n",
        "\n",
        "    finally:\n",
        "        hb[\"flag\"] = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "3RWVZr3zwaLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 8 \\\n",
        "  --n_bbh_date 8 \\\n",
        "  --max_new_tokens 48 \\\n",
        "  --print_every 2 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 16 \\\n",
        "  --attn_only \\\n",
        "  --gamma 0.35 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 1\n"
      ],
      "metadata": {
        "id": "yN-LwnqawbGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 12 \\\n",
        "  --n_bbh_date 12 \\\n",
        "  --max_new_tokens 48 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 24 \\\n",
        "  --gamma 0.55 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 2\n"
      ],
      "metadata": {
        "id": "WsRHADcu06jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/eval_suite.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse, re, threading, datetime\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ---------- Make imports work in Colab or as a script ----------\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))   # .../vps\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))  # .../\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# T4-friendly CUDA allocator & quiet logs\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "os.environ.setdefault(\"PYTHONUNBUFFERED\", \"1\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ---- Your project pieces ----\n",
        "from vps.scripts.infer_vps import build            # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ---- optional date parser (for BBH-Date normalization) ----\n",
        "try:\n",
        "    from dateutil import parser as _dateparse\n",
        "    HAVE_DATEUTIL = True\n",
        "except Exception:\n",
        "    HAVE_DATEUTIL = False\n",
        "\n",
        "# ---------------- utilities ----------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Keep VPS modules present but turn their effect off for baseline.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int,\n",
        "              attn_only: bool, gamma: float, adaptive_gamma: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype              # your build() maps this to torch dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]   # attention only\n",
        "    # VPS knobs:\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "# --------------- data loaders ---------------\n",
        "NUM_RE = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "def _extract_number(s: str):\n",
        "    m = list(NUM_RE.finditer(s or \"\"))\n",
        "    return float(m[-1].group(1)) if m else None\n",
        "\n",
        "def _norm_date(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    if HAVE_DATEUTIL:\n",
        "        try:\n",
        "            dt = _dateparse.parse(s, fuzzy=True, dayfirst=False)\n",
        "            return dt.strftime(\"%Y-%m-%d\")  # canonical YYYY-MM-DD\n",
        "        except Exception:\n",
        "            pass\n",
        "    return s.lower()\n",
        "\n",
        "def load_gsm8k(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    try:\n",
        "        ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    except Exception:\n",
        "        ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q   = ex.get(\"question\", ex.get(\"question_text\", \"\"))\n",
        "        ans = ex[\"answer\"]\n",
        "        m = re.search(r\"####\\s*(.+)\", ans)\n",
        "        gold = m.group(1).strip() if m else ans.strip()\n",
        "        prompt = (\n",
        "            \"Solve the following math word problem. Return ONLY the final number.\\n\\n\"\n",
        "            f\"Problem: {q}\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "def load_bbh_date(n:int, seed:int) -> List[Tuple[str, str]]:\n",
        "    if n <= 0: return []\n",
        "    items = []\n",
        "    # Try common BBH sources; fall back to a tiny built-in set if needed\n",
        "    ds = None\n",
        "    for spec in [(\"lukaemon/bbh\",\"date_understanding\"), (\"tasksource/bbh\",\"date_understanding\")]:\n",
        "        try:\n",
        "            ds = load_dataset(spec[0], spec[1], split=\"test\")\n",
        "            break\n",
        "        except Exception:\n",
        "            continue\n",
        "    if ds is None:\n",
        "        # fallback: a few deterministic samples\n",
        "        samples = [\n",
        "            (\"If today is March 1, 2020, what date is 10 days later? Answer succinctly.\", \"2020-03-11\"),\n",
        "            (\"The day after 2021-12-31 is?\", \"2022-01-01\"),\n",
        "            (\"Two weeks before July 4, 2022 is?\", \"2022-06-20\"),\n",
        "            (\"What date is 3 days after 14 Feb 2021?\", \"2021-02-17\"),\n",
        "            (\"What date is yesterday if today is 1/1/2023?\", \"2022-12-31\"),\n",
        "        ]\n",
        "        random.Random(seed).shuffle(samples)\n",
        "        return samples[:n]\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        inp = ex.get(\"input\", ex.get(\"question\", \"\"))\n",
        "        tgt = ex.get(\"target\", ex.get(\"answer\", \"\"))\n",
        "        prompt = \"Answer correctly. Return ONLY the final date.\\n\\n\" + str(inp) + \"\\nAnswer:\"\n",
        "        items.append((prompt, str(tgt)))\n",
        "    return items\n",
        "\n",
        "def score(task: str, pred: str, gold: str) -> bool:\n",
        "    if task == \"gsm8k\":\n",
        "        pn, gn = _extract_number(pred), _extract_number(gold)\n",
        "        return (pn is not None and gn is not None and abs(pn - gn) < 1e-6)\n",
        "    if task == \"bbh_date\":\n",
        "        return _norm_date(pred) == _norm_date(gold)\n",
        "    if task == \"arc_c\":\n",
        "        # not used by default in this file, but keep for completeness\n",
        "        def _letter(s):\n",
        "            m = re.findall(r\"\\b([A-E])\\b\", s.upper())\n",
        "            if m: return m[-1]\n",
        "            m = re.findall(r\"\\(([A-E])\\)\", s.upper())\n",
        "            return m[-1] if m else None\n",
        "        return _letter(pred) == str(gold).upper()\n",
        "    return False\n",
        "\n",
        "# --------------- one or more VPS iterations ---------------\n",
        "def vps_iterate(model, tok, prompt: str, gold_text: str,\n",
        "                max_new_tokens: int, tail_tokens: int, iters: int) -> str:\n",
        "    # Warmup decode (no sample) to stabilize entropy/policies\n",
        "    _ = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    for k in range(max(1, iters)):\n",
        "        # 1) entropy signal\n",
        "        with torch.no_grad():\n",
        "            out = model(**tail_inputs)\n",
        "            ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "        for m in model.modules():\n",
        "            if hasattr(m, \"policy\") and m.policy is not None:\n",
        "                try:\n",
        "                    m.policy.set_token_entropy(float(ent))\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        # 2) short CE to populate grads\n",
        "        gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "        T = min(8, gold_ids.shape[0])\n",
        "        target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "        old_cache = getattr(model.config, \"use_cache\", True)\n",
        "        model.config.use_cache = False\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        out2 = model(**tail_inputs)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            target.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "        model.config.use_cache = old_cache\n",
        "\n",
        "    # Final VPS-enhanced decode (greedy)\n",
        "    return generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "# --------------- runner ---------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]], tok, model,\n",
        "             print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int, iters: int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {'VPS' if vps_on else 'BASELINE'} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                pred = vps_iterate(model, tok, prompt, gold, max_new_tokens, tail_tokens, iters)\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            # fallback strategy: shrink tail or drop to baseline\n",
        "            if vps_on and tail_tokens > 8:\n",
        "                pred = vps_iterate(model, tok, prompt, gold, max_new_tokens, max(8, tail_tokens//2), max(1, iters//2))\n",
        "            else:\n",
        "                pred = generate(model, tok, prompt, max_new_tokens, temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        is_ok = score(task, pred, gold)\n",
        "        correct += int(is_ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "            print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    acc = correct / n\n",
        "    lat = sum(times)/n\n",
        "    return {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "\n",
        "def heartbeat(stop_evt: threading.Event):\n",
        "    while not stop_evt.is_set():\n",
        "        now = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
        "        print(f\"[hb] alive {now}\", flush=True)\n",
        "        stop_evt.wait(1.0)\n",
        "\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool, gamma: float, adaptive_gamma: bool,\n",
        "              iters: int) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens,\n",
        "                    attn_only=attn_only, gamma=gamma, adaptive_gamma=adaptive_gamma)\n",
        "\n",
        "    tok, model, hooks = build(cfg)\n",
        "    # Turn effect off for baseline:\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    # ---- GAMMA DIAGNOSTIC PRINT (what you asked for) ----\n",
        "    any_gamma = None\n",
        "    for n, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma\n",
        "            break\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "    # -----------------------------------------------------\n",
        "\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"gsm8k\":\n",
        "                items = load_gsm8k(n, seed)\n",
        "            elif task == \"bbh_date\":\n",
        "                items = load_bbh_date(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens, vps_on, tail_tokens, iters)\n",
        "            results.append(res)\n",
        "            print(json.dumps(res), flush=True)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# --------------- CLI ---------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)  # not used by default here\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24)\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\")\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.55)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--iters\", type=int, default=2, help=\"number of VPS CE steps\")\n",
        "    args, _unknown = ap.parse_known_args()  # tolerate Colab's -f kernel.json\n",
        "\n",
        "    tasks, n_map = [], {}\n",
        "    if args.n_gsm8k:   tasks.append(\"gsm8k\");     n_map[\"gsm8k\"] = args.n_gsm8k\n",
        "    if args.n_bbh_date:tasks.append(\"bbh_date\");  n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    if args.n_arc_c:   print(\"[warn] arc_c loader not in this file — set n_arc_c=0\")\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\"); return\n",
        "\n",
        "    # heartbeat thread so you always see progress\n",
        "    stop_evt = threading.Event()\n",
        "    t = threading.Thread(target=heartbeat, args=(stop_evt,), daemon=True)\n",
        "    t.start()\n",
        "    try:\n",
        "        _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                      max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                      model_name=args.model_name, dtype=args.dtype,\n",
        "                      tail_tokens=args.tail_tokens, attn_only=args.attn_only,\n",
        "                      gamma=args.gamma, adaptive_gamma=args.adaptive_gamma, iters=max(1, args.iters//2))\n",
        "\n",
        "        _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                      max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                      model_name=args.model_name, dtype=args.dtype,\n",
        "                      tail_tokens=args.tail_tokens, attn_only=args.attn_only,\n",
        "                      gamma=args.gamma, adaptive_gamma=args.adaptive_gamma, iters=args.iters)\n",
        "    finally:\n",
        "        stop_evt.set()\n",
        "        t.join(timeout=0.1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "gncGHWpC8cd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 6 \\\n",
        "  --n_bbh_date 6 \\\n",
        "  --max_new_tokens 48 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 24 \\\n",
        "  --gamma 0.55 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 2\n"
      ],
      "metadata": {
        "id": "pSjAESHm8dTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/eval_suite.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse, re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ----- Colab / script import hygiene -----\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# Keep TF/XLA chatty logs down and make CUDA alloc robust on T4\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ---- Your project pieces ----\n",
        "from vps.scripts.infer_vps import build            # build(cfg) -> (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ---------------- utils ----------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Toggle VPS effect without removing wrappers (gamma=0 for baseline).\"\"\"\n",
        "    for m in model.modules():\n",
        "        if getattr(m, \"__class__\", None) and m.__class__.__name__ == \"VPSLinear\":\n",
        "            # remember original gamma the first time we see it\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int,\n",
        "              attn_only: bool, gamma: float, adaptive_gamma: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype               # your build() maps to torch dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]   # limit to attention\n",
        "    return cfg\n",
        "\n",
        "# -------------- scoring helpers --------------\n",
        "NUM_RE = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "def _extract_number(s: str):\n",
        "    m = list(NUM_RE.finditer(s or \"\"))\n",
        "    return float(m[-1].group(1)) if m else None\n",
        "\n",
        "def _normalize(s: str):\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "def _extract_choice_letter(s: str):\n",
        "    s = (s or \"\").strip()\n",
        "    m = re.findall(r\"\\b([A-E])\\b\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    m = re.findall(r\"\\(([A-E])\\)\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    return None\n",
        "\n",
        "# ----- BBH date normalization -----\n",
        "def _to_iso_date(text: str) -> str|None:\n",
        "    if not text: return None\n",
        "    t = text.strip()\n",
        "    # direct ISO match\n",
        "    m = re.search(r\"\\b(\\d{4})-(\\d{2})-(\\d{2})\\b\", t)\n",
        "    if m:\n",
        "        y,mn,dd = m.groups()\n",
        "        return f\"{int(y):04d}-{int(mn):02d}-{int(dd):02d}\"\n",
        "    # try dateutil if present\n",
        "    try:\n",
        "        from dateutil import parser as du\n",
        "        dt = du.parse(t, dayfirst=False, yearfirst=True, fuzzy=True)\n",
        "        return f\"{dt.year:04d}-{dt.month:02d}-{dt.day:02d}\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    # very light fallback like \"March 5, 2020\"\n",
        "    m2 = re.search(r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\.?\\s+(\\d{1,2}),?\\s+(\\d{4})\", t, re.I)\n",
        "    if m2:\n",
        "        mon_map = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"sept\":9,\"oct\":10,\"nov\":11,\"dec\":12}\n",
        "        mon = mon_map[m2.group(1).lower()[:3]]\n",
        "        d   = int(m2.group(2)); y = int(m2.group(3))\n",
        "        return f\"{y:04d}-{mon:02d}-{d:02d}\"\n",
        "    return None\n",
        "\n",
        "# -------------- datasets --------------\n",
        "def load_gsm8k(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"] if \"question\" in ex else ex[\"question_text\"]\n",
        "        ans = ex[\"answer\"]\n",
        "        m = re.search(r\"####\\s*(.+)\", ans)\n",
        "        gold = (m.group(1).strip() if m else ans.strip())\n",
        "        prompt = (\n",
        "            \"Solve the problem step by step, but return ONLY the final number.\\n\\n\"\n",
        "            f\"Problem: {q}\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "def load_bbh_date(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        # bbh date has fields \"input\" and \"target\"\n",
        "        inp = ex[\"input\"]\n",
        "        gold = str(ex[\"target\"]).strip()\n",
        "        gold_iso = _to_iso_date(gold) or gold\n",
        "        prompt = (\n",
        "            \"You are given a date reasoning problem.\\n\"\n",
        "            \"Return ONLY the final date in the exact format YYYY-MM-DD.\\n\\n\"\n",
        "            f\"{inp}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_iso))\n",
        "    return items\n",
        "\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Answer the multiple-choice science question.\\n\"\n",
        "            \"Return ONLY the option letter (A, B, C, D, or E).\\n\\n\"\n",
        "            f\"{q}\\n\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "# -------------- one VPS step --------------\n",
        "def vps_step(model, tok, inputs, gold_text: str, tail_tokens: int):\n",
        "    \"\"\"One light CE backprop on last few gold tokens to drive VPS.\"\"\"\n",
        "    tail_inputs = slice_tail(inputs, tail=tail_tokens)\n",
        "\n",
        "    # entropy for policy\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try: m.policy.set_token_entropy(float(ent))\n",
        "            except Exception: pass\n",
        "\n",
        "    # short CE surrogate to create grads\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    with torch.enable_grad():\n",
        "        out2 = model(**tail_inputs)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            target.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "def _score(task:str, pred:str, gold:str) -> bool:\n",
        "    if task == \"gsm8k\":\n",
        "        pn, gn = _extract_number(pred), _extract_number(gold)\n",
        "        return (pn is not None and gn is not None and abs(pn - gn) < 1e-6)\n",
        "    if task == \"bbh_date\":\n",
        "        p_iso = _to_iso_date(pred)\n",
        "        g_iso = _to_iso_date(gold) or gold\n",
        "        return (p_iso == g_iso)\n",
        "    if task == \"arc_c\":\n",
        "        got = _extract_choice_letter(pred)\n",
        "        return (got == str(gold).upper())\n",
        "    return False\n",
        "\n",
        "# -------------- run one task --------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int, iters:int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {('VPS' if vps_on else 'BASELINE')} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times: List[float] = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            if vps_on:\n",
        "                # optional warm decode to anchor context (no-op for scoring)\n",
        "                _ = generate(model, tok, prompt, max_new_tokens=min(24, max_new_tokens),\n",
        "                             temperature=0.0, top_p=1.0, do_sample=False)\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_step(model, tok, inputs, gold, tail_tokens)\n",
        "            # final decode (greedy)\n",
        "            pred = generate(model, tok, prompt, max_new_tokens=max_new_tokens,\n",
        "                            temperature=0.0, top_p=1.0, do_sample=False)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            # shrink tail and retry once if VPS, else vanilla\n",
        "            if vps_on and tail_tokens > 8:\n",
        "                try:\n",
        "                    vps_step(model, tok, inputs, gold, max(8, tail_tokens//2))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            pred = generate(model, tok, prompt, max_new_tokens=max_new_tokens,\n",
        "                            temperature=0.0, top_p=1.0, do_sample=False)\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "        ok = _score(task, pred, gold)\n",
        "        correct += int(ok)\n",
        "\n",
        "        eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "        print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    lat = sum(times)/n\n",
        "    res = {\"task\": task, \"n\": n, \"acc\": correct/n, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "    return res\n",
        "\n",
        "# -------------- whole phase --------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool, gamma: float, adaptive_gamma: bool,\n",
        "              iters:int) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens,\n",
        "                    attn_only=attn_only, gamma=gamma, adaptive_gamma=adaptive_gamma)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Toggle VPS effect\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    # gamma diagnostic (first VPSLinear)\n",
        "    any_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma\n",
        "            break\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"gsm8k\":\n",
        "                items = load_gsm8k(n, seed)\n",
        "            elif task == \"bbh_date\":\n",
        "                items = load_bbh_date(n, seed)\n",
        "            elif task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens,\n",
        "                           vps_on, tail_tokens, iters)\n",
        "            print(json.dumps(res), flush=True)\n",
        "            results.append(res)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------- CLI --------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.6, help=\"VPS gamma (ignored in baseline)\")\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\", help=\"enable adaptive gamma\")\n",
        "    ap.add_argument(\"--iters\", type=int, default=1, help=\"count of VPS CE steps per item\")\n",
        "    args, _unknown = ap.parse_known_args()  # tolerate Colab's -f kernel.json\n",
        "\n",
        "    tasks, n_map = [], {}\n",
        "    if args.n_gsm8k:   tasks.append(\"gsm8k\");     n_map[\"gsm8k\"] = args.n_gsm8k\n",
        "    if args.n_bbh_date:tasks.append(\"bbh_date\");  n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\")\n",
        "        return\n",
        "\n",
        "    # BASELINE\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only,\n",
        "                  gamma=args.gamma, adaptive_gamma=args.adaptive_gamma, iters=0)\n",
        "\n",
        "    # VPS ON\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only,\n",
        "                  gamma=args.gamma, adaptive_gamma=args.adaptive_gamma, iters=max(1,args.iters))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "EXyI6pjiF5hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip -q install python-dateutil\n"
      ],
      "metadata": {
        "id": "koNuJBdFF6Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "# install normalizer just in case\n",
        "pip -q install python-dateutil >/dev/null\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 8 \\\n",
        "  --n_bbh_date 8 \\\n",
        "  --max_new_tokens 16 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 12 \\\n",
        "  --iters 1 \\\n",
        "  --gamma 0.7 \\\n",
        "  --adaptive_gamma \\\n",
        "  --attn_only\n"
      ],
      "metadata": {
        "id": "L0QwmxZGF8ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/eval_suite.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, json, time, random, gc, argparse, re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ---- Colab / script import hygiene\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {PROJ_PARENT, PROJ_ROOT, \"/content\", \"/content/vps\"}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ---- Your project pieces\n",
        "from vps.scripts.infer_vps import build            # build(cfg) -> (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "# ---------------- utils ----------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Toggle VPS effect without removing wrappers (gamma=0 for baseline).\"\"\"\n",
        "    for m in model.modules():\n",
        "        if getattr(m, \"__class__\", None) and m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int,\n",
        "              attn_only: bool, gamma: float, adaptive_gamma: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    return cfg\n",
        "\n",
        "# -------------- decode ONLY new tokens (fixes scoring errors) --------------\n",
        "def greedy_new_text(model, tok, prompt: str, max_new_tokens: int) -> str:\n",
        "    ins = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(\n",
        "        **ins, max_new_tokens=max_new_tokens,\n",
        "        do_sample=False, return_dict_in_generate=True\n",
        "    )\n",
        "    seq = out.sequences[0]\n",
        "    inp_len = ins[\"input_ids\"].shape[1]\n",
        "    new_ids = seq[inp_len:]\n",
        "    return tok.decode(new_ids, skip_special_tokens=True)\n",
        "\n",
        "# -------------- scoring helpers --------------\n",
        "NUM_RE = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
        "\n",
        "def _extract_number(s: str):\n",
        "    m = list(NUM_RE.finditer(s or \"\"))\n",
        "    return float(m[-1].group(1)) if m else None\n",
        "\n",
        "def _normalize(s: str):\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "def _extract_choice_letter(s: str):\n",
        "    s = (s or \"\").strip()\n",
        "    m = re.findall(r\"\\b([A-E])\\b\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    m = re.findall(r\"\\(([A-E])\\)\", s.upper())\n",
        "    if m: return m[-1]\n",
        "    return None\n",
        "\n",
        "# ----- BBH date normalization -----\n",
        "def _to_iso_date(text: str) -> str|None:\n",
        "    if not text: return None\n",
        "    t = text.strip()\n",
        "\n",
        "    # Prefer the LAST ISO-looking date in the generated delta\n",
        "    iso_all = list(re.finditer(r\"(\\d{4})-(\\d{2})-(\\d{2})\", t))\n",
        "    if iso_all:\n",
        "        y,mn,dd = iso_all[-1].groups()\n",
        "        return f\"{int(y):04d}-{int(mn):02d}-{int(dd):02d}\"\n",
        "\n",
        "    # try python-dateutil if present\n",
        "    try:\n",
        "        from dateutil import parser as du\n",
        "        dt = du.parse(t, dayfirst=False, yearfirst=True, fuzzy=True)\n",
        "        return f\"{dt.year:04d}-{dt.month:02d}-{dt.day:02d}\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # light fallback like \"March 5, 2020\"\n",
        "    m2 = re.search(r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\.?\\s+(\\d{1,2}),?\\s+(\\d{4})\", t, re.I)\n",
        "    if m2:\n",
        "        mon_map = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"sept\":9,\"oct\":10,\"nov\":11,\"dec\":12}\n",
        "        mon = mon_map[m2.group(1).lower()[:3]]\n",
        "        d   = int(m2.group(2)); y = int(m2.group(3))\n",
        "        return f\"{y:04d}-{mon:02d}-{d:02d}\"\n",
        "    return None\n",
        "\n",
        "# -------------- datasets --------------\n",
        "def load_gsm8k(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex.get(\"question\", ex.get(\"question_text\", \"\"))\n",
        "        ans = ex[\"answer\"]\n",
        "        m = re.search(r\"####\\s*(.+)\", ans)\n",
        "        gold = (m.group(1).strip() if m else ans.strip())\n",
        "        prompt = (\n",
        "            \"Solve the problem. Return ONLY the final number with no words.\\n\\n\"\n",
        "            f\"Problem: {q}\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "def load_bbh_date(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"lukaemon/bbh\", \"date_understanding\", split=\"test\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        inp = ex[\"input\"]\n",
        "        gold = str(ex[\"target\"]).strip()\n",
        "        gold_iso = _to_iso_date(gold) or gold\n",
        "        prompt = (\n",
        "            \"You are given a date reasoning problem.\\n\"\n",
        "            \"Return ONLY the final date in EXACT format YYYY-MM-DD (no words).\\n\\n\"\n",
        "            f\"{inp}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_iso))\n",
        "    return items\n",
        "\n",
        "def load_arc_c(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    if n <= 0: return []\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold_label = ex[\"answerKey\"]\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"Answer the multiple-choice science question.\\n\"\n",
        "            \"Return ONLY the option letter (A, B, C, D, or E).\\n\\n\"\n",
        "            f\"{q}\\n\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold_label))\n",
        "    return items\n",
        "\n",
        "# -------------- one VPS step --------------\n",
        "def vps_step(model, tok, inputs, gold_text: str, tail_tokens: int):\n",
        "    tail_inputs = slice_tail(inputs, tail=tail_tokens)\n",
        "\n",
        "    # entropy for policy\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try: m.policy.set_token_entropy(float(ent))\n",
        "            except Exception: pass\n",
        "\n",
        "    # short CE surrogate\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(8, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    with torch.enable_grad():\n",
        "        out2 = model(**tail_inputs)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            target.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "def _score(task:str, pred_delta:str, gold:str) -> bool:\n",
        "    if task == \"gsm8k\":\n",
        "        pn, gn = _extract_number(pred_delta), _extract_number(gold)\n",
        "        return (pn is not None and gn is not None and abs(pn - gn) < 1e-6)\n",
        "    if task == \"bbh_date\":\n",
        "        p_iso = _to_iso_date(pred_delta)\n",
        "        g_iso = _to_iso_date(gold) or gold\n",
        "        return (p_iso == g_iso)\n",
        "    if task == \"arc_c\":\n",
        "        got = _extract_choice_letter(pred_delta)\n",
        "        return (got == str(gold).upper())\n",
        "    return False\n",
        "\n",
        "# -------------- run one task --------------\n",
        "def run_task(task: str, items: List[Tuple[str,str]],\n",
        "             tok, model, print_every: int, max_new_tokens: int, vps_on: bool,\n",
        "             tail_tokens: int, iters:int) -> Dict:\n",
        "    if len(items) == 0:\n",
        "        print(f\"=== {('VPS' if vps_on else 'BASELINE')} :: {task} n=0 — skipping ===\", flush=True)\n",
        "        return {\"task\": task, \"n\": 0, \"acc\": 0.0, \"latency_mean_s\": 0.0, \"tokens_per_sec\": 0.0}\n",
        "\n",
        "    correct = 0\n",
        "    times: List[float] = []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            if vps_on:\n",
        "                # quick warm decode (no scoring)\n",
        "                _ = greedy_new_text(model, tok, prompt, max_new_tokens=min(24, max_new_tokens))\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_step(model, tok, inputs, gold, tail_tokens)\n",
        "            # final decode (delta only)\n",
        "            pred_delta = greedy_new_text(model, tok, prompt, max_new_tokens=max_new_tokens)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            if vps_on and tail_tokens > 8:\n",
        "                try:\n",
        "                    vps_step(model, tok, inputs, gold, max(8, tail_tokens//2))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            pred_delta = greedy_new_text(model, tok, prompt, max_new_tokens=max_new_tokens)\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "        ok = _score(task, pred_delta, gold)\n",
        "        correct += int(ok)\n",
        "\n",
        "        eta = (sum(times)/len(times)) * (len(items)-i)\n",
        "        print(f\"[{task}] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    n = len(items)\n",
        "    lat = sum(times)/n\n",
        "    res = {\"task\": task, \"n\": n, \"acc\": correct/n, \"latency_mean_s\": lat, \"tokens_per_sec\": max(1e-9, 1.0/lat)}\n",
        "    return res\n",
        "\n",
        "# -------------- whole phase --------------\n",
        "def run_phase(tasks_order: List[str], n_map: Dict[str,int], seed: int,\n",
        "              vps_on: bool, max_new_tokens: int, print_every: int, model_name: str,\n",
        "              dtype: str, tail_tokens: int, attn_only: bool, gamma: float, adaptive_gamma: bool,\n",
        "              iters:int) -> List[Dict]:\n",
        "    phase_name = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase_name} — building model...\", flush=True)\n",
        "\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name=model_name, dtype=dtype, max_new_tokens=max_new_tokens,\n",
        "                    attn_only=attn_only, gamma=gamma, adaptive_gamma=adaptive_gamma)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Toggle VPS\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    # gamma diagnostic\n",
        "    any_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma; break\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for task in tasks_order:\n",
        "            n = int(n_map.get(task, 0))\n",
        "            if task == \"gsm8k\":\n",
        "                items = load_gsm8k(n, seed)\n",
        "            elif task == \"bbh_date\":\n",
        "                items = load_bbh_date(n, seed)\n",
        "            elif task == \"arc_c\":\n",
        "                items = load_arc_c(n, seed)\n",
        "            else:\n",
        "                items = []\n",
        "            res = run_task(task, items, tok, model, print_every, max_new_tokens,\n",
        "                           vps_on, tail_tokens, iters)\n",
        "            print(json.dumps(res), flush=True)\n",
        "            results.append(res)\n",
        "    finally:\n",
        "        del model; del tok\n",
        "        free_cuda()\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------- CLI --------------\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_gsm8k\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_bbh_date\", type=int, default=0)\n",
        "    ap.add_argument(\"--n_arc_c\", type=int, default=0)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--max_new_tokens\", type=int, default=48)\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24, help=\"prompt tail used for CE/backprop (VPS only)\")\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\", help=\"apply VPS only to attention Q/K/V/O\")\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.6, help=\"VPS gamma (ignored in baseline)\")\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\", help=\"enable adaptive gamma\")\n",
        "    ap.add_argument(\"--iters\", type=int, default=1, help=\"VPS CE steps per item\")\n",
        "    args, _unknown = ap.parse_known_args()\n",
        "\n",
        "    tasks, n_map = [], {}\n",
        "    if args.n_gsm8k:   tasks.append(\"gsm8k\");     n_map[\"gsm8k\"] = args.n_gsm8k\n",
        "    if args.n_bbh_date:tasks.append(\"bbh_date\");  n_map[\"bbh_date\"] = args.n_bbh_date\n",
        "    if args.n_arc_c:   tasks.append(\"arc_c\");     n_map[\"arc_c\"] = args.n_arc_c\n",
        "    if not tasks:\n",
        "        print(\"No tasks requested; nothing to run.\"); return\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=False,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only,\n",
        "                  gamma=args.gamma, adaptive_gamma=args.adaptive_gamma, iters=0)\n",
        "\n",
        "    _ = run_phase(tasks, n_map, args.seed, vps_on=True,\n",
        "                  max_new_tokens=args.max_new_tokens, print_every=args.print_every,\n",
        "                  model_name=args.model_name, dtype=args.dtype,\n",
        "                  tail_tokens=args.tail_tokens, attn_only=args.attn_only,\n",
        "                  gamma=args.gamma, adaptive_gamma=args.adaptive_gamma, iters=max(1,args.iters))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "ZSkvWJQ6Iv89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_gsm8k 8 \\\n",
        "  --n_bbh_date 8 \\\n",
        "  --max_new_tokens 64 \\\n",
        "  --print_every 1 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 24 \\\n",
        "  --gamma 0.6 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 2\n"
      ],
      "metadata": {
        "id": "-tJyc7uZIw7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_arc_c 20 \\\n",
        "  --max_new_tokens 32 \\\n",
        "  --print_every 5 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 24 \\\n",
        "  --gamma 0.6 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 2\n"
      ],
      "metadata": {
        "id": "_cMm1EyxOLIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/eval_suite.py \\\n",
        "  --n_arc_c 80 \\\n",
        "  --max_new_tokens 24 \\\n",
        "  --print_every 5 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 24 \\\n",
        "  --gamma 0.7 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 2 \\\n",
        "  --attn_only\n"
      ],
      "metadata": {
        "id": "XPN7IhUDSI3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/arc_mc_eval.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, re, random, time, json, argparse, gc\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# --- Make imports work no matter how this is launched ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {\"/content\", \"/content/vps\", PROJ_PARENT, PROJ_ROOT}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Your project pieces ---\n",
        "from vps.scripts.infer_vps import build\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "LETTER_RE = re.compile(r\"Answer\\s*[:\\-]?\\s*([A-E])\\b\", re.IGNORECASE)\n",
        "\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool,\n",
        "              gamma: float, adaptive_gamma: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]   # attention-only VPS\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "def load_arc_items(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. IMPORTANT: Output **one letter only** (A/B/C/D/E) and nothing else.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "def extract_letter(s: str) -> str | None:\n",
        "    if not s: return None\n",
        "    m = LETTER_RE.search(s)\n",
        "    if m: return m.group(1).upper()\n",
        "    # Fallback: last 10 chars\n",
        "    tail = s.strip().upper()[-10:]\n",
        "    m2 = re.search(r\"\\b([A-E])\\b\", tail)\n",
        "    return m2.group(1) if m2 else None\n",
        "\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_text: str,\n",
        "                     tail_tokens: int, ce_T: int = 6) -> None:\n",
        "    \"\"\"\n",
        "    One VPS pass that populates grads using a tiny CE on the tail.\n",
        "    No return: it just conditions internal VPS modules via gradients.\n",
        "    \"\"\"\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    # entropy probe (no grad)\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # tiny CE to populate grads\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(ce_T, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)),\n",
        "        target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "def run_phase(n:int, seed:int, vps_on:bool, model_name:str, dtype:str,\n",
        "              tail_tokens:int, gamma:float, adaptive_gamma:bool,\n",
        "              iters:int, attn_only:bool, print_every:int) -> Dict:\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name, dtype, max_new_tokens=1, attn_only=attn_only,\n",
        "                    gamma=gamma, adaptive_gamma=adaptive_gamma)\n",
        "\n",
        "    phase = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase} — building model...\", flush=True)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # baseline: disable effect but keep modules present\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    # show a gamma sample\n",
        "    any_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma; break\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    items = load_arc_items(n, seed=seed)\n",
        "\n",
        "    correct, times = 0, []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                # small number of VPS iterations\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=tail_tokens, ce_T=6)\n",
        "            # final decode: 1 token greedy (must be the letter)\n",
        "            out_ids = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            gen = model.generate(**out_ids, max_new_tokens=1, do_sample=False)\n",
        "            pred_text = tok.decode(gen[0], skip_special_tokens=True)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            # fallback: zero VPS and decode\n",
        "            out_ids = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            gen = model.generate(**out_ids, max_new_tokens=1, do_sample=False)\n",
        "            pred_text = tok.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "\n",
        "        got = extract_letter(pred_text)\n",
        "        ok = (got == gold.upper())\n",
        "        correct += int(ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == n):\n",
        "            eta = (sum(times)/len(times))*(n-i)\n",
        "            print(f\"[arc_c] {i}/{n} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    acc = correct / n if n else 0.0\n",
        "    lat = sum(times)/max(1,len(times))\n",
        "    res = {\"task\":\"arc_c\", \"n\": n, \"acc\": acc, \"latency_mean_s\": lat}\n",
        "    print(json.dumps(res), flush=True)\n",
        "\n",
        "    del model, tok\n",
        "    free_cuda()\n",
        "    return res\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n\", type=int, default=40)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24)\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.65)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--iters\", type=int, default=2)\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\")\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    args, _ = ap.parse_known_args()\n",
        "\n",
        "    # Baseline\n",
        "    _ = run_phase(n=args.n, seed=args.seed, vps_on=False, model_name=args.model_name,\n",
        "                  dtype=args.dtype, tail_tokens=args.tail_tokens, gamma=args.gamma,\n",
        "                  adaptive_gamma=args.adaptive_gamma, iters=args.iters,\n",
        "                  attn_only=args.attn_only, print_every=args.print_every)\n",
        "\n",
        "    # VPS\n",
        "    _ = run_phase(n=args.n, seed=args.seed, vps_on=True, model_name=args.model_name,\n",
        "                  dtype=args.dtype, tail_tokens=args.tail_tokens, gamma=args.gamma,\n",
        "                  adaptive_gamma=args.adaptive_gamma, iters=args.iters,\n",
        "                  attn_only=args.attn_only, print_every=args.print_every)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "wRQqXKwpbWn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/arc_mc_eval.py \\\n",
        "  --n 40 \\\n",
        "  --seed 1234 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 24 \\\n",
        "  --gamma 0.65 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 2 \\\n",
        "  --attn_only \\\n",
        "  --print_every 5\n"
      ],
      "metadata": {
        "id": "YGQvogwGbXf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/arc_mc_eval.py \\\n",
        "  --n 80 \\\n",
        "  --seed 1234 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 32 \\\n",
        "  --gamma 0.70 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 3 \\\n",
        "  --attn_only \\\n",
        "  --print_every 5\n"
      ],
      "metadata": {
        "id": "xutBK9qXcrcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import os, sys, re, random, time, json, argparse, gc, math\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# --- robust imports (Colab/script) ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {\"/content\", \"/content/vps\", PROJ_PARENT, PROJ_ROOT}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# less noise + fewer OOMs on T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- your project pieces ---\n",
        "from vps.scripts.infer_vps import build\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate  # kept for --score gen option\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "LETTER_RE = re.compile(r\"\\b([A-E])\\b\")\n",
        "\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"For baseline: keep VPS modules present but turn their effect off.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int, attn_only: bool,\n",
        "              gamma: float, adaptive_gamma: bool) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    if attn_only:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]   # attention-only\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "def load_arc_items(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds))); random.Random(seed).shuffle(idx); idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. Output ONE LETTER only (A/B/C/D/E).\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "# ---------- robust, generation-free scoring ----------\n",
        "def _candidate_ids(tok, letter:str):\n",
        "    \"\"\"Return the plausible single next-token ids for letter, with/without leading space.\"\"\"\n",
        "    ids_bare = tok(letter, add_special_tokens=False)[\"input_ids\"]\n",
        "    ids_sp   = tok(\" \" + letter, add_special_tokens=False)[\"input_ids\"]\n",
        "    cand = []\n",
        "    if len(ids_bare) >= 1: cand.append(ids_bare[0])\n",
        "    if len(ids_sp)   >= 1: cand.append(ids_sp[0])\n",
        "    # de-dup\n",
        "    return list(dict.fromkeys(cand))\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_by_logits(model, tok, prompt:str) -> str | None:\n",
        "    # Ensure the next-token position is right after \"Answer:\"\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model(**inputs)\n",
        "    logits = out.logits[0, -1, :]  # next-token logits\n",
        "\n",
        "    # candidate id sets per letter\n",
        "    letters = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "    scores = {}\n",
        "    for L in letters:\n",
        "        cand_ids = _candidate_ids(tok, L)\n",
        "        if not cand_ids:\n",
        "            scores[L] = float(\"-inf\")\n",
        "            continue\n",
        "        # use maximum logit among variants to be robust to spacing/BPE\n",
        "        scores[L] = float(torch.max(logits[cand_ids]).item())\n",
        "    # argmax\n",
        "    best = max(letters, key=lambda L: scores[L])\n",
        "    # if all -inf, return None\n",
        "    return best if math.isfinite(scores[best]) else None\n",
        "\n",
        "def extract_letter_from_gen(text:str) -> str | None:\n",
        "    # parse last 10 chars fallback\n",
        "    tail = (text or \"\").strip()[-10:].upper()\n",
        "    m = LETTER_RE.search(tail)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# ---------- one VPS conditioning pass ----------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_text: str,\n",
        "                     tail_tokens: int, ce_T: int = 6) -> None:\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    # entropy probe (no grad)\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try: m.policy.set_token_entropy(float(ent))\n",
        "            except Exception: pass\n",
        "\n",
        "    # tiny CE to populate grads\n",
        "    gold_ids = tok(gold_text, return_tensors=\"pt\").to(model.device)[\"input_ids\"][0]\n",
        "    T = min(ce_T, gold_ids.shape[0])\n",
        "    target = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "    out2 = model(**tail_inputs)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    ce = torch.nn.functional.cross_entropy(\n",
        "        logits2.reshape(-1, logits2.size(-1)), target.reshape(-1)\n",
        "    )\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "# ---------- run both phases on the SAME items ----------\n",
        "def run_phase(items:List[Tuple[str,str]], vps_on:bool, model_name:str, dtype:str,\n",
        "              tail_tokens:int, gamma:float, adaptive_gamma:bool,\n",
        "              iters:int, attn_only:bool, print_every:int,\n",
        "              score_mode:str) -> Dict:\n",
        "    cfg = build_cfg(model_name, dtype, max_new_tokens=1, attn_only=attn_only,\n",
        "                    gamma=gamma, adaptive_gamma=adaptive_gamma)\n",
        "    phase = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase} — building model...\", flush=True)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # (de)activate VPS effect\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    any_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma; break\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    correct, times, preds = 0, [], []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=tail_tokens, ce_T=6)\n",
        "\n",
        "            if score_mode == \"logit\":\n",
        "                got = predict_by_logits(model, tok, prompt)\n",
        "            else:\n",
        "                # legacy: 1-token greedy then parse\n",
        "                out_ids = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                gen = model.generate(**out_ids, max_new_tokens=1, do_sample=False)\n",
        "                text = tok.decode(gen[0], skip_special_tokens=True)\n",
        "                got = extract_letter_from_gen(text)\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            got = None\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "        preds.append(got)\n",
        "        ok = (got == gold.upper())\n",
        "        correct += int(ok)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == len(items)):\n",
        "            eta = (sum(times)/len(times))*(len(items)-i)\n",
        "            print(f\"[arc_c] {i}/{len(items)} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    acc = correct / max(1,len(items))\n",
        "    lat = sum(times)/max(1,len(times))\n",
        "    res = {\"task\":\"arc_c\", \"n\": len(items), \"acc\": acc, \"latency_mean_s\": lat, \"preds\": preds}\n",
        "    print(json.dumps({k:v for k,v in res.items() if k!='preds'}), flush=True)\n",
        "\n",
        "    del model, tok\n",
        "    free_cuda()\n",
        "    return res\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n\", type=int, default=80)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=32)\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.70)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--iters\", type=int, default=3)\n",
        "    ap.add_argument(\"--attn_only\", action=\"store_true\")\n",
        "    ap.add_argument(\"--print_every\", type=int, default=10)\n",
        "    ap.add_argument(\"--score\", type=str, choices=[\"logit\",\"gen\"], default=\"logit\",\n",
        "                    help=\"logit = next-token argmax over letters; gen = 1-token greedy then parse\")\n",
        "    args, _ = ap.parse_known_args()\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    items = load_arc_items(args.n, args.seed)\n",
        "\n",
        "    base = run_phase(items, vps_on=False, model_name=args.model_name, dtype=args.dtype,\n",
        "                     tail_tokens=args.tail_tokens, gamma=args.gamma, adaptive_gamma=args.adaptive_gamma,\n",
        "                     iters=args.iters, attn_only=args.attn_only, print_every=args.print_every,\n",
        "                     score_mode=args.score)\n",
        "\n",
        "    vps  = run_phase(items, vps_on=True, model_name=args.model_name, dtype=args.dtype,\n",
        "                     tail_tokens=args.tail_tokens, gamma=args.gamma, adaptive_gamma=args.adaptive_gamma,\n",
        "                     iters=args.iters, attn_only=args.attn_only, print_every=args.print_every,\n",
        "                     score_mode=args.score)\n",
        "\n",
        "    # paired flips summary\n",
        "    b = c = 0\n",
        "    for (_, gold), pb, pv in zip(items, base[\"preds\"], vps[\"preds\"]):\n",
        "        ok_b = (pb == gold)\n",
        "        ok_v = (pv == gold)\n",
        "        if (not ok_b) and ok_v: b += 1\n",
        "        if ok_b and (not ok_v): c += 1\n",
        "    # McNemar (with continuity correction)\n",
        "    mcnemar = ((abs(b - c) - 1)**2 / max(1, (b + c))) if (b + c) > 0 else 0.0\n",
        "    print(f\"[paired] flips: baseline→VPS improvements b={b}, regressions c={c}, McNemar≈{mcnemar:.2f}\", flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "NFLUred0fQNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/arc_mc_eval.py \\\n",
        "  --n 80 \\\n",
        "  --seed 1234 \\\n",
        "  --model_name Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --dtype fp16 \\\n",
        "  --tail_tokens 32 \\\n",
        "  --gamma 0.70 \\\n",
        "  --adaptive_gamma \\\n",
        "  --iters 3 \\\n",
        "  --attn_only \\\n",
        "  --score logit \\\n",
        "  --print_every 10\n"
      ],
      "metadata": {
        "id": "Gd5TLnEDfRtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import os, sys, re, random, time, json, argparse, gc, math\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# --- Make imports work in Colab or as a script ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {\"/content\", \"/content/vps\", PROJ_PARENT, PROJ_ROOT}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator = fewer OOMs on T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Your project pieces (FULL VPS) ---\n",
        "from vps.scripts.infer_vps import build           # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate     # not used for scoring, but kept for parity\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "LETTERS = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "# ---------------- utils ----------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Baseline: keep VPS modules present but disable their effect (gamma=0).\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int,\n",
        "              gamma: float, adaptive_gamma: bool,\n",
        "              apply_to: str) -> VPSConfig:\n",
        "    \"\"\"\n",
        "    apply_to: 'full' (default) or 'attn' for attention-only.\n",
        "    \"\"\"\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    cfg.qk_coupling = True\n",
        "    if apply_to == \"attn\":\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    # else: full (no restriction)\n",
        "    return cfg\n",
        "\n",
        "# --------------- datasets ---------------\n",
        "def load_arc(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]  # ['A','B','C','D','E'] subset\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. IMPORTANT: Output **one letter only** (A/B/C/D/E) and nothing else.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "def load_csqa(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"commonsense_qa\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = [c[\"text\"] for c in ex[\"choices\"]]\n",
        "        labels  = [c[\"label\"].upper() for c in ex[\"choices\"]]  # A..E\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. IMPORTANT: Output **one letter only** (A/B/C/D/E) and nothing else.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "# --------------- log-prob MC scoring ---------------\n",
        "def _letter_token_ids(tok) -> Dict[str, List[int]]:\n",
        "    \"\"\"\n",
        "    For robustness across BPEs, allow both 'A' and ' A' (space-prefixed) etc.\n",
        "    Return a dict: letter -> [candidate_token_ids...]\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for L in LETTERS:\n",
        "        cand = set()\n",
        "        for s in (L, \" \"+L):\n",
        "            ids = tok.encode(s, add_special_tokens=False)\n",
        "            if len(ids)>=1:\n",
        "                # use only the FIRST token (position 0) as the first-step choice\n",
        "                cand.add(ids[0])\n",
        "        out[L] = sorted(cand)\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_letters_logprob(model, tok, prompt_ids, letter_id_map:Dict[str,List[int]]) -> Dict[str,float]:\n",
        "    \"\"\"\n",
        "    Returns log-prob for each letter at the *first next token* after prompt.\n",
        "    \"\"\"\n",
        "    out = model(**prompt_ids)\n",
        "    logits = out.logits[:, -1, :]           # (1, vocab)\n",
        "    logp   = F.log_softmax(logits, dim=-1)  # (1, vocab)\n",
        "    scores = {}\n",
        "    for L, ids in letter_id_map.items():\n",
        "        if len(ids)==0:\n",
        "            scores[L] = -1e9\n",
        "        elif len(ids)==1:\n",
        "            scores[L] = float(logp[0, ids[0]].item())\n",
        "        else:\n",
        "            # logsumexp over multiple tokenizations (e.g., 'A' vs ' A')\n",
        "            vals = torch.stack([logp[0, j] for j in ids])\n",
        "            scores[L] = float(torch.logsumexp(vals, dim=0).item())\n",
        "    return scores\n",
        "\n",
        "# --------------- VPS iteration (aligned CE on gold) ---------------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_letter: str,\n",
        "                     tail_tokens: int, ce_T: int = 1) -> None:\n",
        "    \"\"\"\n",
        "    Full-power VPS pass:\n",
        "      - entropy probe on tail\n",
        "      - tiny CE on the *gold letter* to populate grads (aligned / teacher-forced)\n",
        "    \"\"\"\n",
        "    # Base prompt tensors\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    # 1) entropy probe (no grad)\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # 2) CE on gold letter (teacher-forced, aligned)\n",
        "    gold_ids_full = tok(\" \"+gold_letter, add_special_tokens=False, return_tensors=None)\n",
        "    if len(gold_ids_full)==0:\n",
        "        gold_ids_full = tok(gold_letter, add_special_tokens=False, return_tensors=None)\n",
        "    gold_ids_full = torch.tensor(gold_ids_full, dtype=torch.long, device=base_inputs[\"input_ids\"].device)\n",
        "    T = max(1, min(ce_T, gold_ids_full.shape[0]))\n",
        "\n",
        "    # Input for CE must include prompt + gold[:-1], target = gold\n",
        "    aug = torch.cat([base_inputs[\"input_ids\"][0], gold_ids_full[:T-1]], dim=0) if T>1 else base_inputs[\"input_ids\"][0]\n",
        "    aug = aug.unsqueeze(0)\n",
        "    attn = torch.ones_like(aug, device=aug.device)\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    out2 = model(input_ids=aug, attention_mask=attn)\n",
        "    logits2 = out2.logits[:, -T:, :]             # predict the gold segment\n",
        "    target  = gold_ids_full[:T].unsqueeze(0)\n",
        "    ce = F.cross_entropy(logits2.reshape(-1, logits2.size(-1)), target.reshape(-1))\n",
        "    ce.backward()\n",
        "    model.config.use_cache = True\n",
        "\n",
        "# --------------- run one task ---------------\n",
        "def run_task(task:str, n:int, seed:int, vps_on:bool,\n",
        "             model_name:str, dtype:str, gamma:float, adaptive_gamma:bool,\n",
        "             tail_tokens:int, iters:int, apply_to:str, print_every:int) -> Dict:\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name, dtype, max_new_tokens=1,\n",
        "                    gamma=gamma, adaptive_gamma=adaptive_gamma, apply_to=apply_to)\n",
        "\n",
        "    phase = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase} — building model...\", flush=True)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Baseline keeps VPS modules but disables effect\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # show a gamma sample\n",
        "    any_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma; break\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    if task == \"arc_c\":\n",
        "        items = load_arc(n, seed)\n",
        "    elif task == \"csqa\":\n",
        "        items = load_csqa(n, seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown task {task}\")\n",
        "\n",
        "    letter_id_map = _letter_token_ids(tok)\n",
        "\n",
        "    preds, golds, times = [], [], []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=tail_tokens, ce_T=1)\n",
        "            # Score letters by log-prob *without* appending the gold\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        preds.append(pred); golds.append(gold)\n",
        "        times.append(dt)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == n):\n",
        "            acc_so_far = sum(int(p==g) for p,g in zip(preds,golds)) / len(preds)\n",
        "            eta = (sum(times)/len(times))*(n-i)\n",
        "            print(f\"[{task}] {i}/{n} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    acc = sum(int(p==g) for p,g in zip(preds,golds)) / max(1,len(golds))\n",
        "    lat = sum(times)/max(1,len(times))\n",
        "    res = {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"preds\": preds, \"golds\": golds}\n",
        "    print(json.dumps({k:v for k,v in res.items() if k!='preds' and k!='golds'}), flush=True)\n",
        "\n",
        "    # cleanup\n",
        "    del model, tok\n",
        "    free_cuda()\n",
        "    return res\n",
        "\n",
        "def mcnemar_from_pairs(base_preds, vps_preds, golds):\n",
        "    b = c = 0\n",
        "    for pb, pv, g in zip(base_preds, vps_preds, golds):\n",
        "        if pb != g and pv == g: b += 1       # improved\n",
        "        if pb == g and pv != g: c += 1       # regressed\n",
        "    # continuity-corrected chi^2 (McNemar)\n",
        "    if (b+c)==0:\n",
        "        stat = 0.0; p = 1.0\n",
        "    else:\n",
        "        stat = (abs(b - c) - 1.0)**2 / (b + c)\n",
        "        # approximate p-value with survival function of chi2(1)\n",
        "        # since we avoid scipy, use simple exp approximation\n",
        "        # p ~ exp(-stat/2) * 1.2533 / sqrt(stat+1e-9)  (rough)\n",
        "        p = math.exp(-stat/2.0)\n",
        "    return b, c, stat, p\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--task\", type=str, choices=[\"arc_c\", \"csqa\"], default=\"arc_c\")\n",
        "    ap.add_argument(\"--n\", type=int, default=80)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"fp32\"], default=\"fp16\")  # T4: fp16\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.65)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24)\n",
        "    ap.add_argument(\"--iters\", type=int, default=2)\n",
        "    ap.add_argument(\"--apply_to\", type=str, choices=[\"full\",\"attn\"], default=\"full\")\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    args, _ = ap.parse_known_args()\n",
        "\n",
        "    # Baseline\n",
        "    base = run_task(task=args.task, n=args.n, seed=args.seed, vps_on=False,\n",
        "                    model_name=args.model_name, dtype=args.dtype,\n",
        "                    gamma=args.gamma, adaptive_gamma=args.adaptive_gamma,\n",
        "                    tail_tokens=args.tail_tokens, iters=args.iters,\n",
        "                    apply_to=args.apply_to, print_every=args.print_every)\n",
        "\n",
        "    # VPS\n",
        "    vps = run_task(task=args.task, n=args.n, seed=args.seed, vps_on=True,\n",
        "                   model_name=args.model_name, dtype=args.dtype,\n",
        "                   gamma=args.gamma, adaptive_gamma=args.adaptive_gamma,\n",
        "                   tail_tokens=args.tail_tokens, iters=args.iters,\n",
        "                   apply_to=args.apply_to, print_every=args.print_every)\n",
        "\n",
        "    # Paired stats\n",
        "    b, c, stat, p = mcnemar_from_pairs(base[\"preds\"], vps[\"preds\"], base[\"golds\"])\n",
        "    delta = vps[\"acc\"] - base[\"acc\"]\n",
        "    summary = {\n",
        "        \"task\": args.task,\n",
        "        \"n\": args.n,\n",
        "        \"baseline_acc\": base[\"acc\"],\n",
        "        \"vps_acc\": vps[\"acc\"],\n",
        "        \"delta_acc\": delta,\n",
        "        \"improved_pairs_b\": b,\n",
        "        \"regressed_pairs_c\": c,\n",
        "        \"mcnemar_chi2\": stat,\n",
        "        \"mcnemar_p_approx\": p,\n",
        "        \"baseline_latency_mean_s\": base[\"latency_mean_s\"],\n",
        "        \"vps_latency_mean_s\": vps[\"latency_mean_s\"],\n",
        "    }\n",
        "    print(\"[paired]\", json.dumps(summary), flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9958485511394ac49b23f93ef29de011",
            "50ec2dc56be34865a73f1b49d97971d9",
            "ff3aba8266e0447594092fec814cba99",
            "2fa38928a4a94c03a81ee9ddd8421f99",
            "1db54dd256bc41eeb8015da9f3ee73a5",
            "3aa5db2a9b3f4262bc80fa1175c2a10c",
            "f58f0d21785145e8839c48f831db9cac",
            "7761b2edca18431b8ba22e699d497fd8",
            "a2fd89990ee149adab6e7d1d2a9e2acf",
            "237144c14e1944289c0522e975fcfda5",
            "facfa5a00aa9432ab1446188218e7419",
            "3c1aa244c8f847c7ab3299494dc48f7a",
            "6fd026945a564e788c1b5bfa5eb4852a",
            "bbdbd7769a8044fd8567a465f33a0dbc",
            "a89d247026c54bb289df262ac8e777dc",
            "bb01a5711e0840c997db10370c568128",
            "4ec0b5fb641e40bd82d028d2d781fb78",
            "f2d82fcf7d1246af9a805484da64319c",
            "9fcb53fb03aa43759c59aa98c41d06ee",
            "4dfe5ca2a3f6416080d77f234400b7e1",
            "aa6445a0db56406588dd1a365202e83f",
            "ac57975e458f4db9bd2dcd9194836321",
            "9458fe0e9de44dc490bf7aa2d1ed5a91",
            "e1daa0ea798c49179b3dc5541f2c1b07",
            "3ec3b3343ba9412fa6dc54bb4fbdc326",
            "2a475d1314c74cecb6c3a4f273be04fd",
            "13d94bc069db42e6aaae56fa489460f8",
            "c160ffd0a59543aaaaa43c0a50bafcb8",
            "1b67b689dde04037b85f9327910c2ab4",
            "df9a1346c2c3420d9547777cf152f1b8",
            "575757e1a73b4f1e85f3bf6a566fa98a",
            "45dddf3ffbe441a9907a2259f840017d",
            "13787f5477a54e118df4d010c18b3eb7",
            "132072a3b2724ceab32d9f1a3507a6df",
            "64f9284ec79b4aaeac4ae5378ae1e5b5",
            "aaed969bcd7849ac959af011d76cb768",
            "9929e5569f804874b8c0046060da4e44",
            "da6a1a9f59694fe8b5561670ca344666",
            "fb9b9be614b5403e84c8e779b269ced9",
            "223bfef27540441bad3026d9731d77aa",
            "be8af0a0a3dd46ba982cada303e63eb8",
            "e5dc226cfe9640dca442fbc75630b9f8",
            "ff10532b20a64c9ba723577930ba2afa",
            "80845c9dfec94611bc7ab4ccc6972a52",
            "46cb6924313a4742b0715338da60f5c8",
            "c4af9fc33ad444149935ceb66760c31b",
            "99fd4f8de5b14ef586f3bff3d31ab934",
            "585684a5417e474ca9ffc5b7abb8ff4e",
            "694e37b87a874d2b90bf40c0478baaef",
            "d590cd611ccf45859da68a3a53628d06",
            "ec4dafa0181447cb8288a37c31174fd4",
            "14221394e7ac437dbde0fb0e6a6ad53b",
            "5f50292c51e149c8a0779e4f46ff60f0",
            "dd07d11c575c483e95b20d8c19a4c2ed",
            "f10d3dadbd714e78a5056dbdd238c704",
            "46b4b41c2a7f43de98020782fedc98f7",
            "08cdcab91d394e2a85ef4812f4212e8b",
            "e3777299a62c4799b667bd25d0c89da0",
            "7a9b0714ba4f4634ba92eebb6ec039c8",
            "c3c6a6a288a040ffb8dc864c6f58cec5",
            "7886ceea38694487a08de20fcf7b56a9",
            "5bb85701dda84be98f01277abfac77b1",
            "a439d112b24b409f9e864334f866029f",
            "8c425d706bc6492b8acbb6b6d020a8cb",
            "00ad1481c6234b4ba0d4f93e5f913194",
            "43534a8522564d928a31cbaf1bc805bd",
            "68a4bf2f5e644473a931bbb34d68df56",
            "1e71a86bfa874ebfbbff6862dcf29b98",
            "8abf53d2467641ef80333ccbfa76bdbd",
            "e6c16a963b1a4d72b7c410e5584ecf59",
            "5bc8452fde3f4f2fa7ca670984ce01cd",
            "7287137ea4e141cca9aa8f1a4d6b52ca",
            "a3aa7e06f1914b8986564ee3266157a4",
            "4a756ec584ba40dba34b84d7141cfdba",
            "813132f9641d4828b740f53dd2bedfe8",
            "0f9de07e9fe9453cb13475ce847791f9",
            "d3249e5912d243a2a0d6c4e31789c16c",
            "30e538dd29254a4081c1900623ecaf0f",
            "6e60a78e2207459686633d25dbb3086a",
            "2b3cbdb83f844f6d8ccc51a5b9f9e94b",
            "ae7e22dcf1b443519149b401b9ff4776",
            "8a59b86dc77e427db3dd8eefcd1ba77f",
            "27cc132989e9420a890df678f37ce3a4",
            "cb43700a253242378eff551325802f72",
            "e1675227bea8409a866ccf36d781fba2",
            "dee92192935648adb2c49e1a8ae34599",
            "ef55dbbb074d4e938fb3b94d28c49967",
            "e225e0232d094f139641230bb8c7a155",
            "8fd3d31e08a444d584f2d4f611d799b6",
            "5ca53e2630da4332be1e92b823919a88",
            "38284b035e4045d795078bd93f5aedd8",
            "a801bf225b3c4b54ae4695a4bd62bfa5",
            "28d7ec74fab4404c9b456f17605eddd0",
            "3b7581487ecd4148b58f4c69d07705e6",
            "9f8bbc7b70a348ed9cefa49c443a7485",
            "12c88da586a64dd383534f2bcf5a4ba4",
            "6b5c559f085242359078a77e9a74147d",
            "3a5b7aa17e3e43ac882cd2e74cccc5fb",
            "9862176c4a364098973a41c5af52439c"
          ]
        },
        "id": "JISMKgvVFMfo",
        "outputId": "1f2f253c-9672-48ec-f45a-1fcf167f1b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase] BASELINE (VPS OFF) — building model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9958485511394ac49b23f93ef29de011"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.0\n",
            "[Phase] model ready.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c1aa244c8f847c7ab3299494dc48f7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ARC-Challenge/train-00000-of-00001.parqu(…):   0%|          | 0.00/190k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9458fe0e9de44dc490bf7aa2d1ed5a91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ARC-Challenge/test-00000-of-00001.parque(…):   0%|          | 0.00/204k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "132072a3b2724ceab32d9f1a3507a6df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ARC-Challenge/validation-00000-of-00001.(…):   0%|          | 0.00/55.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46cb6924313a4742b0715338da60f5c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46b4b41c2a7f43de98020782fedc98f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68a4bf2f5e644473a931bbb34d68df56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30e538dd29254a4081c1900623ecaf0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[arc_c] 5/80 acc_so_far=0.600 last_dt=0.38s ~ETA 0m49s\n",
            "[arc_c] 10/80 acc_so_far=0.600 last_dt=0.36s ~ETA 0m36s\n",
            "[arc_c] 15/80 acc_so_far=0.667 last_dt=0.39s ~ETA 0m30s\n",
            "[arc_c] 20/80 acc_so_far=0.700 last_dt=0.44s ~ETA 0m27s\n",
            "[arc_c] 25/80 acc_so_far=0.760 last_dt=0.37s ~ETA 0m24s\n",
            "[arc_c] 30/80 acc_so_far=0.767 last_dt=0.37s ~ETA 0m21s\n",
            "[arc_c] 35/80 acc_so_far=0.771 last_dt=0.39s ~ETA 0m19s\n",
            "[arc_c] 40/80 acc_so_far=0.775 last_dt=0.38s ~ETA 0m16s\n",
            "[arc_c] 45/80 acc_so_far=0.778 last_dt=0.37s ~ETA 0m14s\n",
            "[arc_c] 50/80 acc_so_far=0.780 last_dt=0.42s ~ETA 0m12s\n",
            "[arc_c] 55/80 acc_so_far=0.782 last_dt=0.39s ~ETA 0m10s\n",
            "[arc_c] 60/80 acc_so_far=0.800 last_dt=0.37s ~ETA 0m8s\n",
            "[arc_c] 65/80 acc_so_far=0.800 last_dt=0.37s ~ETA 0m6s\n",
            "[arc_c] 70/80 acc_so_far=0.814 last_dt=0.40s ~ETA 0m4s\n",
            "[arc_c] 75/80 acc_so_far=0.800 last_dt=0.39s ~ETA 0m2s\n",
            "[arc_c] 80/80 acc_so_far=0.775 last_dt=0.37s ~ETA 0m0s\n",
            "{\"task\": \"arc_c\", \"n\": 80, \"acc\": 0.775, \"latency_mean_s\": 0.402903813123703}\n",
            "\n",
            "[Phase] VPS ON — building model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fd3d31e08a444d584f2d4f611d799b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.65\n",
            "[Phase] model ready.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'str' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-927774333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-927774333.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# VPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     vps = run_task(task=args.task, n=args.n, seed=args.seed, vps_on=True,\n\u001b[0m\u001b[1;32m    328\u001b[0m                    \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                    \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptive_gamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_gamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-927774333.py\u001b[0m in \u001b[0;36mrun_task\u001b[0;34m(task, n, seed, vps_on, model_name, dtype, gamma, adaptive_gamma, tail_tokens, iters, apply_to, print_every)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvps_on\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                     \u001b[0mvps_iterate_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtail_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce_T\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0;31m# Score letters by log-prob *without* appending the gold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-927774333.py\u001b[0m in \u001b[0;36mvps_iterate_once\u001b[0;34m(model, tok, prompt, gold_letter, tail_tokens, ce_T)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_ids_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mgold_ids_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_letter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mgold_ids_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_ids_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mce_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_ids_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vps/scripts/mc_logprob_eval.py\n",
        "from __future__ import annotations\n",
        "import os, sys, re, random, time, json, argparse, gc, math\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# --- Make imports work in Colab or as a script ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {\"/content\", \"/content/vps\", PROJ_PARENT, PROJ_ROOT}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator = fewer OOMs on T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Your project pieces (FULL VPS) ---\n",
        "from vps.scripts.infer_vps import build           # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate     # not used for scoring, but kept for parity\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "LETTERS = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "# ---------------- utils ----------------\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    \"\"\"Baseline: keep VPS modules present but disable their effect (gamma=0).\"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int,\n",
        "              gamma: float, adaptive_gamma: bool,\n",
        "              apply_to: str) -> VPSConfig:\n",
        "    \"\"\"\n",
        "    apply_to: 'full' (default) or 'attn' for attention-only.\n",
        "    \"\"\"\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    cfg.qk_coupling = True\n",
        "    if apply_to == \"attn\":\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    # else: full (no restriction)\n",
        "    return cfg\n",
        "\n",
        "# Robust encode helper: always returns List[int]\n",
        "def encode_ids(tok, text: str) -> List[int]:\n",
        "    try:\n",
        "        ids = tok.encode(text, add_special_tokens=False)\n",
        "    except Exception:\n",
        "        out = tok(text, add_special_tokens=False, return_tensors=None)\n",
        "        if isinstance(out, dict):\n",
        "            ids = out.get(\"input_ids\", [])\n",
        "            if isinstance(ids, list) and len(ids) > 0 and isinstance(ids[0], list):\n",
        "                ids = ids[0]\n",
        "        else:\n",
        "            ids = out\n",
        "    if isinstance(ids, torch.Tensor):\n",
        "        ids = ids.tolist()\n",
        "    # ensure ints\n",
        "    ids = [int(x) for x in ids] if isinstance(ids, list) else []\n",
        "    return ids\n",
        "\n",
        "# --------------- datasets ---------------\n",
        "def load_arc(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]  # ['A','B','C','D','E'] subset\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. IMPORTANT: Output **one letter only** (A/B/C/D/E) and nothing else.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "def load_csqa(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"commonsense_qa\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = [c[\"text\"] for c in ex[\"choices\"]]\n",
        "        labels  = [c[\"label\"].upper() for c in ex[\"choices\"]]  # A..E\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. IMPORTANT: Output **one letter only** (A/B/C/D/E) and nothing else.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "# --------------- log-prob MC scoring ---------------\n",
        "def _letter_token_ids(tok) -> Dict[str, List[int]]:\n",
        "    \"\"\"\n",
        "    Allow both 'A' and ' A' (space-prefixed). Return letter -> [candidate_token_ids...]\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for L in LETTERS:\n",
        "        cand = set()\n",
        "        for s in (L, \" \"+L):\n",
        "            ids = encode_ids(tok, s)\n",
        "            if len(ids) >= 1:\n",
        "                cand.add(ids[0])  # next-token choice is the first subtoken\n",
        "        out[L] = sorted(cand)\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_letters_logprob(model, tok, prompt_ids, letter_id_map:Dict[str,List[int]]) -> Dict[str,float]:\n",
        "    \"\"\"\n",
        "    Returns log-prob for each letter at the *first next token* after prompt.\n",
        "    \"\"\"\n",
        "    out = model(**prompt_ids)\n",
        "    logits = out.logits[:, -1, :]           # (1, vocab)\n",
        "    logp   = F.log_softmax(logits, dim=-1)  # (1, vocab)\n",
        "    scores = {}\n",
        "    for L, ids in letter_id_map.items():\n",
        "        if len(ids)==0:\n",
        "            scores[L] = -1e9\n",
        "        elif len(ids)==1:\n",
        "            scores[L] = float(logp[0, ids[0]].item())\n",
        "        else:\n",
        "            vals = torch.stack([logp[0, j] for j in ids])\n",
        "            scores[L] = float(torch.logsumexp(vals, dim=0).item())\n",
        "    return scores\n",
        "\n",
        "# --------------- VPS iteration (aligned CE on gold) ---------------\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_letter: str,\n",
        "                     tail_tokens: int, ce_T: int = 1) -> None:\n",
        "    \"\"\"\n",
        "    Full-power VPS pass:\n",
        "      - entropy probe on tail\n",
        "      - tiny CE on the *gold letter* to populate grads (aligned / teacher-forced)\n",
        "    \"\"\"\n",
        "    gold_letter = (gold_letter or \"\").strip().upper()\n",
        "    if len(gold_letter) == 0:\n",
        "        return\n",
        "    gold_letter = gold_letter[0]  # ensure single char\n",
        "\n",
        "    # Base prompt tensors\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    # 1) entropy probe (no grad)\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # 2) CE on gold letter (teacher-forced, aligned)\n",
        "    gold_ids_full = encode_ids(tok, \" \" + gold_letter)\n",
        "    if len(gold_ids_full) == 0:\n",
        "        gold_ids_full = encode_ids(tok, gold_letter)\n",
        "    if len(gold_ids_full) == 0:\n",
        "        return  # nothing to backprop\n",
        "\n",
        "    gold_ids_full = torch.tensor(gold_ids_full, dtype=torch.long, device=base_inputs[\"input_ids\"].device)\n",
        "    T = max(1, min(ce_T, gold_ids_full.shape[0]))\n",
        "\n",
        "    # Input for CE must include prompt + gold[:-1], target = gold\n",
        "    if T > 1:\n",
        "        aug = torch.cat([base_inputs[\"input_ids\"][0], gold_ids_full[:T-1]], dim=0)\n",
        "    else:\n",
        "        aug = base_inputs[\"input_ids\"][0]\n",
        "    aug = aug.unsqueeze(0)\n",
        "    attn = torch.ones_like(aug, device=aug.device)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    out2 = model(input_ids=aug, attention_mask=attn)\n",
        "    logits2 = out2.logits[:, -T:, :]             # predict the gold segment\n",
        "    target  = gold_ids_full[:T].unsqueeze(0)\n",
        "    ce = F.cross_entropy(logits2.reshape(-1, logits2.size(-1)), target.reshape(-1))\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "# --------------- run one task ---------------\n",
        "def run_task(task:str, n:int, seed:int, vps_on:bool,\n",
        "             model_name:str, dtype:str, gamma:float, adaptive_gamma:bool,\n",
        "             tail_tokens:int, iters:int, apply_to:str, print_every:int) -> Dict:\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name, dtype, max_new_tokens=1,\n",
        "                    gamma=gamma, adaptive_gamma=adaptive_gamma, apply_to=apply_to)\n",
        "\n",
        "    phase = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase} — building model...\", flush=True)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # Baseline keeps VPS modules but disables effect\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # show a gamma sample\n",
        "    any_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma; break\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "\n",
        "    model.eval()\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    if task == \"arc_c\":\n",
        "        items = load_arc(n, seed)\n",
        "    elif task == \"csqa\":\n",
        "        items = load_csqa(n, seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown task {task}\")\n",
        "\n",
        "    letter_id_map = _letter_token_ids(tok)\n",
        "\n",
        "    preds, golds, times = [], [], []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=tail_tokens, ce_T=1)\n",
        "            # Score letters by log-prob *without* appending the gold\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        preds.append(pred); golds.append(gold)\n",
        "        times.append(dt)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == n):\n",
        "            acc_so_far = sum(int(p==g) for p,g in zip(preds,golds)) / len(preds)\n",
        "            eta = (sum(times)/len(times))*(n-i)\n",
        "            print(f\"[{task}] {i}/{n} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    acc = sum(int(p==g) for p,g in zip(preds,golds)) / max(1,len(golds))\n",
        "    lat = sum(times)/max(1,len(times))\n",
        "    res = {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"preds\": preds, \"golds\": golds}\n",
        "    print(json.dumps({k:v for k,v in res.items() if k!='preds' and k!='golds'}), flush=True)\n",
        "\n",
        "    # cleanup\n",
        "    del model, tok\n",
        "    free_cuda()\n",
        "    return res\n",
        "\n",
        "def mcnemar_from_pairs(base_preds, vps_preds, golds):\n",
        "    b = c = 0\n",
        "    for pb, pv, g in zip(base_preds, vps_preds, golds):\n",
        "        if pb != g and pv == g: b += 1       # improved\n",
        "        if pb == g and pv != g: c += 1       # regressed\n",
        "    # continuity-corrected chi^2 (McNemar)\n",
        "    if (b+c)==0:\n",
        "        stat = 0.0; p = 1.0\n",
        "    else:\n",
        "        stat = (abs(b - c) - 1.0)**2 / (b + c)\n",
        "        p = math.exp(-stat/2.0)  # rough approx without scipy\n",
        "    return b, c, stat, p\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--task\", type=str, choices=[\"arc_c\", \"csqa\"], default=\"arc_c\")\n",
        "    ap.add_argument(\"--n\", type=int, default=80)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"fp32\"], default=\"fp16\")  # T4: fp16\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.65)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24)\n",
        "    ap.add_argument(\"--iters\", type=int, default=2)\n",
        "    ap.add_argument(\"--apply_to\", type=str, choices=[\"full\",\"attn\"], default=\"full\")\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    args, _ = ap.parse_known_args()\n",
        "\n",
        "    # Baseline\n",
        "    base = run_task(task=args.task, n=args.n, seed=args.seed, vps_on=False,\n",
        "                    model_name=args.model_name, dtype=args.dtype,\n",
        "                    gamma=args.gamma, adaptive_gamma=args.adaptive_gamma,\n",
        "                    tail_tokens=args.tail_tokens, iters=args.iters,\n",
        "                    apply_to=args.apply_to, print_every=args.print_every)\n",
        "\n",
        "    # VPS\n",
        "    vps = run_task(task=args.task, n=args.n, seed=args.seed, vps_on=True,\n",
        "                   model_name=args.model_name, dtype=args.dtype,\n",
        "                   gamma=args.gamma, adaptive_gamma=args.adaptive_gamma,\n",
        "                   tail_tokens=args.tail_tokens, iters=args.iters,\n",
        "                   apply_to=args.apply_to, print_every=args.print_every)\n",
        "\n",
        "    # Paired stats\n",
        "    b, c, stat, p = mcnemar_from_pairs(base[\"preds\"], vps[\"preds\"], base[\"golds\"])\n",
        "    delta = vps[\"acc\"] - base[\"acc\"]\n",
        "    summary = {\n",
        "        \"task\": args.task,\n",
        "        \"n\": args.n,\n",
        "        \"baseline_acc\": base[\"acc\"],\n",
        "        \"vps_acc\": vps[\"acc\"],\n",
        "        \"delta_acc\": delta,\n",
        "        \"improved_pairs_b\": b,\n",
        "        \"regressed_pairs_c\": c,\n",
        "        \"mcnemar_chi2\": stat,\n",
        "        \"mcnemar_p_approx\": p,\n",
        "        \"baseline_latency_mean_s\": base[\"latency_mean_s\"],\n",
        "        \"vps_latency_mean_s\": vps[\"latency_mean_s\"],\n",
        "    }\n",
        "    print(\"[paired]\", json.dumps(summary), flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527,
          "referenced_widgets": [
            "95e69943c4d44a5f90736b8179caa973",
            "1b00f2c1b90b4d1284c9bb0308264d2f",
            "bad1b55ada9247e0b2a7312cfe7544ac",
            "8f60ab63681d4b52a229666edf39b4a7",
            "bf616ac047de459fbfefd89acc381a52",
            "4c8bcf6ea5f54adb924a413036cfec44",
            "fd0d74cf235747278444302fc74e372d",
            "5a08bb30b99a4b788f8a632213b57334",
            "b4f138ce6b3e418c88c6db93e37d07cb",
            "7acabd39a7bd4e7cb46940295c8ffb5d",
            "ed58a46b465d4aaa9abb8f7a7ab6f44c"
          ]
        },
        "id": "wnby9dZEGZDL",
        "outputId": "915755c3-1f8f-4f4d-8498-5e9355e834e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase] BASELINE (VPS OFF) — building model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95e69943c4d44a5f90736b8179caa973"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.0\n",
            "[Phase] model ready.\n",
            "[arc_c] 5/80 acc_so_far=0.600 last_dt=0.38s ~ETA 0m29s\n",
            "[arc_c] 10/80 acc_so_far=0.600 last_dt=0.37s ~ETA 0m27s\n",
            "[arc_c] 15/80 acc_so_far=0.667 last_dt=0.37s ~ETA 0m25s\n",
            "[arc_c] 20/80 acc_so_far=0.700 last_dt=0.45s ~ETA 0m24s\n",
            "[arc_c] 25/80 acc_so_far=0.760 last_dt=0.37s ~ETA 0m21s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3952937952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3952937952.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     base = run_task(task=args.task, n=args.n, seed=args.seed, vps_on=False,\n\u001b[0m\u001b[1;32m    346\u001b[0m                     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptive_gamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_gamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3952937952.py\u001b[0m in \u001b[0;36mrun_task\u001b[0;34m(task, n, seed, vps_on, model_name, dtype, gamma, adaptive_gamma, tail_tokens, iters, apply_to, print_every)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;31m# Score letters by log-prob *without* appending the gold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_letters_logprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletter_id_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfMemoryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3952937952.py\u001b[0m in \u001b[0;36mscore_letters_logprob\u001b[0;34m(model, tok, prompt_ids, letter_id_map)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mletter\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfirst\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \"\"\"\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mprompt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# (1, vocab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mlogp\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, vocab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 449\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1880\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1825\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1827\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1828\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m                 for hook_id, hook in (\n",
            "\u001b[0;32m/content/vps/vpscore/vps_linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbfgs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "# ARC-Challenge, n=80\n",
        "python -u vps/scripts/mc_logprob_eval.py \\\n",
        "  --task arc_c --n 80 --seed 1234 \\\n",
        "  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\n",
        "  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\n",
        "  --apply_to full --print_every 5\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "A-4qmFxxHL5L",
        "outputId": "59d6fddd-ba24-481c-fe8a-6a6af59ee8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "python3: can't open file '/content/vps/scripts/mc_logprob_eval.py': [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b'set -e\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\n# ARC-Challenge, n=80\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n\\n'' returned non-zero exit status 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2764397231.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set -e\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\n# ARC-Challenge, n=80\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'set -e\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\n# ARC-Challenge, n=80\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n\\n'' returned non-zero exit status 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "\n",
        "# --- write the evaluator (full, fixed) ---\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/mc_logprob_eval.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, re, random, time, json, argparse, gc, math\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# --- Make imports work in Colab or as a script ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {\"/content\", \"/content/vps\", PROJ_PARENT, PROJ_ROOT}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# CUDA allocator = fewer OOMs on T4\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Your project pieces (FULL VPS) ---\n",
        "from vps.scripts.infer_vps import build           # returns (tok, model, hooks)\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.utils.generation import generate     # kept for parity\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "LETTERS = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def set_vps_enabled(model, enabled: bool):\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def build_cfg(model_name: str, dtype: str, max_new_tokens: int,\n",
        "              gamma: float, adaptive_gamma: bool,\n",
        "              apply_to: str) -> VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.max_new_tokens = max_new_tokens\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    cfg.qk_coupling = True\n",
        "    if apply_to == \"attn\":\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    return cfg\n",
        "\n",
        "def encode_ids(tok, text: str) -> List[int]:\n",
        "    try:\n",
        "        ids = tok.encode(text, add_special_tokens=False)\n",
        "    except Exception:\n",
        "        out = tok(text, add_special_tokens=False, return_tensors=None)\n",
        "        if isinstance(out, dict):\n",
        "            ids = out.get(\"input_ids\", [])\n",
        "            if isinstance(ids, list) and len(ids) > 0 and isinstance(ids[0], list):\n",
        "                ids = ids[0]\n",
        "        else:\n",
        "            ids = out\n",
        "    if isinstance(ids, torch.Tensor):\n",
        "        ids = ids.tolist()\n",
        "    ids = [int(x) for x in ids] if isinstance(ids, list) else []\n",
        "    return ids\n",
        "\n",
        "def load_arc(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. IMPORTANT: Output **one letter only** (A/B/C/D/E) and nothing else.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "def load_csqa(n:int, seed:int) -> List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"commonsense_qa\", split=\"validation\")\n",
        "    idx = list(range(len(ds)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    items = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = [c[\"text\"] for c in ex[\"choices\"]]\n",
        "        labels  = [c[\"label\"].upper() for c in ex[\"choices\"]]\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. IMPORTANT: Output **one letter only** (A/B/C/D/E) and nothing else.\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        items.append((prompt, gold))\n",
        "    return items\n",
        "\n",
        "def _letter_token_ids(tok) -> Dict[str, List[int]]:\n",
        "    out = {}\n",
        "    for L in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
        "        cand = set()\n",
        "        for s in (L, \" \"+L):\n",
        "            ids = encode_ids(tok, s)\n",
        "            if len(ids) >= 1:\n",
        "                cand.add(ids[0])\n",
        "        out[L] = sorted(cand)\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_letters_logprob(model, tok, prompt_ids, letter_id_map:Dict[str,List[int]]) -> Dict[str,float]:\n",
        "    out = model(**prompt_ids)\n",
        "    logits = out.logits[:, -1, :]\n",
        "    logp   = F.log_softmax(logits, dim=-1)\n",
        "    scores = {}\n",
        "    for L, ids in letter_id_map.items():\n",
        "        if len(ids)==0:\n",
        "            scores[L] = -1e9\n",
        "        elif len(ids)==1:\n",
        "            scores[L] = float(logp[0, ids[0]].item())\n",
        "        else:\n",
        "            vals = torch.stack([logp[0, j] for j in ids])\n",
        "            scores[L] = float(torch.logsumexp(vals, dim=0).item())\n",
        "    return scores\n",
        "\n",
        "def vps_iterate_once(model, tok, prompt: str, gold_letter: str,\n",
        "                     tail_tokens: int, ce_T: int = 1) -> None:\n",
        "    gold_letter = (gold_letter or \"\").strip().upper()\n",
        "    if len(gold_letter) == 0:\n",
        "        return\n",
        "    gold_letter = gold_letter[0]\n",
        "\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**tail_inputs)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try:\n",
        "                m.policy.set_token_entropy(float(ent))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    gold_ids_full = encode_ids(tok, \" \" + gold_letter)\n",
        "    if len(gold_ids_full) == 0:\n",
        "        gold_ids_full = encode_ids(tok, gold_letter)\n",
        "    if len(gold_ids_full) == 0:\n",
        "        return\n",
        "\n",
        "    gold_ids_full = torch.tensor(gold_ids_full, dtype=torch.long, device=base_inputs[\"input_ids\"].device)\n",
        "    T = max(1, min(ce_T, gold_ids_full.shape[0]))\n",
        "\n",
        "    if T > 1:\n",
        "        aug = torch.cat([base_inputs[\"input_ids\"][0], gold_ids_full[:T-1]], dim=0)\n",
        "    else:\n",
        "        aug = base_inputs[\"input_ids\"][0]\n",
        "    aug = aug.unsqueeze(0)\n",
        "    attn = torch.ones_like(aug, device=aug.device)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    out2 = model(input_ids=aug, attention_mask=attn)\n",
        "    logits2 = out2.logits[:, -T:, :]\n",
        "    target  = gold_ids_full[:T].unsqueeze(0)\n",
        "    ce = F.cross_entropy(logits2.reshape(-1, logits2.size(-1)), target.reshape(-1))\n",
        "    ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "def run_task(task:str, n:int, seed:int, vps_on:bool,\n",
        "             model_name:str, dtype:str, gamma:float, adaptive_gamma:bool,\n",
        "             tail_tokens:int, iters:int, apply_to:str, print_every:int) -> Dict:\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name, dtype, max_new_tokens=1,\n",
        "                    gamma=gamma, adaptive_gamma=adaptive_gamma, apply_to=apply_to)\n",
        "\n",
        "    phase = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase} — building model...\", flush=True)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "    try:\n",
        "        if vps_on:\n",
        "            model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    any_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma; break\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "    print(\"[Phase] model ready.\", flush=True)\n",
        "\n",
        "    if task == \"arc_c\":\n",
        "        items = load_arc(n, seed)\n",
        "    elif task == \"csqa\":\n",
        "        items = load_csqa(n, seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown task {task}\")\n",
        "\n",
        "    letter_id_map = _letter_token_ids(tok)\n",
        "\n",
        "    preds, golds, times = [], [], []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=tail_tokens, ce_T=1)\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        preds.append(pred); golds.append(gold); times.append(dt)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == n):\n",
        "            acc_so_far = sum(int(p==g) for p,g in zip(preds,golds)) / len(preds)\n",
        "            eta = (sum(times)/len(times))*(n-i)\n",
        "            print(f\"[{task}] {i}/{n} acc_so_far={acc_so_far:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    acc = sum(int(p==g) for p,g in zip(preds,golds)) / max(1,len(golds))\n",
        "    lat = sum(times)/max(1,len(times))\n",
        "    res = {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat, \"preds\": preds, \"golds\": golds}\n",
        "    print(json.dumps({k:v for k,v in res.items() if k not in ('preds','golds')}), flush=True)\n",
        "\n",
        "    del model, tok\n",
        "    free_cuda()\n",
        "    return res\n",
        "\n",
        "def mcnemar_from_pairs(base_preds, vps_preds, golds):\n",
        "    b = c = 0\n",
        "    for pb, pv, g in zip(base_preds, vps_preds, golds):\n",
        "        if pb != g and pv == g: b += 1\n",
        "        if pb == g and pv != g: c += 1\n",
        "    if (b+c)==0:\n",
        "        stat = 0.0; p = 1.0\n",
        "    else:\n",
        "        stat = (abs(b - c) - 1.0)**2 / (b + c)\n",
        "        p = math.exp(-stat/2.0)\n",
        "    return b, c, stat, p\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--task\", type=str, choices=[\"arc_c\", \"csqa\"], default=\"arc_c\")\n",
        "    ap.add_argument(\"--n\", type=int, default=80)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.65)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24)\n",
        "    ap.add_argument(\"--iters\", type=int, default=2)\n",
        "    ap.add_argument(\"--apply_to\", type=str, choices=[\"full\",\"attn\"], default=\"full\")\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    args, _ = ap.parse_known_args()\n",
        "\n",
        "    base = run_task(args.task, args.n, args.seed, False,\n",
        "                    args.model_name, args.dtype, args.gamma, args.adaptive_gamma,\n",
        "                    args.tail_tokens, args.iters, args.apply_to, args.print_every)\n",
        "    vps  = run_task(args.task, args.n, args.seed, True,\n",
        "                    args.model_name, args.dtype, args.gamma, args.adaptive_gamma,\n",
        "                    args.tail_tokens, args.iters, args.apply_to, args.print_every)\n",
        "    b, c, stat, p = mcnemar_from_pairs(base[\"preds\"], vps[\"preds\"], base[\"golds\"])\n",
        "    summary = {\n",
        "        \"task\": args.task,\n",
        "        \"n\": args.n,\n",
        "        \"baseline_acc\": base[\"acc\"],\n",
        "        \"vps_acc\": vps[\"acc\"],\n",
        "        \"delta_acc\": vps[\"acc\"] - base[\"acc\"],\n",
        "        \"improved_pairs_b\": b,\n",
        "        \"regressed_pairs_c\": c,\n",
        "        \"mcnemar_chi2\": stat,\n",
        "        \"mcnemar_p_approx\": p,\n",
        "        \"baseline_latency_mean_s\": base[\"latency_mean_s\"],\n",
        "        \"vps_latency_mean_s\": vps[\"latency_mean_s\"],\n",
        "    }\n",
        "    print(\"[paired]\", json.dumps(summary), flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY"
      ],
      "metadata": {
        "id": "cXyBiFpfICzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/mc_logprob_eval.py \\\n",
        "  --task arc_c --n 80 --seed 1234 \\\n",
        "  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\n",
        "  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\n",
        "  --apply_to full --print_every 5\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ho1s3piiIwlC",
        "outputId": "46626568-e030-4107-85bc-abef978687a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase] BASELINE (VPS OFF) — building model...\n",
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.0\n",
            "[Phase] model ready.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757365444.191697    9345 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757365444.196898    9345 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757365444.210175    9345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757365444.210194    9345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757365444.210197    9345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757365444.210199    9345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:17<00:17, 17.03s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 12.59s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.26s/it]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 255, in run_task\n",
            "    scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 153, in score_letters_logprob\n",
            "    out = model(**prompt_ids)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 175, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 940, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 449, in forward\n",
            "    outputs: BaseModelOutputWithPast = self.model(\n",
            "                                       ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 1064, in wrapper\n",
            "    outputs = func(self, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 384, in forward\n",
            "    hidden_states = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
            "    return super().__call__(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 175, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 249, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 175, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 46, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
            "    return inner()\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
            "    result = forward_call(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/vps/vpscore/vps_linear.py\", line 114, in forward\n",
            "    base_out = self.base(x)\n",
            "               ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 360, in pre_forward\n",
            "    set_module_tensor_to_device(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py\", line 343, in set_module_tensor_to_device\n",
            "    new_value = value.to(device, non_blocking=non_blocking)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 28.12 MiB is free. Process 8516 has 12.31 GiB memory in use. Process 99929 has 2.40 GiB memory in use. Of the allocated memory 2.20 GiB is allocated by PyTorch, and 78.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 330, in <module>\n",
            "    main()\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 307, in main\n",
            "    base = run_task(args.task, args.n, args.seed, False,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 260, in run_task\n",
            "    scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 153, in score_letters_logprob\n",
            "    out = model(**prompt_ids)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 175, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 940, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 449, in forward\n",
            "    outputs: BaseModelOutputWithPast = self.model(\n",
            "                                       ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 1064, in wrapper\n",
            "    outputs = func(self, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 384, in forward\n",
            "    hidden_states = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
            "    return super().__call__(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 175, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 249, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 175, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 46, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "                                           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
            "    return inner()\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
            "    result = forward_call(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/vps/vpscore/vps_linear.py\", line 114, in forward\n",
            "    base_out = self.base(x)\n",
            "               ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\", line 360, in pre_forward\n",
            "    set_module_tensor_to_device(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py\", line 343, in set_module_tensor_to_device\n",
            "    new_value = value.to(device, non_blocking=non_blocking)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 8516 has 12.31 GiB memory in use. Process 99929 has 2.40 GiB memory in use. Of the allocated memory 2.20 GiB is allocated by PyTorch, and 77.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b'set -e\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n\\n'' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1149181436.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set -e\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'set -e\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n\\n'' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "# Free up ~2.4 GiB that JAX often grabs on Colab/T4:\n",
        "export JAX_PLATFORMS=cpu\n",
        "export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "export XLA_PYTHON_CLIENT_MEM_FRACTION=.0\n",
        "\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "# ARC-Challenge, n=80, full VPS\n",
        "python -u vps/scripts/mc_logprob_eval.py \\\n",
        "  --task arc_c --n 80 --seed 1234 \\\n",
        "  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\n",
        "  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\n",
        "  --apply_to full --print_every 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwjO3R2YKert",
        "outputId": "29ad0383-7fc5-4638-d891-5ef5ae7f8928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/mc_logprob_eval.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, re, json, time, gc, argparse, random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# --- robust imports (Colab / script) ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {\"/content\", \"/content/vps\", PROJ_PARENT, PROJ_ROOT}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from vps.scripts.infer_vps import build\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "LETTER_SET = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def build_cfg(model_name:str, dtype:str, apply_to:str, gamma:float, adaptive_gamma:bool)->VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"     # accelerate handles T4\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.max_new_tokens = 1\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    if apply_to == \"attn\":\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    else:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"]\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "def set_vps_enabled(model, enabled:bool):\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def robust_letter_id(tok, letter:str)->int:\n",
        "    # try single-token encodings first\n",
        "    cands = [\n",
        "        letter,                 # \"A\"\n",
        "        \" \" + letter,           # \" A\"\n",
        "        \"\\n\" + letter,          # \"\\nA\"\n",
        "    ]\n",
        "    for s in cands:\n",
        "        ids = tok(s, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(ids) == 1:\n",
        "            return ids[0]\n",
        "    # fallback: take the last token id (still works for next-token logit)\n",
        "    ids = tok(\" \" + letter, add_special_tokens=False)[\"input_ids\"]\n",
        "    return ids[-1]\n",
        "\n",
        "def load_arc(n:int, seed:int)->List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds))); random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    out = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]\n",
        "        labels  = ex[\"choices\"][\"label\"]\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. Output **one letter only** (A/B/C/D/E).\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out.append((prompt, gold))\n",
        "    return out\n",
        "\n",
        "def load_csqa(n:int, seed:int)->List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
        "    idx = list(range(len(ds))); random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    out = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = [c[\"text\"] for c in ex[\"choices\"]]\n",
        "        labels  = [c[\"label\"] for c in ex[\"choices\"]]\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. Output **one letter only** (A/B/C/D/E).\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out.append((prompt, gold))\n",
        "    return out\n",
        "\n",
        "@torch.inference_mode()\n",
        "def score_letters_logprob(model, tok, prompt_ids:Dict[str,torch.Tensor], letter_id_map:Dict[str,int]) -> Dict[str,float]:\n",
        "    # ensure no KV cache to save memory\n",
        "    model.config.use_cache = False\n",
        "    out = model(**prompt_ids, use_cache=False)\n",
        "    logits = out.logits[0, -1, :]  # next-token distribution\n",
        "    scores = {L: float(logits[letter_id_map[L]].item()) for L in LETTER_SET}\n",
        "    return scores\n",
        "\n",
        "def vps_iterate_once(model, tok, prompt:str, gold_letter:str, tail_tokens:int, ce_T:int=6):\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "    # entropy probe (no grad)\n",
        "    with torch.inference_mode():\n",
        "        out = model(**tail_inputs, use_cache=False)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try: m.policy.set_token_entropy(float(ent))\n",
        "            except: pass\n",
        "    # tiny CE on the letter token(s)\n",
        "    gold_ids_full = tok(gold_letter, add_special_tokens=False, return_tensors=None)\n",
        "    gold_ids_full = torch.tensor(gold_ids_full, dtype=torch.long, device=base_inputs[\"input_ids\"].device)\n",
        "    T = max(1, min(ce_T, gold_ids_full.shape[0]))\n",
        "    targets = gold_ids_full[-T:].unsqueeze(0)\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    with torch.enable_grad():\n",
        "        out2 = model(**tail_inputs, use_cache=False)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(logits2.reshape(-1, logits2.size(-1)),\n",
        "                                               targets.reshape(-1))\n",
        "        ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "def run_task(task:str, n:int, seed:int, vps_on:bool,\n",
        "             model_name:str, dtype:str, gamma:float, adaptive_gamma:bool,\n",
        "             tail_tokens:int, iters:int, apply_to:str, print_every:int):\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name, dtype, apply_to, gamma, adaptive_gamma)\n",
        "\n",
        "    phase = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase} — building model...\", flush=True)\n",
        "    tok, model, hooks = build(cfg)\n",
        "\n",
        "    # enable/disable VPS effect\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    sample_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            sample_gamma = m.cfg.gamma; break\n",
        "    print(\"[VPS] Wrapped layers:\", sum(1 for _ in model.modules() if _.__class__.__name__==\"VPSLinear\"))\n",
        "    print(f\"[diag] sample gamma = {sample_gamma}\", flush=True)\n",
        "\n",
        "    model.eval()\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    # data\n",
        "    if task == \"arc_c\":\n",
        "        items = load_arc(n, seed)\n",
        "    elif task == \"csqa\":\n",
        "        items = load_csqa(n, seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "    # letter ids once\n",
        "    letter_id_map = {L: robust_letter_id(tok, L) for L in LETTER_SET}\n",
        "\n",
        "    correct, times = 0, []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=tail_tokens, ce_T=6)\n",
        "\n",
        "            inputs = tok(prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
        "            # no grad scoring\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            # fall back: shrink tail and try once more (or bypass VPS if already in baseline)\n",
        "            if vps_on and tail_tokens > 8:\n",
        "                try:\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=8, ce_T=4)\n",
        "                    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                    scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "                    pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "                except Exception:\n",
        "                    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                    scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "                    pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "            else:\n",
        "                inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "                pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "        correct += int(pred == gold)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == n):\n",
        "            eta = (sum(times)/len(times))*(n-i)\n",
        "            print(f\"[{task}] {i}/{n} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    acc = correct / n if n else 0.0\n",
        "    lat = sum(times)/max(1,len(times))\n",
        "    res = {\"task\": task, \"n\": n, \"acc\": acc, \"latency_mean_s\": lat}\n",
        "    print(json.dumps(res), flush=True)\n",
        "\n",
        "    del model, tok\n",
        "    free_cuda()\n",
        "    return res\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--task\", type=str, choices=[\"arc_c\",\"csqa\"], default=\"arc_c\")\n",
        "    ap.add_argument(\"--n\", type=int, default=80)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.65)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24)\n",
        "    ap.add_argument(\"--iters\", type=int, default=2)\n",
        "    ap.add_argument(\"--apply_to\", type=str, choices=[\"full\",\"attn\"], default=\"full\")\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    args, _ = ap.parse_known_args()\n",
        "\n",
        "    # BASELINE\n",
        "    base = run_task(args.task, args.n, args.seed, False,\n",
        "                    args.model_name, args.dtype, args.gamma, args.adaptive_gamma,\n",
        "                    args.tail_tokens, args.iters, args.apply_to, args.print_every)\n",
        "    # VPS\n",
        "    vps  = run_task(args.task, args.n, args.seed, True,\n",
        "                    args.model_name, args.dtype, args.gamma, args.adaptive_gamma,\n",
        "                    args.tail_tokens, args.iters, args.apply_to, args.print_every)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "Y22Phv4aKicC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "# Free up ~2.4 GiB that JAX often grabs on Colab/T4:\n",
        "export JAX_PLATFORMS=cpu\n",
        "export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "export XLA_PYTHON_CLIENT_MEM_FRACTION=.0\n",
        "\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "# ARC-Challenge, n=80, full VPS\n",
        "python -u vps/scripts/mc_logprob_eval.py \\\n",
        "  --task arc_c --n 80 --seed 1234 \\\n",
        "  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\n",
        "  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\n",
        "  --apply_to full --print_every 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3EzYGPVdKj_o",
        "outputId": "25ee45b3-ba9c-4dcb-b9ff-12589349b149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase] BASELINE (VPS OFF) — building model...\n",
            "[VPS] Wrapped layers: 252\n",
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.0\n",
            "[arc_c] 5/80 acc_so_far=0.600 last_dt=0.36s ~ETA 0m39s\n",
            "[arc_c] 10/80 acc_so_far=0.600 last_dt=0.35s ~ETA 0m30s\n",
            "[arc_c] 15/80 acc_so_far=0.600 last_dt=0.35s ~ETA 0m26s\n",
            "[arc_c] 20/80 acc_so_far=0.650 last_dt=0.34s ~ETA 0m23s\n",
            "[arc_c] 25/80 acc_so_far=0.720 last_dt=0.40s ~ETA 0m21s\n",
            "[arc_c] 30/80 acc_so_far=0.733 last_dt=0.34s ~ETA 0m19s\n",
            "[arc_c] 35/80 acc_so_far=0.743 last_dt=0.37s ~ETA 0m17s\n",
            "[arc_c] 40/80 acc_so_far=0.750 last_dt=0.36s ~ETA 0m15s\n",
            "[arc_c] 45/80 acc_so_far=0.756 last_dt=0.35s ~ETA 0m13s\n",
            "[arc_c] 50/80 acc_so_far=0.760 last_dt=0.35s ~ETA 0m11s\n",
            "[arc_c] 55/80 acc_so_far=0.764 last_dt=0.35s ~ETA 0m9s\n",
            "[arc_c] 60/80 acc_so_far=0.783 last_dt=0.47s ~ETA 0m7s\n",
            "[arc_c] 65/80 acc_so_far=0.785 last_dt=0.35s ~ETA 0m5s\n",
            "[arc_c] 70/80 acc_so_far=0.800 last_dt=0.35s ~ETA 0m3s\n",
            "[arc_c] 75/80 acc_so_far=0.787 last_dt=0.37s ~ETA 0m1s\n",
            "[arc_c] 80/80 acc_so_far=0.762 last_dt=0.34s ~ETA 0m0s\n",
            "{\"task\": \"arc_c\", \"n\": 80, \"acc\": 0.7625, \"latency_mean_s\": 0.3727484971284866}\n",
            "\n",
            "[Phase] VPS ON — building model...\n",
            "[VPS] Wrapped layers: 252\n",
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757366233.077714   12626 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757366233.106226   12626 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757366233.156884   12626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757366233.156968   12626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757366233.156984   12626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757366233.156996   12626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:17<00:17, 17.95s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 12.73s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.51s/it]\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.52s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.06s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.73s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 275, in <module>\n",
            "    main()\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 270, in main\n",
            "    vps  = run_task(args.task, args.n, args.seed, True,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 209, in run_task\n",
            "    vps_iterate_once(model, tok, prompt, gold, tail_tokens=tail_tokens, ce_T=6)\n",
            "  File \"/content/vps/scripts/mc_logprob_eval.py\", line 155, in vps_iterate_once\n",
            "    gold_ids_full = torch.tensor(gold_ids_full, dtype=torch.long, device=base_inputs[\"input_ids\"].device)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: 'str' object cannot be interpreted as an integer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b'set -e\\n# Free up ~2.4 GiB that JAX often grabs on Colab/T4:\\nexport JAX_PLATFORMS=cpu\\nexport XLA_PYTHON_CLIENT_PREALLOCATE=false\\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=.0\\n\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\n# ARC-Challenge, n=80, full VPS\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n'' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1060761337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set -e\\n# Free up ~2.4 GiB that JAX often grabs on Colab/T4:\\nexport JAX_PLATFORMS=cpu\\nexport XLA_PYTHON_CLIENT_PREALLOCATE=false\\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=.0\\n\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\n# ARC-Challenge, n=80, full VPS\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'set -e\\n# Free up ~2.4 GiB that JAX often grabs on Colab/T4:\\nexport JAX_PLATFORMS=cpu\\nexport XLA_PYTHON_CLIENT_PREALLOCATE=false\\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=.0\\n\\nexport PYTHONPATH=/content:/content/vps:$PYTHONPATH\\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\\nexport PYTHONUNBUFFERED=1\\n\\n# ARC-Challenge, n=80, full VPS\\npython -u vps/scripts/mc_logprob_eval.py \\\\\\n  --task arc_c --n 80 --seed 1234 \\\\\\n  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\\\\n  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\\\\n  --apply_to full --print_every 5\\n'' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/mc_logprob_eval.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, re, json, time, gc, argparse, random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# --- robust imports ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {\"/content\", \"/content/vps\", PROJ_PARENT, PROJ_ROOT}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "# calm logs + allocator\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from vps.scripts.infer_vps import build\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "LETTER_SET = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def build_cfg(model_name:str, dtype:str, apply_to:str, gamma:float, adaptive_gamma:bool)->VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.max_new_tokens = 1\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive_gamma)\n",
        "    # keep VRAM under control on T4\n",
        "    cfg.max_memory = {\"cuda:0\": \"10GiB\", \"cpu\": \"64GiB\"}\n",
        "    if apply_to == \"attn\":\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    else:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"]\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "def set_vps_enabled(model, enabled:bool):\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def slice_tail(inputs:Dict[str,torch.Tensor], tail:int)->Dict[str,torch.Tensor]:\n",
        "    if tail <= 0: return inputs\n",
        "    L = inputs[\"input_ids\"].shape[1]\n",
        "    if L <= tail: return inputs\n",
        "    sl = slice(L - tail, L)\n",
        "    out = {}\n",
        "    for k,v in inputs.items():\n",
        "        if isinstance(v, torch.Tensor) and v.dim()==2 and v.size(1)==L:\n",
        "            out[k] = v[:, sl]\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def robust_letter_id(tok, letter:str)->int:\n",
        "    \"\"\"Return a single token id for this letter, trying common variants.\"\"\"\n",
        "    for s in (letter, \" \"+letter, \"\\n\"+letter):\n",
        "        ids = tok(s, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(ids)==1: return ids[0]\n",
        "    ids = tok(\" \"+letter, add_special_tokens=False)[\"input_ids\"]\n",
        "    return ids[-1]\n",
        "\n",
        "def robust_letter_ids(tok, letter:str)->List[int]:\n",
        "    \"\"\"Return a non-empty list of ids for the letter (never a string).\"\"\"\n",
        "    for s in (letter, \" \"+letter, \"\\n\"+letter):\n",
        "        ids = tok(s, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(ids) >= 1: return ids\n",
        "    # last resort: wrap single id\n",
        "    return [robust_letter_id(tok, letter)]\n",
        "\n",
        "def load_arc(n:int, seed:int)->List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "    idx = list(range(len(ds))); random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    out = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = ex[\"choices\"][\"text\"]; labels = ex[\"choices\"][\"label\"]\n",
        "        gold = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. Output **one letter only** (A/B/C/D/E).\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out.append((prompt, gold))\n",
        "    return out\n",
        "\n",
        "def load_csqa(n:int, seed:int)->List[Tuple[str,str]]:\n",
        "    ds = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
        "    idx = list(range(len(ds))); random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n]\n",
        "    out = []\n",
        "    for i in idx:\n",
        "        ex = ds[i]\n",
        "        q = ex[\"question\"]\n",
        "        choices = [c[\"text\"] for c in ex[\"choices\"]]\n",
        "        labels  = [c[\"label\"] for c in ex[\"choices\"]]\n",
        "        gold    = ex[\"answerKey\"].strip().upper()\n",
        "        opts = \"\\n\".join([f\"{l}) {t}\" for l,t in zip(labels, choices)])\n",
        "        prompt = (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. Output **one letter only** (A/B/C/D/E).\\n\\n\"\n",
        "            f\"Question: {q}\\n\\nOptions:\\n{opts}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out.append((prompt, gold))\n",
        "    return out\n",
        "\n",
        "@torch.inference_mode()\n",
        "def score_letters_logprob(model, tok, prompt_ids:Dict[str,torch.Tensor], letter_id_map:Dict[str,int]) -> Dict[str,float]:\n",
        "    model.config.use_cache = False\n",
        "    out = model(**prompt_ids, use_cache=False)\n",
        "    logits = out.logits[0, -1, :]\n",
        "    return {L: float(logits[letter_id_map[L]].item()) for L in LETTER_SET}\n",
        "\n",
        "def vps_iterate_once(model, tok, prompt:str, gold_letter:str, tail_tokens:int, ce_T:int=6):\n",
        "    base_inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    tail_inputs = slice_tail(base_inputs, tail=tail_tokens)\n",
        "\n",
        "    # entropy probe\n",
        "    with torch.inference_mode():\n",
        "        out = model(**tail_inputs, use_cache=False)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try: m.policy.set_token_entropy(float(ent))\n",
        "            except: pass\n",
        "\n",
        "    # CE on gold letter ids (always ints)\n",
        "    gold_ids = robust_letter_ids(tok, gold_letter)            # <- FIXED: list[int], never str\n",
        "    gold_ids = torch.tensor(gold_ids, dtype=torch.long, device=base_inputs[\"input_ids\"].device)\n",
        "    T = max(1, min(ce_T, gold_ids.shape[0]))\n",
        "    targets = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    with torch.enable_grad():\n",
        "        out2 = model(**tail_inputs, use_cache=False)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            targets.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "def run_task(task:str, n:int, seed:int, vps_on:bool,\n",
        "             model_name:str, dtype:str, gamma:float, adaptive_gamma:bool,\n",
        "             tail_tokens:int, iters:int, apply_to:str, print_every:int):\n",
        "    set_seed(seed)\n",
        "    cfg = build_cfg(model_name, dtype, apply_to, gamma, adaptive_gamma)\n",
        "\n",
        "    phase = \"VPS ON\" if vps_on else \"BASELINE (VPS OFF)\"\n",
        "    print(f\"\\n[Phase] {phase} — building model...\", flush=True)\n",
        "    tok, model, hooks = build(cfg)\n",
        "    set_vps_enabled(model, enabled=vps_on)\n",
        "\n",
        "    wrapped = sum(1 for _ in model.modules() if _.__class__.__name__==\"VPSLinear\")\n",
        "    any_gamma = None\n",
        "    for _, m in model.named_modules():\n",
        "        if hasattr(m, \"cfg\") and hasattr(m.cfg, \"gamma\"):\n",
        "            any_gamma = m.cfg.gamma; break\n",
        "    print(f\"[VPS] Wrapped layers: {wrapped}\")\n",
        "    print(f\"[diag] sample gamma = {any_gamma}\", flush=True)\n",
        "\n",
        "    model.eval()\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    items = load_arc(n, seed) if task==\"arc_c\" else load_csqa(n, seed)\n",
        "    letter_id_map = {L: robust_letter_id(tok, L) for L in LETTER_SET}\n",
        "\n",
        "    correct, times = 0, []\n",
        "    for i, (prompt, gold) in enumerate(items, 1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            if vps_on:\n",
        "                for _ in range(max(1, iters)):\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=tail_tokens, ce_T=6)\n",
        "            inputs = tok(prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            if vps_on:\n",
        "                try:\n",
        "                    vps_iterate_once(model, tok, prompt, gold, tail_tokens=max(8, tail_tokens//2), ce_T=4)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            scores = score_letters_logprob(model, tok, inputs, letter_id_map)\n",
        "            pred = max(scores.items(), key=lambda kv: kv[1])[0]\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "        correct += int(pred == gold)\n",
        "\n",
        "        if (i % max(1, print_every) == 0) or (i == n):\n",
        "            eta = (sum(times)/len(times))*(n-i)\n",
        "            print(f\"[{task}] {i}/{n} acc_so_far={correct/i:.3f} last_dt={dt:.2f}s ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    acc = correct / n if n else 0.0\n",
        "    lat = sum(times)/max(1,len(times))\n",
        "    print(json.dumps({\"task\":task,\"n\":n,\"acc\":acc,\"latency_mean_s\":lat}), flush=True)\n",
        "\n",
        "    del model, tok\n",
        "    free_cuda()\n",
        "    return {\"task\":task,\"n\":n,\"acc\":acc,\"latency_mean_s\":lat}\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--task\", type=str, choices=[\"arc_c\",\"csqa\"], default=\"arc_c\")\n",
        "    ap.add_argument(\"--n\", type=int, default=80)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", type=str, choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.65)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24)\n",
        "    ap.add_argument(\"--iters\", type=int, default=2)\n",
        "    ap.add_argument(\"--apply_to\", type=str, choices=[\"full\",\"attn\"], default=\"full\")\n",
        "    ap.add_argument(\"--print_every\", type=int, default=5)\n",
        "    args, _ = ap.parse_known_args()\n",
        "\n",
        "    base = run_task(args.task, args.n, args.seed, False,\n",
        "                    args.model_name, args.dtype, args.gamma, args.adaptive_gamma,\n",
        "                    args.tail_tokens, args.iters, args.apply_to, args.print_every)\n",
        "\n",
        "    vps  = run_task(args.task, args.n, args.seed, True,\n",
        "                    args.model_name, args.dtype, args.gamma, args.adaptive_gamma,\n",
        "                    args.tail_tokens, args.iters, args.apply_to, args.print_every)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "tuGPjh9hMymo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "# free up stray JAX VRAM on Colab/T4\n",
        "export JAX_PLATFORMS=cpu\n",
        "export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "export XLA_PYTHON_CLIENT_MEM_FRACTION=.0\n",
        "\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "python -u vps/scripts/mc_logprob_eval.py \\\n",
        "  --task arc_c --n 80 --seed 1234 \\\n",
        "  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\n",
        "  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 2 \\\n",
        "  --apply_to full --print_every 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ_HePcYNQAa",
        "outputId": "c1e04845-a180-48d3-951b-f6d615d81663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase] BASELINE (VPS OFF) — building model...\n",
            "[VPS] Wrapped layers: 252\n",
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.0\n",
            "[arc_c] 5/80 acc_so_far=0.600 last_dt=0.36s ~ETA 0m38s\n",
            "[arc_c] 10/80 acc_so_far=0.600 last_dt=0.34s ~ETA 0m30s\n",
            "[arc_c] 15/80 acc_so_far=0.600 last_dt=0.36s ~ETA 0m26s\n",
            "[arc_c] 20/80 acc_so_far=0.650 last_dt=0.34s ~ETA 0m23s\n",
            "[arc_c] 25/80 acc_so_far=0.720 last_dt=0.44s ~ETA 0m21s\n",
            "[arc_c] 30/80 acc_so_far=0.733 last_dt=0.34s ~ETA 0m19s\n",
            "[arc_c] 35/80 acc_so_far=0.743 last_dt=0.37s ~ETA 0m17s\n",
            "[arc_c] 40/80 acc_so_far=0.750 last_dt=0.34s ~ETA 0m15s\n",
            "[arc_c] 45/80 acc_so_far=0.756 last_dt=0.35s ~ETA 0m13s\n",
            "[arc_c] 50/80 acc_so_far=0.760 last_dt=0.34s ~ETA 0m11s\n",
            "[arc_c] 55/80 acc_so_far=0.764 last_dt=0.35s ~ETA 0m9s\n",
            "[arc_c] 60/80 acc_so_far=0.783 last_dt=0.46s ~ETA 0m7s\n",
            "[arc_c] 65/80 acc_so_far=0.785 last_dt=0.35s ~ETA 0m5s\n",
            "[arc_c] 70/80 acc_so_far=0.800 last_dt=0.36s ~ETA 0m3s\n",
            "[arc_c] 75/80 acc_so_far=0.787 last_dt=0.36s ~ETA 0m1s\n",
            "[arc_c] 80/80 acc_so_far=0.762 last_dt=0.36s ~ETA 0m0s\n",
            "{\"task\": \"arc_c\", \"n\": 80, \"acc\": 0.7625, \"latency_mean_s\": 0.37114493250846864}\n",
            "\n",
            "[Phase] VPS ON — building model...\n",
            "[VPS] Wrapped layers: 252\n",
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.65\n",
            "[arc_c] 5/80 acc_so_far=0.600 last_dt=2.29s ~ETA 3m41s\n",
            "[arc_c] 10/80 acc_so_far=0.700 last_dt=2.26s ~ETA 3m7s\n",
            "[arc_c] 15/80 acc_so_far=0.667 last_dt=2.28s ~ETA 2m48s\n",
            "[arc_c] 20/80 acc_so_far=0.700 last_dt=2.35s ~ETA 2m32s\n",
            "[arc_c] 25/80 acc_so_far=0.720 last_dt=2.44s ~ETA 2m18s\n",
            "[arc_c] 30/80 acc_so_far=0.733 last_dt=2.55s ~ETA 2m4s\n",
            "[arc_c] 35/80 acc_so_far=0.743 last_dt=2.59s ~ETA 1m51s\n",
            "[arc_c] 40/80 acc_so_far=0.750 last_dt=2.71s ~ETA 1m38s\n",
            "[arc_c] 45/80 acc_so_far=0.756 last_dt=2.78s ~ETA 1m25s\n",
            "[arc_c] 50/80 acc_so_far=0.780 last_dt=2.77s ~ETA 1m13s\n",
            "[arc_c] 55/80 acc_so_far=0.782 last_dt=2.83s ~ETA 1m1s\n",
            "[arc_c] 60/80 acc_so_far=0.800 last_dt=2.84s ~ETA 0m48s\n",
            "[arc_c] 65/80 acc_so_far=0.800 last_dt=2.68s ~ETA 0m36s\n",
            "[arc_c] 70/80 acc_so_far=0.814 last_dt=2.55s ~ETA 0m24s\n",
            "[arc_c] 75/80 acc_so_far=0.800 last_dt=2.45s ~ETA 0m12s\n",
            "[arc_c] 80/80 acc_so_far=0.762 last_dt=2.41s ~ETA 0m0s\n",
            "{\"task\": \"arc_c\", \"n\": 80, \"acc\": 0.7625, \"latency_mean_s\": 2.420207437872887}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757366544.812666   13948 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757366544.820173   13948 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757366544.854844   13948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757366544.854873   13948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757366544.854876   13948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757366544.854880   13948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.97s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 12.42s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.10s/it]\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.47s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.09s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.74s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "mkdir -p vps/scripts\n",
        "cat > vps/scripts/confidence_filtered_eval.py << 'PY'\n",
        "from __future__ import annotations\n",
        "import os, sys, json, math, time, argparse, random, gc\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# --- robust imports and quiet logs ---\n",
        "try:\n",
        "    THIS_DIR = os.path.abspath(os.path.dirname(__file__))\n",
        "except NameError:\n",
        "    THIS_DIR = os.path.abspath(os.getcwd())\n",
        "PROJ_ROOT   = os.path.abspath(os.path.join(THIS_DIR, \"..\"))\n",
        "PROJ_PARENT = os.path.abspath(os.path.join(PROJ_ROOT, \"..\"))\n",
        "for cand in {\"/content\", \"/content/vps\", PROJ_PARENT, PROJ_ROOT}:\n",
        "    if cand not in sys.path and os.path.isdir(cand):\n",
        "        sys.path.insert(0, cand)\n",
        "\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from vps.scripts.infer_vps import build\n",
        "from vpscore.config import VPSConfig\n",
        "from vpscore.math_utils import compute_token_entropy\n",
        "\n",
        "LETTERS = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "def set_seed(seed:int):\n",
        "    random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def build_cfg(model_name:str, dtype:str, apply_to:str, gamma:float, adaptive:bool)->VPSConfig:\n",
        "    cfg = VPSConfig()\n",
        "    cfg.model_name = model_name\n",
        "    cfg.device_map = \"auto\"\n",
        "    cfg.dtype = dtype\n",
        "    cfg.torch_dtype_str = dtype\n",
        "    cfg.temperature = 0.0\n",
        "    cfg.top_p = 1.0\n",
        "    cfg.top_k = 0\n",
        "    cfg.max_new_tokens = 1\n",
        "    cfg.gamma = float(gamma)\n",
        "    cfg.adaptive_gamma = bool(adaptive)\n",
        "    cfg.max_memory = {\"cuda:0\": \"10GiB\", \"cpu\": \"64GiB\"}\n",
        "    if apply_to == \"attn\":\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "    else:\n",
        "        cfg.apply_to = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"]\n",
        "    cfg.qk_coupling = True\n",
        "    return cfg\n",
        "\n",
        "def set_vps_enabled(model, enabled:bool):\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__ == \"VPSLinear\":\n",
        "            if not hasattr(m, \"_saved_gamma\"):\n",
        "                m._saved_gamma = float(getattr(m.cfg, \"gamma\", 0.0))\n",
        "            m.cfg.gamma = float(m._saved_gamma) if enabled else 0.0\n",
        "\n",
        "def robust_letter_id(tok, letter:str)->int:\n",
        "    for s in (letter, \" \"+letter, \"\\n\"+letter):\n",
        "        ids = tok(s, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(ids)==1: return ids[0]\n",
        "    ids = tok(\" \"+letter, add_special_tokens=False)[\"input_ids\"]\n",
        "    return ids[-1]\n",
        "\n",
        "def robust_letter_ids(tok, letter:str)->List[int]:\n",
        "    for s in (letter, \" \"+letter, \"\\n\"+letter):\n",
        "        ids = tok(s, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(ids) >= 1: return ids\n",
        "    return [robust_letter_id(tok, letter)]\n",
        "\n",
        "def load_pool(task:str, n_pool:int, seed:int)->List[Tuple[str,str]]:\n",
        "    if task==\"arc_c\":\n",
        "        ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
        "        get = lambda i: (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. Output **one letter only** (A/B/C/D/E).\\n\\n\"\n",
        "            f\"Question: {ds[i]['question']}\\n\\nOptions:\\n\" +\n",
        "            \"\\n\".join([f\"{l}) {t}\" for l,t in zip(ds[i][\"choices\"][\"label\"], ds[i][\"choices\"][\"text\"])]) +\n",
        "            \"\\n\\nAnswer:\"\n",
        "            , ds[i][\"answerKey\"].strip().upper()\n",
        "        )\n",
        "        total = len(ds)\n",
        "    else:\n",
        "        ds = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
        "        get = lambda i: (\n",
        "            \"You are a careful reasoning assistant.\\n\"\n",
        "            \"Pick the BEST option. Output **one letter only** (A/B/C/D/E).\\n\\n\"\n",
        "            f\"Question: {ds[i]['question']}\\n\\nOptions:\\n\" +\n",
        "            \"\\n\".join([f\"{c['label']}) {c['text']}\" for c in ds[i][\"choices\"]]) +\n",
        "            \"\\n\\nAnswer:\"\n",
        "            , ds[i][\"answerKey\"].strip().upper()\n",
        "        )\n",
        "        total = len(ds)\n",
        "\n",
        "    idx = list(range(total)); random.Random(seed).shuffle(idx)\n",
        "    idx = idx[:n_pool]\n",
        "    return [get(i) for i in idx]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def phrase_scores(model, tok, prompt:str, phrase_for:Dict[str,str]) -> Dict[str,float]:\n",
        "    model.config.use_cache = False\n",
        "    dev = next(model.parameters()).device\n",
        "    p = tok(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    p = {k: v.to(dev) for k,v in p.items()}\n",
        "    prefix = p[\"input_ids\"][0]\n",
        "\n",
        "    out_scores = {}\n",
        "    for L, phrase in phrase_for.items():\n",
        "        t = tok(phrase, add_special_tokens=False, return_tensors=\"pt\")\n",
        "        t_ids = t[\"input_ids\"][0].to(dev)\n",
        "        full = torch.cat([prefix, t_ids], dim=0).unsqueeze(0)\n",
        "        attn = torch.ones_like(full, dtype=torch.long, device=dev)\n",
        "        out = model(input_ids=full, attention_mask=attn, use_cache=False)\n",
        "        logits = out.logits[0]\n",
        "        T = t_ids.shape[0]\n",
        "        start = prefix.shape[0] - 1\n",
        "        logp = torch.log_softmax(logits[start:start+T, :], dim=-1)\n",
        "        tok_logp = logp[torch.arange(T, device=dev), t_ids]\n",
        "        out_scores[L] = float(tok_logp.sum().item())\n",
        "    return out_scores\n",
        "\n",
        "def vps_iter_once(model, tok, prompt:str, gold_letter:str, tail_tokens:int, ce_T:int=6):\n",
        "    dev = next(model.parameters()).device\n",
        "    base = tok(prompt, return_tensors=\"pt\").to(dev)\n",
        "    # tail\n",
        "    L = base[\"input_ids\"].shape[1]\n",
        "    if tail_tokens > 0 and L > tail_tokens:\n",
        "        sl = slice(L - tail_tokens, L)\n",
        "        tail = {k:(v[:,sl] if v.dim()==2 and v.size(1)==L else v) for k,v in base.items()}\n",
        "    else:\n",
        "        tail = base\n",
        "    # entropy probe\n",
        "    with torch.inference_mode():\n",
        "        out = model(**tail, use_cache=False)\n",
        "        ent = compute_token_entropy(out.logits[0, -1, :])\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"policy\") and m.policy is not None:\n",
        "            try: m.policy.set_token_entropy(float(ent))\n",
        "            except: pass\n",
        "    # tiny CE on gold letter tokens\n",
        "    gold_ids = torch.tensor(robust_letter_ids(tok, gold_letter), dtype=torch.long, device=dev)\n",
        "    T = max(1, min(ce_T, gold_ids.shape[0]))\n",
        "    targets = gold_ids[-T:].unsqueeze(0)\n",
        "\n",
        "    old_cache = getattr(model.config, \"use_cache\", True)\n",
        "    model.config.use_cache = False\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    with torch.enable_grad():\n",
        "        out2 = model(**tail, use_cache=False)\n",
        "        logits2 = out2.logits[:, -T:, :]\n",
        "        ce = torch.nn.functional.cross_entropy(\n",
        "            logits2.reshape(-1, logits2.size(-1)),\n",
        "            targets.reshape(-1)\n",
        "        )\n",
        "        ce.backward()\n",
        "    model.config.use_cache = old_cache\n",
        "\n",
        "def margin_from_scores(scores:Dict[str,float])->float:\n",
        "    # top1 - top2\n",
        "    vals = sorted(scores.values(), reverse=True)\n",
        "    return float(vals[0] - vals[1]) if len(vals)>=2 else 0.0\n",
        "\n",
        "def mcnemar_chi2(b:int, c:int)->float:\n",
        "    if b+c == 0: return 0.0\n",
        "    return (abs(b-c)-1)**2 / (b+c)\n",
        "\n",
        "def run(task:str, n_pool:int, n_select:int, seed:int,\n",
        "        model_name:str, dtype:str, gamma:float, adaptive:bool,\n",
        "        tail_tokens:int, iters:int, apply_to:str, phrase_template:str):\n",
        "\n",
        "    set_seed(seed)\n",
        "    phrase_for = {L: phrase_template.replace(\"{L}\", L) for L in LETTERS}\n",
        "\n",
        "    # -------- Baseline model (no VPS) --------\n",
        "    print(\"\\n[Phase] BASELINE (VPS OFF) — building model...\", flush=True)\n",
        "    cfg0 = build_cfg(model_name, dtype, apply_to, gamma, adaptive)\n",
        "    tok0, model0, hooks0 = build(cfg0)\n",
        "    set_vps_enabled(model0, False)\n",
        "    wrapped0 = sum(1 for _ in model0.modules() if _.__class__.__name__==\"VPSLinear\")\n",
        "    any_gamma0 = None\n",
        "    for _,m in model0.named_modules():\n",
        "        if hasattr(m,\"cfg\") and hasattr(m.cfg,\"gamma\"): any_gamma0 = m.cfg.gamma; break\n",
        "    print(f\"[VPS] Wrapped layers: {wrapped0}\")\n",
        "    print(f\"[diag] sample gamma = {any_gamma0}\", flush=True)\n",
        "    model0.eval(); model0.config.use_cache=False\n",
        "\n",
        "    pool = load_pool(task, n_pool, seed)\n",
        "\n",
        "    # First pass: baseline phrase scores for uncertainty selection\n",
        "    pool_recs = []\n",
        "    for i,(prompt,gold) in enumerate(pool,1):\n",
        "        sc = phrase_scores(model0, tok0, prompt, phrase_for)\n",
        "        pred = max(sc.items(), key=lambda kv: kv[1])[0]\n",
        "        marg = margin_from_scores(sc)\n",
        "        pool_recs.append((prompt, gold, sc, pred, marg))\n",
        "        if i % 20 == 0 or i==len(pool):\n",
        "            print(f\"[select] scored {i}/{len(pool)}\", flush=True)\n",
        "\n",
        "    # select most uncertain n_select (smallest margin)\n",
        "    pool_recs.sort(key=lambda r: r[4])\n",
        "    sel = pool_recs[:n_select]\n",
        "\n",
        "    # Baseline accuracy on selected\n",
        "    base_correct = sum(int(r[3]==r[1]) for r in sel)\n",
        "    base_acc = base_correct / max(1,len(sel))\n",
        "    print(json.dumps({\"phase\":\"baseline\",\"task\":task,\"n\":len(sel),\"acc\":base_acc}), flush=True)\n",
        "\n",
        "    del model0, tok0\n",
        "    free_cuda()\n",
        "\n",
        "    # -------- VPS model (full power) --------\n",
        "    print(\"\\n[Phase] VPS ON — building model...\", flush=True)\n",
        "    cfg1 = build_cfg(model_name, dtype, apply_to, gamma, adaptive)\n",
        "    tok1, model1, hooks1 = build(cfg1)\n",
        "    set_vps_enabled(model1, True)\n",
        "    wrapped1 = sum(1 for _ in model1.modules() if _.__class__.__name__==\"VPSLinear\")\n",
        "    any_gamma1 = None\n",
        "    for _,m in model1.named_modules():\n",
        "        if hasattr(m,\"cfg\") and hasattr(m.cfg,\"gamma\"): any_gamma1 = m.cfg.gamma; break\n",
        "    print(f\"[VPS] Wrapped layers: {wrapped1}\")\n",
        "    print(f\"[diag] sample gamma = {any_gamma1}\", flush=True)\n",
        "    model1.eval(); model1.config.use_cache=False\n",
        "\n",
        "    b_to_v, v_to_b = 0, 0\n",
        "    vps_correct = 0\n",
        "    t0 = time.time()\n",
        "    for i,(prompt,gold,sc_base,pred_base,_) in enumerate(sel,1):\n",
        "        try:\n",
        "            for _ in range(max(1,iters)):\n",
        "                vps_iter_once(model1, tok1, prompt, gold, tail_tokens=tail_tokens, ce_T=6)\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            free_cuda()\n",
        "            try:\n",
        "                vps_iter_once(model1, tok1, prompt, gold, tail_tokens=max(8, tail_tokens//2), ce_T=4)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        sc_vps = phrase_scores(model1, tok1, prompt, phrase_for)\n",
        "        pred_vps = max(sc_vps.items(), key=lambda kv: kv[1])[0]\n",
        "        vps_correct += int(pred_vps==gold)\n",
        "        if pred_base!=gold and pred_vps==gold: b_to_v += 1\n",
        "        if pred_base==gold and pred_vps!=gold: v_to_b += 1\n",
        "\n",
        "        if i%10==0 or i==len(sel):\n",
        "            eta = (time.time()-t0)/i*(len(sel)-i)\n",
        "            print(f\"[eval] {i}/{len(sel)} done ~ETA {int(eta//60)}m{int(eta%60)}s\", flush=True)\n",
        "\n",
        "    vps_acc = vps_correct / max(1,len(sel))\n",
        "    chi2 = mcnemar_chi2(b_to_v, v_to_b)\n",
        "\n",
        "    print(json.dumps({\n",
        "        \"phase\":\"vps\",\n",
        "        \"task\":task,\n",
        "        \"n\":len(sel),\n",
        "        \"acc\":vps_acc,\n",
        "        \"delta_acc\": vps_acc - base_acc,\n",
        "        \"flips_to_correct\": b_to_v,\n",
        "        \"flips_to_wrong\": v_to_b,\n",
        "        \"mcnemar_chi2\": chi2\n",
        "    }), flush=True)\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--task\", choices=[\"arc_c\",\"csqa\"], default=\"arc_c\")\n",
        "    ap.add_argument(\"--n_pool\", type=int, default=240)\n",
        "    ap.add_argument(\"--n_select\", type=int, default=80)\n",
        "    ap.add_argument(\"--seed\", type=int, default=1234)\n",
        "    ap.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "    ap.add_argument(\"--dtype\", choices=[\"fp16\",\"bf16\",\"fp32\"], default=\"fp16\")\n",
        "    ap.add_argument(\"--gamma\", type=float, default=0.65)\n",
        "    ap.add_argument(\"--adaptive_gamma\", action=\"store_true\")\n",
        "    ap.add_argument(\"--tail_tokens\", type=int, default=24)\n",
        "    ap.add_argument(\"--iters\", type=int, default=3)\n",
        "    ap.add_argument(\"--apply_to\", choices=[\"full\",\"attn\"], default=\"full\")\n",
        "    ap.add_argument(\"--phrase_template\", type=str, default=\" Answer: {L}\")\n",
        "    args, _ = ap.parse_known_args()\n",
        "\n",
        "    run(args.task, args.n_pool, args.n_select, args.seed,\n",
        "        args.model_name, args.dtype, args.gamma, args.adaptive_gamma,\n",
        "        args.tail_tokens, args.iters, args.apply_to, args.phrase_template)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "metadata": {
        "id": "uRQw8-dFQx2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Experiments results:\n",
        "%%bash\n",
        "set -e\n",
        "# free stray JAX VRAM on Colab/T4\n",
        "export JAX_PLATFORMS=cpu\n",
        "export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "export XLA_PYTHON_CLIENT_MEM_FRACTION=.0\n",
        "\n",
        "export PYTHONPATH=/content:/content/vps:$PYTHONPATH\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "export PYTHONUNBUFFERED=1\n",
        "\n",
        "# ARC-Challenge, pool=240 → select 80 hardest-by-margin\n",
        "python -u vps/scripts/confidence_filtered_eval.py \\\n",
        "  --task arc_c --n_pool 240 --n_select 80 --seed 1234 \\\n",
        "  --model_name \"Qwen/Qwen2.5-3B-Instruct\" --dtype fp16 \\\n",
        "  --gamma 0.65 --adaptive_gamma --tail_tokens 24 --iters 3 \\\n",
        "  --apply_to full --phrase_template \" Answer: {L}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUB7oWiBQ_qi",
        "outputId": "9b83e68e-3234-49ec-94ce-ecb78e39cab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase] BASELINE (VPS OFF) — building model...\n",
            "[VPS] Wrapped layers: 252\n",
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.0\n",
            "[select] scored 20/240\n",
            "[select] scored 40/240\n",
            "[select] scored 60/240\n",
            "[select] scored 80/240\n",
            "[select] scored 100/240\n",
            "[select] scored 120/240\n",
            "[select] scored 140/240\n",
            "[select] scored 160/240\n",
            "[select] scored 180/240\n",
            "[select] scored 200/240\n",
            "[select] scored 220/240\n",
            "[select] scored 240/240\n",
            "{\"phase\": \"baseline\", \"task\": \"arc_c\", \"n\": 80, \"acc\": 0.5125}\n",
            "\n",
            "[Phase] VPS ON — building model...\n",
            "[VPS] Wrapped layers: 252\n",
            "[VPS] Wrapped layers: 252\n",
            "[diag] sample gamma = 0.65\n",
            "[eval] 10/80 done ~ETA 5m37s\n",
            "[eval] 20/80 done ~ETA 4m48s\n",
            "[eval] 30/80 done ~ETA 4m1s\n",
            "[eval] 40/80 done ~ETA 3m12s\n",
            "[eval] 50/80 done ~ETA 2m24s\n",
            "[eval] 60/80 done ~ETA 1m36s\n",
            "[eval] 70/80 done ~ETA 0m48s\n",
            "[eval] 80/80 done ~ETA 0m0s\n",
            "{\"phase\": \"vps\", \"task\": \"arc_c\", \"n\": 80, \"acc\": 0.5375, \"delta_acc\": 0.025000000000000022, \"flips_to_correct\": 6, \"flips_to_wrong\": 4, \"mcnemar_chi2\": 0.1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757367528.894703   17965 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757367528.919549   17965 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757367528.984725   17965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757367528.984820   17965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757367528.984834   17965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757367528.984846   17965 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:21<00:21, 21.13s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 15.41s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.26s/it]\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:21<00:21, 21.10s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 15.45s/it]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.30s/it]\n"
          ]
        }
      ]
    }
  ]
}